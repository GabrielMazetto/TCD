{"id_funcao": 0, "titulo": "load_data_file", "categoria": "Coleta e Carregamento de Dados", "subcategoria": "Carregamento de Arquivos", "descricao": "Carrega dados de um arquivo (CSV, XLS, XLSX) para um DataFrame pandas.", "codigo_funcao": "def load_data_file(file_path: Union[str, Path], sheet_name: Union[int, str, None] = 0) -> pd.DataFrame:\n    \"\"\"\n    Descrição: Carrega dados de um arquivo (CSV, XLS, XLSX) para um DataFrame pandas.\n\n    Args:\n        file_path (Union[str, Path]): O caminho completo (string ou objeto Path) para o arquivo.\n        sheet_name (Union[int, str, None], optional):\n            O nome ou índice da planilha a ser lida (relevante apenas para arquivos Excel).\n            O padrão é 0 (a primeira planilha). Ignorado para CSV.\n\n    Returns:\n        pd.DataFrame: DataFrame do pandas contendo os dados carregados.\n\n    Raises:\n        TypeError: Se 'file_path' não for uma string ou objeto Path.\n        FileNotFoundError: Se o arquivo no 'file_path' não for encontrado.\n        ValueError: Se a extensão do arquivo não for suportada (.csv, .xls, .xlsx).\n        Exception: Para outros erros de leitura do pandas ou openpyxl.\n    \"\"\"\n    if not isinstance(file_path, (str, Path)):\n        raise TypeError(\"O argumento 'file_path' deve ser uma string ou objeto Path.\")\n\n    file_path_obj = Path(file_path)\n\n    if not file_path_obj.exists():\n        raise FileNotFoundError(f\"O arquivo não foi encontrado em: {file_path_obj}\")\n\n    file_extension = file_path_obj.suffix.lower()\n\n    try:\n        if file_extension == '.csv':\n            df = pd.read_csv(file_path_obj)\n        elif file_extension in ['.xls', '.xlsx']:\n            # A leitura de .xlsx requer a biblioteca 'openpyxl' instalada no ambiente.\n            df = pd.read_excel(file_path_obj, sheet_name=sheet_name)\n        else:\n            raise ValueError(f\"Formato de arquivo não suportado: '{file_extension}'. \"\n                             \"Use .csv, .xls ou .xlsx.\")\n    \n    except ValueError as ve: \n        raise ve \n    except Exception as e:\n        raise Exception(f\"Erro ao ler o arquivo '{file_path_obj}': {e}\")\n\n    return df", "bibliotecas": ["pandas", "pathlib", "typing"], "versao": "0.1.0"}
{"id_funcao": 1, "titulo": "filter_dataframe", "categoria": "Exploração e Limpeza de Dados", "subcategoria": "Limpeza e Tratamento de Dados", "descricao": "Filtra um DataFrame mantendo linhas com base em uma condição aplicada a uma coluna específica.", "codigo_funcao": "def filter_dataframe(\n    df: pd.DataFrame,\n    column: str,\n    values: Union[Any, List[Any]],\n    operator: Literal['isin', 'eq', 'ne', 'gt', 'lt', 'ge', 'le'] = 'isin'\n) -> pd.DataFrame:\n    \"\"\"\n    Descrição: Filtra um DataFrame mantendo linhas com base em uma condição\n               aplicada a uma coluna específica.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        column (str): Nome da coluna a ser usada para o filtro.\n        values (Union[Any, List[Any]]): O valor ou lista de valores para comparar.\n            Para 'isin', deve ser uma lista. Para os outros, geralmente um valor único.\n        operator (Literal['isin', 'eq', 'ne', 'gt', 'lt', 'ge', 'le'], optional):\n            Operador de comparação:\n            'isin': Mantém linhas onde o valor da coluna ESTÁ na lista 'values'.\n            'eq': Mantém linhas onde o valor da coluna é IGUAL a 'values'.\n            'ne': Mantém linhas onde o valor da coluna é DIFERENTE de 'values'.\n            'gt': Maior que 'values'.\n            'lt': Menor que 'values'.\n            'ge': Maior ou igual a 'values'.\n            'le': Menor ou igual a 'values'.\n            Defaults to 'isin'.\n\n    Returns:\n        pd.DataFrame: Um novo DataFrame contendo apenas as linhas filtradas.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame, ou 'values' for inadequado para 'operator'.\n        ValueError: Se 'column' não existir no DataFrame ou 'operator' for inválido.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if column not in df.columns:\n        raise ValueError(f\"A coluna '{column}' não existe no DataFrame.\")\n\n    valid_operators = ['isin', 'eq', 'ne', 'gt', 'lt', 'ge', 'le']\n    if operator not in valid_operators:\n        raise ValueError(f\"Operador '{operator}' inválido. Escolha entre: {valid_operators}\")\n\n    mask = None\n    try:\n        if operator == 'isin':\n            if not isinstance(values, (list, set, tuple, pd.Series, np.ndarray)):\n                 raise TypeError(\"Para o operador 'isin', 'values' deve ser uma lista ou similar.\")\n            mask = df[column].isin(values)\n        elif operator == 'eq':\n            mask = (df[column] == values)\n        elif operator == 'ne':\n            mask = (df[column] != values)\n        elif operator == 'gt':\n            mask = (df[column] > values)\n        elif operator == 'lt':\n            mask = (df[column] < values)\n        elif operator == 'ge':\n            mask = (df[column] >= values)\n        elif operator == 'le':\n            mask = (df[column] <= values)\n\n        return df.loc[mask].copy()\n\n    except Exception as e:\n        raise Exception(f\"Erro durante a filtragem com operador '{operator}': {e}\")", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 2, "titulo": "bin_numeric_feature", "categoria": "Engenharia e Transformação de Features", "subcategoria": "Discretização (Binning)", "descricao": "Cria colunas binned (faixas) para colunas numéricas usando quantis ou largura fixa.", "codigo_funcao": "def bin_numeric_feature(\n    df: pd.DataFrame,\n    cols: Optional[List[str]] = None,\n    bins: Union[int, List[float]] = 10,\n    strategy: str = \"quantile\",\n    inplace: bool = False,\n    labels: Optional[List[str]] = None,\n    new_col_suffix: str = \"_binned\"\n) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Descrição: Cria colunas binned (faixas) para colunas numéricas usando quantis ou largura fixa.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols (List[str], optional): Colunas a serem binned. Se None, infere colunas numéricas.\n        bins (int or List[float], optional): Número de bins (int) ou edges (list). Default: 10.\n        strategy (str, optional): \"quantile\" para bins por quantil, \"uniform\" para largura fixa (pd.cut). Default: \"quantile\".\n        inplace (bool, optional): Se True, modifica o DataFrame original. Default: False.\n        labels (List[str], optional): Labels para os bins. Se None, usa intervalos automáticos.\n        new_col_suffix (str, optional): Sufixo a ser adicionado ao nome da nova coluna. Default: \"_binned\".\n\n    Returns:\n        Optional[pd.DataFrame]: DataFrame com novas colunas binned, ou None se inplace=True.\n\n    Raises:\n        TypeError: Se 'df' não for pd.DataFrame ou 'cols' contiver colunas inexistentes.\n        ValueError: Se 'strategy' não for suportado ou 'bins' for inválido.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"df precisa ser um pandas.DataFrame\")\n\n    df_out = df if inplace else df.copy()\n\n    if cols is None:\n        cols_to_process = df_out.select_dtypes(include=[np.number]).columns.tolist()\n    else:\n        if not isinstance(cols, list):\n             raise TypeError(\"'cols' deve ser uma lista de nomes de colunas.\")\n        missing = [c for c in cols if c not in df_out.columns]\n        if missing:\n            raise ValueError(f\"As seguintes colunas não existem no df: {missing}\")\n        cols_to_process = cols\n\n    if strategy not in {\"quantile\", \"uniform\"}:\n        raise ValueError(\"strategy deve ser 'quantile' ou 'uniform'\")\n\n    for col in cols_to_process:\n        new_col_name = f\"{col}{new_col_suffix}\"\n        series = df_out[col].dropna() # pd.cut/qcut não lidam bem com NaN implicitamente\n\n        if series.empty:\n             df_out[new_col_name] = pd.NA # Coluna vazia ou só NaN\n             continue\n\n        try:\n            if strategy == \"quantile\":\n                if not isinstance(bins, int) or bins <= 0:\n                     raise ValueError(\"Para strategy='quantile', 'bins' deve ser um inteiro positivo (número de quantis).\")\n                df_out[new_col_name] = pd.qcut(df_out[col], q=bins, labels=labels, duplicates='drop')\n            else: # uniform (pd.cut)\n                # pd.cut aceita int (nº de bins de largura igual) ou list (edges)\n                df_out[new_col_name] = pd.cut(df_out[col], bins=bins, labels=labels, include_lowest=True, right=False)\n        except ValueError as e:\n            # Captura erro comum do qcut e dá uma mensagem melhor\n            if \"Bin edges must be unique\" in str(e) and strategy == \"quantile\":\n                 print(f\"Aviso: pd.qcut falhou para '{col}' (provavelmente muitos valores repetidos). \"\n                       f\"Tentando pd.cut com {bins} bins de largura uniforme como fallback.\")\n                 try:\n                      df_out[new_col_name] = pd.cut(df_out[col], bins=bins, labels=labels, include_lowest=True, right=False)\n                 except Exception as cut_e:\n                      print(f\"  Fallback pd.cut também falhou para '{col}': {cut_e}\")\n                      df_out[new_col_name] = pd.NA # Marca como falha\n            else:\n                 print(f\"Erro ao binar coluna '{col}': {e}\")\n                 df_out[new_col_name] = pd.NA # Marca como falha\n        except Exception as e:\n             print(f\"Erro inesperado ao binar coluna '{col}': {e}\")\n             df_out[new_col_name] = pd.NA # Marca como falha\n\n    if inplace:\n        return None\n    return df_out", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 3, "titulo": "encode_categorical", "categoria": "Engenharia e Transformação de Features", "subcategoria": "Encoding de Variáveis Categóricas", "descricao": "Codifica colunas categóricas usando One-Hot, Mapeamento, Replace ou Ordinal.", "codigo_funcao": "def encode_categorical(\n    df: pd.DataFrame,\n    cols: Optional[List[str]] = None,\n    method: Literal['onehot', 'map', 'replace', 'ordinal'] = 'onehot',\n    mapping_dict: Optional[Dict[str, Dict[Any, Any]]] = None,\n    replace_dict: Optional[Dict[str, Dict[Any, Any]]] = None,\n    drop_original: bool = False,\n    new_col_suffix: str = '_encoded',\n    inplace: bool = False\n) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Descrição: Codifica colunas categóricas usando One-Hot, Mapeamento, Replace ou Ordinal.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols (List[str], optional): Colunas a serem codificadas. Se None, tenta\n            inferir colunas 'object' ou 'category'. Defaults to None.\n        method (Literal['onehot', 'map', 'replace', 'ordinal'], optional):\n            'onehot': Usa pd.get_dummies.\n            'map': Usa .map() com base no 'mapping_dict'. Cria novas colunas.\n            'replace': Usa .replace() com base no 'replace_dict'. Modifica colunas existentes.\n            'ordinal': Usa sklearn.preprocessing.OrdinalEncoder. Modifica colunas existentes.\n            Defaults to 'onehot'.\n        mapping_dict (Dict[str, Dict[Any, Any]], optional): Dicionário onde as chaves são\n            nomes de colunas e os valores são dicionários de mapeamento (valor_antigo: valor_novo).\n            Necessário para method='map'. Defaults to None.\n        replace_dict (Dict[str, Dict[Any, Any]], optional): Dicionário similar a mapping_dict,\n            usado para method='replace'. Permite substituir múltiplos valores por um novo.\n            Defaults to None.\n        drop_original (bool, optional): Se True e method='onehot' ou method='map',\n            remove as colunas originais. Defaults to False.\n        new_col_suffix (str, optional): Sufixo para novas colunas criadas por 'map'.\n            Defaults to '_encoded'.\n        inplace (bool, optional): Se True, modifica o DataFrame original (aplicável a\n            'replace' e 'ordinal'). Defaults to False.\n\n    Returns:\n        Optional[pd.DataFrame]: DataFrame com colunas codificadas, ou None se inplace=True.\n\n    Raises:\n        TypeError: Se 'df' não for DataFrame, ou dicionários inválidos.\n        ValueError: Se colunas não existirem, ou método/parâmetros incompatíveis.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n\n    df_out = df if inplace and method in ['replace', 'ordinal'] else df.copy()\n\n    if cols is None:\n        cols_to_process = df_out.select_dtypes(include=['object', 'category']).columns.tolist()\n        print(f\"Aviso: 'cols' não fornecido. Inferindo colunas categóricas: {cols_to_process}\")\n    else:\n        if not isinstance(cols, list):\n             raise TypeError(\"'cols' deve ser uma lista de nomes de colunas.\")\n        missing = [c for c in cols if c not in df_out.columns]\n        if missing:\n            raise ValueError(f\"As seguintes colunas não existem no df: {missing}\")\n        cols_to_process = cols\n\n    if not cols_to_process:\n        print(\"Aviso: Nenhuma coluna selecionada ou inferida para codificação.\")\n        return df_out if not inplace else None\n\n    original_cols_to_drop = []\n\n    if method == 'onehot':\n        df_out = pd.get_dummies(df_out, columns=cols_to_process, drop_first=False, dummy_na=False)\n        if drop_original:\n             pass # get_dummies já remove se columns for especificado\n        else:\n             # pd.get_dummies remove a original por padrão se 'columns' é usado.\n             if drop_original == False:\n                 print(\"Aviso: `pd.get_dummies` remove a coluna original quando `columns` é especificado.\")\n\n    elif method == 'map':\n        if mapping_dict is None:\n            raise ValueError(\"O argumento 'mapping_dict' é obrigatório para method='map'.\")\n        if not isinstance(mapping_dict, dict):\n            raise TypeError(\"'mapping_dict' deve ser um dicionário.\")\n\n        for col in cols_to_process:\n            if col not in mapping_dict:\n                print(f\"Aviso: Coluna '{col}' não encontrada em 'mapping_dict'. Pulando.\")\n                continue\n            if not isinstance(mapping_dict[col], dict):\n                 raise TypeError(f\"'mapping_dict' para '{col}' deve ser um dicionário interno.\")\n\n            new_col_name = f\"{col}{new_col_suffix}\"\n            df_out[new_col_name] = df_out[col].map(mapping_dict[col])\n    \n            if drop_original:\n                 original_cols_to_drop.append(col)\n\n    elif method == 'replace':\n        if replace_dict is None:\n            raise ValueError(\"O argumento 'replace_dict' é obrigatório para method='replace'.\")\n        if not isinstance(replace_dict, dict):\n             raise TypeError(\"'replace_dict' deve ser um dicionário.\")\n\n        for col in cols_to_process:\n             if col not in replace_dict:\n                 print(f\"Aviso: Coluna '{col}' não encontrada em 'replace_dict'. Pulando.\")\n                 continue\n             if not isinstance(replace_dict[col], dict):\n                 raise TypeError(f\"'replace_dict' para '{col}' deve ser um dicionário interno.\")\n\n             # O replace do pandas pode aceitar um dict diretamente para a coluna\n             df_out[col].replace(replace_dict[col], inplace=True) # Modifica df_out\n\n    elif method == 'ordinal':\n        encoder = OrdinalEncoder()\n        try:\n             # OrdinalEncoder lida com múltiplas colunas\n             df_out[cols_to_process] = encoder.fit_transform(df_out[cols_to_process])\n        except Exception as e:\n             raise ValueError(f\"Erro ao aplicar OrdinalEncoder: {e}. Verifique os dtypes das colunas.\")\n\n    else:\n        raise ValueError(f\"Método '{method}' inválido. Escolha 'onehot', 'map', 'replace', 'ordinal'.\")\n\n    # Drop colunas originais se necessário (para map)\n    if drop_original and method == 'map' and original_cols_to_drop:\n         cols_to_drop_unique = list(set(original_cols_to_drop) & set(df_out.columns))\n         if cols_to_drop_unique:\n             df_out.drop(columns=cols_to_drop_unique, inplace=True)\n\n\n    if inplace and method in ['replace', 'ordinal']:\n        return None\n    elif inplace and method in ['onehot', 'map']:\n         print(f\"Aviso: inplace=True não é recomendado para method='{method}'. Retornando cópia modificada.\")\n         return df_out\n    else: \n        return df_out", "bibliotecas": ["pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 4, "titulo": "create_column_from_conditions", "categoria": "Engenharia e Transformação de Features", "subcategoria": "Criação de Features", "descricao": "Cria uma nova coluna com base em múltiplas condições usando np.select.", "codigo_funcao": "def create_column_from_conditions(\n    df: pd.DataFrame,\n    new_col_name: str,\n    conditions: List[pd.Series], \n    choices: List[Any],\n    default: Any = None\n) -> pd.DataFrame:\n    \"\"\"\n    Descrição: Cria uma nova coluna com base em múltiplas condições usando np.select.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        new_col_name (str): Nome da nova coluna a ser criada.\n        conditions (List[pd.Series]): Lista de condições booleanas (máscaras do pandas, ex: [df['A'] > 5, df['B'] == 'X']).\n        choices (List[Any]): Lista de valores a serem atribuídos para cada condição correspondente.\n                             Deve ter o mesmo tamanho de 'conditions'.\n        default (Any, optional): Valor padrão se nenhuma condição for atendida. Defaults to None.\n\n    Returns:\n        pd.DataFrame: DataFrame com a nova coluna condicional adicionada (uma cópia).\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame do pandas ou 'conditions' não for lista de Series.\n        ValueError: Se o número de 'conditions' não for igual ao número de 'choices',\n                    ou se 'new_col_name' já existir.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame do pandas.\")\n    if new_col_name in df.columns:\n        raise ValueError(f\"A coluna '{new_col_name}' já existe no DataFrame.\")\n    if not isinstance(conditions, list) or not all(isinstance(c, pd.Series) for c in conditions):\n         raise TypeError(\"'conditions' deve ser uma lista de Series booleanas (máscaras).\")\n    if not isinstance(choices, list):\n         raise TypeError(\"'choices' deve ser uma lista.\")\n    if len(conditions) != len(choices):\n        raise ValueError(f\"O número de 'conditions' ({len(conditions)}) deve ser igual \"\n                         f\"ao número de 'choices' ({len(choices)}).\")\n\n    df_copy = df.copy()\n    df_copy[new_col_name] = np.select(conditions, choices, default=default)\n\n    return df_copy", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 5, "titulo": "scale_features", "categoria": "Engenharia e Transformação de Features", "subcategoria": "Escalonamento e Normalização", "descricao": "Aplica escalonamento (Z-Score ou Min-Max), treinando no df_train e aplicando em ambos.", "codigo_funcao": "def scale_features(\n    df_train: pd.DataFrame,\n    df_test: Optional[pd.DataFrame] = None,\n    cols_to_scale: Optional[List[str]] = None,\n    method: Literal['zscore', 'minmax'] = 'zscore'\n) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Union[StandardScaler, MinMaxScaler]]:\n    \"\"\"\n    Descrição: Aplica escalonamento (Z-Score ou Min-Max) em colunas numéricas,\n               treinando no df_train e aplicando em df_train e df_test.\n\n    Args:\n        df_train (pd.DataFrame): DataFrame de treino.\n        df_test (pd.DataFrame, optional): DataFrame de teste. Se None, apenas o treino é transformado.\n        cols_to_scale (List[str], optional): Lista de colunas numéricas para escalar.\n            Se None, tenta escalar todas as colunas numéricas. Defaults to None.\n        method (Literal['zscore', 'minmax'], optional): Método de escalonamento.\n            'zscore': Usa StandardScaler (média 0, desvio padrão 1).\n            'minmax': Usa MinMaxScaler (escala para [0, 1]).\n            Defaults to 'zscore'.\n\n    Returns:\n        Tuple[pd.DataFrame, Optional[pd.DataFrame], Union[StandardScaler, MinMaxScaler]]:\n            (df_train_scaled, df_test_scaled, scaler_object)\n            Os DataFrames são cópias com as colunas escaladas. df_test_scaled é None se df_test não for fornecido.\n            O objeto scaler treinado também é retornado.\n\n    Raises:\n        TypeError: Se 'df_train' ou 'df_test' não forem DataFrames (se fornecido).\n        ValueError: Se 'method' for inválido, alguma coluna em 'cols_to_scale' não existir,\n                    ou se nenhuma coluna numérica for encontrada/especificada.\n    \"\"\"\n    if not isinstance(df_train, pd.DataFrame):\n        raise TypeError(\"'df_train' deve ser um DataFrame pandas.\")\n    if df_test is not None and not isinstance(df_test, pd.DataFrame):\n         raise TypeError(\"'df_test' deve ser um DataFrame pandas ou None.\")\n    if method not in ['zscore', 'minmax']:\n        raise ValueError(\"Método inválido. Escolha 'zscore' ou 'minmax'.\")\n\n    if cols_to_scale is None:\n        cols_to_scale = df_train.select_dtypes(include=np.number).columns.tolist()\n        if not cols_to_scale:\n             raise ValueError(\"Nenhuma coluna numérica encontrada para escalar e 'cols_to_scale' não foi fornecido.\")\n        print(f\"Aviso: 'cols_to_scale' não fornecido. Escalando colunas numéricas: {cols_to_scale}\")\n    else:\n        if not isinstance(cols_to_scale, list):\n             raise TypeError(\"'cols_to_scale' deve ser uma lista de nomes de colunas.\")\n        for df, name in [(df_train, 'df_train'), (df_test, 'df_test')]:\n             if df is not None:\n                 non_existent = [col for col in cols_to_scale if col not in df.columns]\n                 if non_existent:\n                     raise ValueError(f\"Colunas não encontradas em '{name}': {non_existent}\")\n\n    df_train_copy = df_train.copy()\n    df_test_copy = df_test.copy() if df_test is not None else None\n\n    # Escolhe e inicializa o scaler\n    if method == 'zscore':\n        scaler = StandardScaler()\n    else: # minmax\n        scaler = MinMaxScaler()\n\n    # Fit no treino\n    try:\n        scaler.fit(df_train_copy[cols_to_scale])\n    except Exception as e:\n        raise RuntimeError(f\"Erro durante o .fit() do scaler ({method}): {e}. Verifique os dtypes das colunas.\")\n\n    # Transform em treino\n    df_train_copy[cols_to_scale] = scaler.transform(df_train_copy[cols_to_scale])\n\n    # Transform em teste (se existir)\n    if df_test_copy is not None:\n        try:\n             df_test_copy[cols_to_scale] = scaler.transform(df_test_copy[cols_to_scale])\n        except NotFittedError:\n             raise RuntimeError(\"Scaler não foi treinado antes de transformar df_test.\")\n        except Exception as e:\n             raise RuntimeError(f\"Erro durante o .transform() do scaler ({method}) em df_test: {e}\")\n\n    return df_train_copy, df_test_copy, scaler", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 6, "titulo": "select_features_by_percentile", "categoria": "Modelagem e Machine Learning", "subcategoria": "Preparação de Dados", "descricao": "Seleciona as features com base em um percentil de pontuação univariada.", "codigo_funcao": "def select_features_by_percentile(\n    X: pd.DataFrame,\n    y: Union[pd.Series, np.ndarray],\n    percentile: int = 10, \n    score_func: Callable = f_classif\n) -> pd.DataFrame:\n    \"\"\"\n    Descrição: Seleciona as features com base em um percentil de pontuação univariada.\n\n    Args:\n        X (pd.DataFrame): DataFrame de features.\n        y (Union[pd.Series, np.ndarray]): Series ou array do alvo.\n        percentile (int, optional): Percentil (0-100) de features a serem mantidas. Defaults to 10.\n        score_func (Callable, optional): Função de pontuação univariada do sklearn.feature_selection\n                                         (ex: f_classif, mutual_info_classif). Defaults to f_classif.\n\n    Returns:\n        pd.DataFrame: Um novo DataFrame contendo apenas as features selecionadas,\n                      mantendo o índice original.\n\n    Raises:\n        TypeError: Se 'X' não for um DataFrame ou 'y' não for Series/array.\n        ValueError: Se o número de amostras em X e y não for igual,\n                    ou se 'percentile' não estiver entre 0 e 100.\n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise TypeError(\"O argumento 'X' deve ser um DataFrame pandas.\")\n    if not isinstance(y, (pd.Series, np.ndarray)):\n        raise TypeError(\"O argumento 'y' deve ser uma Series pandas ou array NumPy.\")\n    if X.shape[0] != len(y):\n        raise ValueError(f\"O número de amostras em X ({X.shape[0]}) e y ({len(y)}) não é igual.\")\n    if not (0 <= percentile <= 100):\n        raise ValueError(\"O argumento 'percentile' deve estar entre 0 e 100.\")\n\n    if isinstance(y, pd.Series):\n        y_np = y.values\n    else:\n        y_np = y\n\n    selector = SelectPercentile(score_func, percentile=percentile)\n\n    try:\n        X_selected_np = selector.fit_transform(X, y_np)\n        selected_mask = selector.get_support()\n        selected_cols = X.columns[selected_mask]\n        X_selected_df = pd.DataFrame(X_selected_np, columns=selected_cols, index=X.index)\n        return X_selected_df\n\n    except Exception as e:\n        raise RuntimeError(f\"Erro durante a seleção de features: {e}\")", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 7, "titulo": "get_fclassif_stats_df", "categoria": "Modelagem e Machine Learning", "subcategoria": "Preparação de Dados", "descricao": "Calcula estatísticas F (ANOVA) e valores-p para features em relação a um alvo categórico.", "codigo_funcao": "def get_fclassif_stats_df(\n    X: pd.DataFrame,\n    y: Union[pd.Series, np.ndarray],\n    sort_by_p: bool = True\n) -> pd.DataFrame:\n    \"\"\"\n    Descrição: Calcula estatísticas F (ANOVA) e valores-p para features em relação\n               a um alvo categórico e retorna um DataFrame ordenado.\n\n    Args:\n        X (pd.DataFrame): DataFrame contendo as features (variáveis independentes, numéricas).\n        y (Union[pd.Series, np.ndarray]): Series ou array contendo a variável alvo\n                                           (dependente, categórica/discreta).\n        sort_by_p (bool, optional): Se True, ordena o DataFrame resultante pelo\n                                   'p value' (ascendente). Defaults to True.\n\n    Returns:\n        pd.DataFrame: DataFrame com as colunas ['Feature', 'F_statistic', 'p_value'].\n\n    Raises:\n        TypeError: Se 'X' não for um DataFrame ou 'y' não for Series/array.\n        ValueError: Se o número de amostras em X e y não for igual, ou se X estiver vazio.\n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise TypeError(\"O argumento 'X' deve ser um DataFrame pandas.\")\n    if not isinstance(y, (pd.Series, np.ndarray)):\n        raise TypeError(\"O argumento 'y' deve ser uma Series pandas ou array NumPy.\")\n    if X.empty:\n         raise ValueError(\"DataFrame 'X' de features não pode estar vazio.\")\n    if X.shape[0] != len(y):\n        raise ValueError(f\"O número de amostras em X ({X.shape[0]}) e y ({len(y)}) não é igual.\")\n\n    # Garantir que y seja np.ndarray para sklearn\n    if isinstance(y, pd.Series):\n        y_np = y.values\n    else:\n        y_np = y\n\n    feature_names = X.columns.tolist()\n\n    try:\n        f_stats, p_values = f_classif(X, y_np)\n\n        # Tratar possíveis NaNs ou Infs retornados por f_classif (raro, mas possível)\n        f_stats = np.nan_to_num(f_stats, nan=-1.0, posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n        p_values = np.nan_to_num(p_values, nan=1.0) # p-value NaN geralmente indica irrelevância\n\n        f_test_df = pd.DataFrame({\n            'Feature': feature_names,\n            'F_statistic': f_stats,\n            'p_value': p_values\n        })\n\n        if sort_by_p:\n            f_test_df = f_test_df.sort_values('p_value')\n\n        return f_test_df\n\n    except Exception as e:\n        raise RuntimeError(f\"Erro ao calcular f_classif: {e}\")", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 8, "titulo": "perform_train_test_split", "categoria": "Modelagem e Machine Learning", "subcategoria": "Preparação de Dados", "descricao": "Divide DataFrames em subconjuntos de treino e teste, com opção de estratificação.", "codigo_funcao": "def perform_train_test_split(\n    X: Union[np.ndarray, pd.DataFrame],\n    y: Union[np.ndarray, pd.Series],\n    test_size: float = 0.2,\n    random_state: Optional[int] = 42,\n    stratify: Optional[Union[np.ndarray, pd.Series]] = None\n) -> Tuple[Any, Any, Any, Any]:\n    \"\"\"\n    Descrição: Divide arrays ou DataFrames em subconjuntos aleatórios de treino e teste,\n               com opção de estratificação. Preserva o tipo (DataFrame/array).\n\n    Args:\n        X (Union[np.ndarray, pd.DataFrame]): As features a serem divididas.\n        y (Union[np.ndarray, pd.Series]): O alvo (target) a ser dividido.\n        test_size (float, optional): Proporção do dataset a ser incluída no split de teste.\n                                     Defaults to 0.2.\n        random_state (Optional[int], optional): Controla o embaralhamento. Defaults to 42.\n        stratify (Optional[Union[np.ndarray, pd.Series]], optional): Se não for None, os dados\n            são divididos de forma estratificada usando este array/Series como rótulos de classe.\n            Normalmente, passa-se a própria variável 'y' aqui para classificação. Defaults to None.\n\n    Returns:\n        Tuple[Any, Any, Any, Any]: Uma tupla contendo (X_train, X_test, y_train, y_test),\n                                   mantendo o tipo original de X e y.\n\n    Raises:\n        ValueError: Se 'X' e 'y' tiverem um número de amostras inconsistente,\n                    ou se a estratificação falhar (ex: classes com < 2 membros).\n        TypeError: Se 'X' ou 'y' não forem tipos suportados (array/DataFrame/Series).\n    \"\"\"\n    if not isinstance(X, (np.ndarray, pd.DataFrame)):\n        raise TypeError(\"O argumento 'X' deve ser um array NumPy ou DataFrame pandas.\")\n    if not isinstance(y, (np.ndarray, pd.Series)):\n        raise TypeError(\"O argumento 'y' deve ser um array NumPy ou Series pandas.\")\n    if X.shape[0] != len(y): # Usar len(y) funciona para ambos Series e np.ndarray\n        raise ValueError(f\"O número de amostras em X ({X.shape[0]}) e y ({len(y)}) não é igual.\")\n\n    try:\n        # train_test_split preserva o tipo (DataFrame/Series) se a entrada for desse tipo\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y,\n            test_size=test_size,\n            random_state=random_state,\n            stratify=stratify\n        )\n        return X_train, X_test, y_train, y_test\n    except ValueError as e:\n        if \"stratify\" in str(e):\n             raise ValueError(\"Falha na estratificação. Para 'stratify=y' em classificação, \"\n                             \"certifique-se de que cada classe tenha pelo menos 2 membros (ou mais, dependendo de test_size).\")\n        else:\n            raise e # Outro ValueError\n    except Exception as e:\n        raise RuntimeError(f\"Erro inesperado durante train_test_split: {e}\")", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 9, "titulo": "train_sklearn_model", "categoria": "Modelagem e Machine Learning", "subcategoria": "Treinamento de Modelos", "descricao": "Treina (fit) um modelo scikit-learn clonado usando os dados fornecidos.", "codigo_funcao": "def train_sklearn_model(model: BaseEstimator, X_train: np.ndarray, y_train: np.ndarray) -> BaseEstimator:\n    \"\"\"\n    Treina (fit) um modelo scikit-learn (ou compatível) usando os dados fornecidos.\n\n    Args:\n        model (BaseEstimator): O objeto do modelo scikit-learn *não* treinado.\n                               A função clona o modelo antes de treinar para evitar\n                               efeitos colaterais no objeto original.\n        X_train (np.ndarray): Array NumPy (ou compatível) de features de treinamento.\n        y_train (np.ndarray): Array NumPy (ou compatível) do target de treinamento.\n\n    Returns:\n        BaseEstimator: O objeto do modelo treinado (fitted). É uma cópia do modelo original.\n\n    Raises:\n        TypeError: Se 'model' não for um estimador sklearn válido (não tiver o método .fit),\n                   ou se X_train/y_train não forem adequados para o fit.\n        ValueError: Se 'X_train' e 'y_train' tiverem um número incompatível de amostras.\n    \"\"\"\n    if not hasattr(model, 'fit'):\n        raise TypeError(\"O 'model' fornecido não possui um método .fit().\")\n    # Validação de X_train e y_train é feita implicitamente pelo .fit()\n\n    try:\n        # Clonar o modelo para evitar modificar o original passado como argumento\n        model_to_fit = clone(model)\n        model_to_fit.fit(X_train, y_train)\n        return model_to_fit\n    except ValueError as e:\n         if \"inconsistent numbers of samples\" in str(e):\n              raise ValueError(f\"Incompatibilidade de shapes: X_train ({X_train.shape[0]} amostras) \"\n                               f\"e y_train ({len(y_train)} amostras)\")\n         else:\n              raise e # Outro ValueError do fit\n    except Exception as e:\n        raise RuntimeError(f\"Erro durante o treinamento do modelo: {e}\")", "bibliotecas": ["numpy", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 10, "titulo": "get_predictions", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Gera predições (labels) e/ou probabilidades a partir de um estimador treinado.", "codigo_funcao": "def get_predictions(\n    estimator: Any,\n    X: Union[np.ndarray, pd.DataFrame],\n    return_proba: bool = True,\n    threshold: Optional[float] = 0.5,\n    positive_class_index: int = 1\n) -> Dict[str, Optional[np.ndarray]]:\n    \"\"\"\n    Descrição: Gera predições (labels) e/ou probabilidades a partir de um estimador treinado.\n               Opcionalmente, binariza probabilidades usando um threshold.\n\n    Args:\n        estimator (Any): Estimador treinado com método `predict` e opcionalmente `predict_proba`.\n        X (Union[np.ndarray, pd.DataFrame]): Features para predição.\n        return_proba (bool, optional): Se True, tenta obter `predict_proba`. Default: True.\n        threshold (Optional[float], optional): Limiar para converter probabilidades em\n            rótulos binários (0 ou 1). Se None, não faz binarização. Default: 0.5.\n        positive_class_index (int, optional): Índice da classe positiva nas probabilidades\n            retornadas por `predict_proba` (usado para binarização). Default: 1.\n\n    Returns:\n        Dict[str, Optional[np.ndarray]]: Dicionário contendo:\n            'y_pred': np.ndarray com os rótulos preditos por `.predict()`.\n            'y_proba': np.ndarray com as probabilidades de `.predict_proba()` (ou None se não disponível/solicitado).\n            'y_proba_positive': np.ndarray 1D com probs da classe positiva (ou None).\n            'y_pred_binary': np.ndarray com rótulos binários (0/1) baseados no threshold (ou None).\n\n    Raises:\n        AttributeError: Se o estimador não implementar `.predict()`.\n        TypeError: Se X não for array NumPy ou DataFrame.\n        ValueError: Se o threshold for inválido ou o índice da classe positiva for inválido.\n    \"\"\"\n    if not hasattr(estimator, \"predict\"):\n        raise AttributeError(\"O estimador precisa implementar 'predict'\")\n    if not isinstance(X, (np.ndarray, pd.DataFrame)):\n         raise TypeError(\"X deve ser um array NumPy ou DataFrame pandas.\")\n\n    y_pred = None\n    y_proba = None\n    y_proba_positive = None\n    y_pred_binary = None\n\n    try:\n        y_pred = np.asarray(estimator.predict(X))\n    except Exception as e:\n        raise RuntimeError(f\"Erro ao chamar estimator.predict(X): {e}\")\n\n    if return_proba and hasattr(estimator, \"predict_proba\"):\n        try:\n            y_proba = np.asarray(estimator.predict_proba(X))\n            if y_proba.ndim == 2 and y_proba.shape[1] > positive_class_index:\n                y_proba_positive = y_proba[:, positive_class_index]\n            elif y_proba.ndim == 1:\n                y_proba_positive = y_proba\n            else:\n                 print(f\"Aviso: Formato de y_proba ({y_proba.shape}) não esperado para extrair classe positiva no índice {positive_class_index}.\")\n        except Exception as e:\n            print(f\"Aviso: Erro ao chamar estimator.predict_proba(X): {e}. Probabilidades não serão retornadas.\")\n            y_proba = None\n            y_proba_positive = None\n\n    if threshold is not None and y_proba_positive is not None:\n         if not isinstance(threshold, (int, float)) or not (0 <= threshold <= 1):\n             raise ValueError(\"Threshold deve ser um número entre 0 e 1.\")\n         try:\n             y_pred_binary = (y_proba_positive >= threshold).astype(int)\n         except Exception as e:\n              raise RuntimeError(f\"Erro ao aplicar threshold nas probabilidades: {e}\")\n\n    return {\n        \"y_pred\": y_pred,\n        \"y_proba\": y_proba,\n        \"y_proba_positive\": y_proba_positive,\n        \"y_pred_binary\": y_pred_binary\n    }", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 11, "titulo": "calculate_classification_metrics", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Calcula um conjunto abrangente de métricas de classificação (accuracy, F1, ROC AUC, etc.).", "codigo_funcao": "def calculate_classification_metrics(\n    y_true: Union[np.ndarray, pd.Series],\n    y_pred: Union[np.ndarray, pd.Series],\n    y_proba_positive: Optional[Union[np.ndarray, pd.Series]] = None,\n    average: str = \"binary\", # 'binary' para binário, 'macro'/'weighted' para multi-classe\n    pos_label: Any = 1,\n    include_pr_curve: bool = True # Flag para incluir curva PR (pode ser grande)\n) -> Dict[str, Any]:\n    \"\"\"\n    Descrição: Calcula um conjunto abrangente de métricas de classificação.\n\n    Args:\n        y_true (Union[np.ndarray, pd.Series]): Rótulos verdadeiros (0 ou 1 para binário).\n        y_pred (Union[np.ndarray, pd.Series]): Rótulos preditos pelo modelo.\n        y_proba_positive (Optional[Union[np.ndarray, pd.Series]], optional):\n            Probabilidades preditas para a classe positiva (classe `pos_label`).\n            Necessário para ROC AUC e Curva PR. Shape (n_samples,). Defaults to None.\n        average (str, optional): Estratégia de média para precision, recall, f1 em\n            problemas multiclasse ('macro', 'micro', 'weighted'). Para binário, use 'binary'.\n            Defaults to \"binary\".\n        pos_label (Any, optional): O rótulo da classe considerada positiva em problemas binários.\n            Ignorado se average != 'binary'. Defaults to 1.\n        include_pr_curve (bool, optional): Se True e y_proba_positive for fornecido,\n            calcula os dados da curva Precision-Recall. Defaults to True.\n\n    Returns:\n        Dict[str, Any]: Dicionário com métricas: 'accuracy', 'precision', 'recall', 'f1',\n                        'mcc' (Matthews Corr Coef), 'roc_auc' (se y_proba_positive),\n                        'confusion_matrix' (lista de listas), 'classification_report_dict',\n                        'pr_curve_data' (se calculado, dict com 'precision', 'recall', 'thresholds').\n\n    Raises:\n        TypeError: Se y_true ou y_pred não forem array-like.\n        ValueError: Se shapes forem incompatíveis ou average/pos_label inválidos.\n    \"\"\"\n    # Garantir que sejam arrays numpy para sklearn\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError(f\"y_true ({y_true.shape[0]}) e y_pred ({y_pred.shape[0]}) devem ter mesmo número de amostras\")\n\n    out: Dict[str, Any] = {} # Type hint para o dicionário\n\n    # Métricas baseadas em y_pred\n    try:\n        out['accuracy'] = accuracy_score(y_true, y_pred)\n        # Usar pos_label apenas se average='binary'\n        pl = pos_label if average == 'binary' else None\n        out['precision'] = precision_score(y_true, y_pred, average=average, zero_division=0, pos_label=pl)\n        out['recall'] = recall_score(y_true, y_pred, average=average, zero_division=0, pos_label=pl)\n        out['f1'] = f1_score(y_true, y_pred, average=average, zero_division=0, pos_label=pl)\n        out['mcc'] = matthews_corrcoef(y_true, y_pred)\n        out['confusion_matrix'] = confusion_matrix(y_true, y_pred).tolist() # Para ser serializável\n        out['classification_report_dict'] = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n    except Exception as e:\n        print(f\"Erro ao calcular métricas baseadas em y_pred: {e}\")\n        # Preencher com NaN ou valores padrão? Ou deixar ausente?\n        out.update({'accuracy': np.nan, 'precision': np.nan, 'recall': np.nan, 'f1': np.nan, 'mcc': np.nan})\n        if 'confusion_matrix' not in out: out['confusion_matrix'] = None\n        if 'classification_report_dict' not in out: out['classification_report_dict'] = None\n\n\n    # Métricas baseadas em y_proba_positive\n    if y_proba_positive is not None:\n        y_proba_positive = np.asarray(y_proba_positive)\n        if y_true.shape[0] != y_proba_positive.shape[0]:\n             print(f\"Aviso: y_true ({y_true.shape[0]}) e y_proba_positive ({y_proba_positive.shape[0]}) têm tamanhos diferentes. Pulando métricas baseadas em score.\")\n        else:\n            # ROC AUC\n            try:\n                # roc_auc_score lida com multiclasse se y_proba tiver shape (n, n_classes)\n                # Mas aqui esperamos y_proba_positive (n,), então é para binário ou OVR implícito\n                 out['roc_auc'] = float(roc_auc_score(y_true, y_proba_positive))\n            except Exception as e:\n                print(f\"Erro ao calcular ROC AUC: {e}\")\n                out['roc_auc'] = np.nan\n\n            # Precision-Recall Curve\n            if include_pr_curve:\n                try:\n                    precision, recall, thresholds = precision_recall_curve(y_true, y_proba_positive, pos_label=pos_label)\n                    # Adicionar threshold final para alinhar tamanhos P/R\n                    out['pr_curve_data'] = {\"precision\": precision.tolist(), \"recall\": recall.tolist(), \"thresholds\": np.append(thresholds, np.nan).tolist()}\n                except Exception as e:\n                    print(f\"Erro ao calcular curva PR: {e}\")\n                    out['pr_curve_data'] = None\n    else:\n        out['roc_auc'] = None\n        out['pr_curve_data'] = None\n\n    return out", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 12, "titulo": "execute_cross_val_score", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Executa cross-validation e retorna as pontuações de cada fold para uma métrica específica.", "codigo_funcao": "def execute_cross_val_score(\n    model: BaseEstimator,\n    X: np.ndarray,\n    y: np.ndarray,\n    n_splits: int = 5, # Default mais comum\n    scoring: str = 'roc_auc',\n    stratify: bool = True, # Adicionar opção de estratificar\n    shuffle: bool = True,\n    random_state: Optional[int] = 42\n) -> np.ndarray:\n    \"\"\"\n    Descrição: Executa cross-validation (usando sklearn's cross_val_score) e retorna\n               as pontuações de cada fold para uma métrica específica.\n\n    Args:\n        model (BaseEstimator): Modelo scikit-learn (ou compatível) a ser avaliado.\n                               Será clonado para cada fold.\n        X (np.ndarray): Array NumPy de features.\n        y (np.ndarray): Array NumPy de alvo.\n        n_splits (int, optional): Número de folds para a validação cruzada. Defaults to 5.\n        scoring (str, optional): A métrica de pontuação (string válida para sklearn).\n                                 Defaults to 'roc_auc'.\n        stratify (bool, optional): Se True, usa StratifiedKFold (bom para classificação\n                                   desbalanceada). Se False, usa KFold. Defaults to True.\n        shuffle (bool, optional): Se deve embaralhar os dados antes de dividir em folds.\n                                  Defaults to True.\n        random_state (Optional[int], optional): Seed para reprodutibilidade do shuffle.\n                                               Defaults to 42.\n\n    Returns:\n        np.ndarray: Array com as pontuações da métrica 'scoring' para cada fold.\n\n    Raises:\n        TypeError: Se 'X' ou 'y' não forem arrays NumPy.\n        ValueError: Se n_splits <= 1 ou shapes incompatíveis.\n        Exception: Se `cross_val_score` falhar (ex: `scoring` inválido).\n    \"\"\"\n    if not all(isinstance(arr, np.ndarray) for arr in [X, y]):\n        raise TypeError(\"'X' e 'y' devem ser arrays NumPy.\")\n    if X.shape[0] != y.shape[0]:\n         raise ValueError(f\"Incompatibilidade de amostras: X ({X.shape[0]}) e y ({y.shape[0]})\")\n    if not isinstance(n_splits, int) or n_splits <= 1:\n        raise ValueError(\"'n_splits' deve ser um inteiro maior que 1.\")\n\n    # Escolher o tipo de KFold\n    if stratify:\n        # StratifiedKFold precisa de y para estratificação, mas cross_val_score lida com isso\n        cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n    else:\n        cv_splitter = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n\n    try:\n        # cross_val_score clona o modelo internamente\n        cv_scores = cross_val_score(model, X, y, cv=cv_splitter, scoring=scoring, n_jobs=-1) # Usar n_jobs\n        return cv_scores\n    except Exception as e:\n        raise Exception(f\"Erro durante cross_val_score com scoring='{scoring}': {e}\")", "bibliotecas": ["numpy", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 13, "titulo": "run_grid_search_cv", "categoria": "Modelagem e Machine Learning", "subcategoria": "Treinamento de Modelos", "descricao": "Executa GridSearchCV para encontrar os melhores hiperparâmetros e retorna o objeto treinado.", "codigo_funcao": "def run_grid_search_cv(\n    model: BaseEstimator,\n    X: np.ndarray,\n    y: np.ndarray,\n    param_grid: Dict[str, Any],\n    cv: int = 5,\n    scoring: str = 'roc_auc',\n    stratify: bool = True,\n    shuffle: bool = True,\n    random_state: Optional[int] = 42,\n    n_jobs: int = -1,\n    # random_search_iterations: Optional[int] = None # Para adicionar RandomizedSearchCV depois\n) -> GridSearchCV: # Por enquanto, focado em GridSearchCV\n    \"\"\"\n    Descrição: Executa GridSearchCV para encontrar os melhores hiperparâmetros\n               para um modelo e retorna o objeto GridSearchCV treinado.\n\n    Args:\n        model (BaseEstimator): O estimador scikit-learn (não treinado).\n        X (np.ndarray): Array NumPy de features de treino.\n        y (np.ndarray): Array NumPy de alvo de treino.\n        param_grid (Dict[str, Any]): Dicionário com os hiperparâmetros a testar\n            (formato esperado por GridSearchCV).\n        cv (int, optional): Número de folds para a validação cruzada interna. Defaults to 5.\n        scoring (str, optional): Métrica de pontuação para avaliar os parâmetros. Defaults to 'roc_auc'.\n        stratify (bool, optional): Se True, usa StratifiedKFold na CV interna. Defaults to True.\n        shuffle (bool, optional): Se deve embaralhar os dados para a CV interna. Defaults to True.\n        random_state (Optional[int], optional): Seed para shuffle. Defaults to 42.\n        n_jobs (int, optional): Número de jobs a rodar em paralelo (-1 usa todos os processadores). Defaults to -1.\n\n    Returns:\n        GridSearchCV: O objeto GridSearchCV treinado (fitted). Contém atributos como\n                      .best_params_, .best_score_, .best_estimator_, .cv_results_.\n\n    Raises:\n        TypeError: Se 'X' ou 'y' não forem arrays NumPy.\n        ValueError: Se parâmetros como 'cv' forem inválidos ou shapes incompatíveis.\n        Exception: Se o .fit() do GridSearchCV falhar.\n    \"\"\"\n    if not all(isinstance(arr, np.ndarray) for arr in [X, y]):\n        raise TypeError(\"'X' e 'y' devem ser arrays NumPy.\")\n    if X.shape[0] != y.shape[0]:\n         raise ValueError(f\"Incompatibilidade de amostras: X ({X.shape[0]}) e y ({y.shape[0]})\")\n    if not isinstance(cv, int) or cv <= 1:\n        raise ValueError(\"'cv' deve ser um inteiro maior que 1.\")\n\n    # Configurar o splitter da CV interna\n    if stratify:\n        cv_splitter = StratifiedKFold(n_splits=cv, shuffle=shuffle, random_state=random_state)\n    else:\n        cv_splitter = KFold(n_splits=cv, shuffle=shuffle, random_state=random_state)\n\n    gs = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        scoring=scoring,\n        cv=cv_splitter,\n        n_jobs=n_jobs,\n        refit=True # Garante que best_estimator_ seja treinado no final\n    )\n\n    try:\n        gs.fit(X, y)\n        return gs\n    except Exception as e:\n        raise Exception(f\"Falha no .fit() do GridSearchCV: {e}\")", "bibliotecas": ["numpy", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 14, "titulo": "get_feature_importances", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Extrai e ordena as importâncias de features de um modelo treinado (ex: RandomForest).", "codigo_funcao": "def get_feature_importances(model: BaseEstimator, feature_names: List[str]) -> pd.Series:\n    \"\"\"\n    Extrai as importâncias de features (atributo .feature_importances_) de um\n    modelo treinado (baseado em árvore ou similar) e retorna como uma Série ordenada.\n\n    Args:\n        model (BaseEstimator): Um modelo scikit-learn treinado que expõe o atributo\n                               `.feature_importances_` (ex: RandomForest, GradientBoosting,\n                               LGBMClassifier, ou o `.best_estimator_` de um GridSearchCV).\n        feature_names (List[str]): Lista dos nomes das features *na mesma ordem*\n                                   que foram usadas para treinar o modelo.\n\n    Returns:\n        pd.Series: Série pandas com nomes das features como índice e importâncias\n                   como valores, ordenada decrescentemente pela importância.\n\n    Raises:\n        AttributeError: Se o objeto 'model' não possuir o atributo '.feature_importances_'.\n        NotFittedError: Se o modelo parecer não estar treinado (acesso ao atributo falha).\n        ValueError: Se o número de 'feature_names' não corresponder ao número de\n                    importâncias encontradas no modelo.\n        TypeError: Se 'feature_names' não for uma lista.\n    \"\"\"\n    if not hasattr(model, 'feature_importances_'):\n        raise AttributeError(\"O objeto 'model' fornecido não possui o atributo '.feature_importances_'. \"\n                             \"É um modelo baseado em árvore (ou similar) e está treinado?\")\n    if not isinstance(feature_names, list):\n         raise TypeError(\"'feature_names' deve ser uma lista de strings.\")\n\n    try:\n        # Acessa o atributo. Pode falhar se o modelo for 'prefit' mas não foi treinado.\n        importances = model.feature_importances_\n    except NotFittedError as e:\n         raise NotFittedError(f\"Modelo não parece estar treinado. Erro ao acessar .feature_importances_: {e}\")\n    except Exception as e:\n        raise RuntimeError(f\"Erro inesperado ao acessar .feature_importances_: {e}\")\n\n    if len(feature_names) != len(importances):\n        raise ValueError(f\"O número de feature_names ({len(feature_names)}) não \"\n                         f\"corresponde ao número de importâncias ({len(importances)}) retornado pelo modelo.\")\n\n    # Criar e ordenar a Série\n    feat_imp_series = pd.Series(importances, index=feature_names)\n    return feat_imp_series.sort_values(ascending=False)", "bibliotecas": ["pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 15, "titulo": "calculate_shap_values", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Calcula os valores SHAP usando um explainer pré-construído e um conjunto de dados.", "codigo_funcao": "def calculate_shap_values(\n    explainer: Any, # Objeto explainer do SHAP (ex: shap.TreeExplainer)\n    X_data: Union[pd.DataFrame, np.ndarray],\n    check_additivity: bool = False \n) -> Optional[Union[np.ndarray, List[np.ndarray]]]:\n    \"\"\"\n    Calcula os valores SHAP (SHapley Additive exPlanations) usando um explainer\n    SHAP pré-construído e um conjunto de dados.\n\n    Args:\n        explainer (Any): Um objeto explainer SHAP (ex: shap.TreeExplainer, shap.KernelExplainer)\n                         que já foi inicializado com um modelo ou função de predição.\n        X_data (Union[pd.DataFrame, np.ndarray]): Os dados (features) para os quais\n                         os valores SHAP devem ser calculados. Recomenda-se usar um\n                         DataFrame pandas se possível para manter nomes de features.\n        check_additivity (bool, optional): Se True, realiza uma verificação de consistência\n                                          interna nos valores SHAP (pode ser lento).\n                                          Defaults to False.\n\n    Returns:\n        Optional[Union[np.ndarray, List[np.ndarray]]]: Os valores SHAP.\n            - Para regressão ou classificação binária com TreeExplainer, geralmente um array (n_amostras, n_features).\n            - Para classificação multiclasse com TreeExplainer, geralmente uma lista de arrays\n              [shap_classe_0, shap_classe_1, ...].\n            - Para KernelExplainer, pode variar.\n            - Retorna None se a biblioteca SHAP não estiver instalada.\n\n    Raises:\n        ImportError: Se a biblioteca 'shap' não estiver instalada.\n        TypeError: Se o 'explainer' não tiver o método `.shap_values()`.\n        RuntimeError: Se o cálculo do SHAP falhar (ex: incompatibilidade de dados/modelo/explainer).\n    \"\"\"\n    if shap is None:\n        raise ImportError(\"Biblioteca 'shap' não está instalada. Execute 'pip install shap'.\")\n\n    if not hasattr(explainer, 'shap_values') and not callable(explainer):\n         raise TypeError(\"O objeto 'explainer' não possui método '.shap_values()' nem é chamável como esperado.\")\n\n    try:\n        if hasattr(explainer, 'shap_values') and callable(explainer.shap_values):\n             shap_values = explainer.shap_values(X_data, check_additivity=check_additivity)\n        elif callable(explainer):\n             explanation = explainer(X_data, check_additivity=check_additivity)\n             if hasattr(explanation, 'values'):\n                  shap_values = explanation.values\n             else: \n                  shap_values = explanation \n        else:\n              raise TypeError(\"Não foi possível chamar o explainer SHAP.\")\n\n        return shap_values\n\n    except Exception as e:\n        # Tentar dar uma dica sobre DataFrames vs NumPy arrays\n        error_msg = f\"Falha ao calcular os valores SHAP. Erro: {e}\"\n        if isinstance(X_data, np.ndarray) and hasattr(explainer, 'feature_names') and explainer.feature_names is not None:\n             error_msg += (\"\\nDica: O explainer parece esperar nomes de features. \"\n                           \"Tente passar X_data como um DataFrame pandas com colunas nomeadas.\")\n        elif isinstance(X_data, pd.DataFrame) and getattr(explainer, 'feature_names', None) is None:\n             error_msg += (\"\\nDica: O explainer pode não lidar bem com DataFrames. \"\n                           \"Tente passar X_data.values (array NumPy).\")\n\n        raise RuntimeError(error_msg)", "bibliotecas": ["numpy", "pandas", "scikit-learn", "shap", "typing"], "versao": "0.1.0"}
{"id_funcao": 16, "titulo": "calculate_financial_impact_by_threshold", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Calcula o impacto financeiro (custo, economia) para uma série de thresholds de classificação.", "codigo_funcao": "def calculate_financial_impact_by_threshold(\n    y_true: np.ndarray,\n    y_proba_positive: np.ndarray,\n    thresholds: Union[List[float], np.ndarray],\n    cost_per_intervention: float,\n    savings_per_tp: float,\n    effectiveness: float = 1.0\n) -> pd.DataFrame:\n    \"\"\"\n    Calcula o impacto financeiro (custo, economia, economia líquida)\n    para uma série de thresholds de classificação binária.\n\n    Args:\n        y_true (np.ndarray): Array NumPy de valores reais (0 ou 1).\n        y_proba_positive (np.ndarray): Array NumPy de probabilidades preditas para\n                                       a classe positiva (classe 1).\n        thresholds (Union[List[float], np.ndarray]): Lista ou array de thresholds\n                                                    (valores entre 0 e 1) a serem testados.\n        cost_per_intervention (float): O custo de aplicar a intervenção a uma\n                                       instância classificada como positiva.\n        savings_per_tp (float): A economia (valor bruto) gerada por um True Positive\n                                corretamente identificado e tratado com sucesso.\n        effectiveness (float, optional): A taxa de eficácia da intervenção (0.0 a 1.0).\n                                         A economia é multiplicada por este fator. Defaults to 1.0.\n\n    Returns:\n        pd.DataFrame: DataFrame com colunas ['threshold', 'n_interventions' (pred pos),\n                      'n_true_pos', 'total_cost', 'total_savings', 'net_savings'].\n\n    Raises:\n        ValueError: Se os comprimentos de y_true e y_proba_positive não corresponderem,\n                    ou se effectiveness estiver fora do intervalo [0, 1].\n        TypeError: Se os inputs não forem arrays NumPy ou se os custos/savings não forem numéricos.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_proba_positive, np.ndarray):\n        raise TypeError(\"'y_true' e 'y_proba_positive' devem ser arrays NumPy.\")\n    if len(y_true) != len(y_proba_positive):\n        raise ValueError(f\"Os arrays 'y_true' ({len(y_true)}) e 'y_proba_positive' ({len(y_proba_positive)}) \"\n                         \"devem ter o mesmo comprimento.\")\n    if not isinstance(thresholds, (list, np.ndarray)):\n        raise TypeError(\"'thresholds' deve ser uma lista ou array NumPy.\")\n    if not all(isinstance(v, (int, float)) for v in [cost_per_intervention, savings_per_tp, effectiveness]):\n         raise TypeError(\"Custos, savings e effectiveness devem ser numéricos.\")\n    if not (0.0 <= effectiveness <= 1.0):\n        raise ValueError(\"'effectiveness' deve estar entre 0.0 e 1.0.\")\n\n    results = []\n    y_true_bool = y_true.astype(bool) # Converter para booleano para masking\n\n    for threshold in sorted(np.unique(thresholds), reverse=True): # Ordenar para clareza\n        if not (0.0 <= threshold <= 1.0):\n            print(f\"Aviso: Threshold {threshold} fora do intervalo [0, 1]. Pulando.\")\n            continue\n\n        # 1. Predições positivas (intervenções)\n        pos_pred_mask = (y_proba_positive >= threshold) \n        n_interventions = int(np.sum(pos_pred_mask))\n\n        # 2. Custo total\n        total_cost = n_interventions * cost_per_intervention\n\n        # 3. True Positives\n        true_pos_mask = pos_pred_mask & y_true_bool\n        n_true_pos = int(np.sum(true_pos_mask))\n\n        # 4. Economia \n        total_savings = n_true_pos * savings_per_tp * effectiveness\n\n        # 5. Economia líquida\n        net_savings = total_savings - total_cost\n\n        results.append({\n            'threshold': threshold,\n            'n_interventions': n_interventions,\n            'n_true_pos': n_true_pos,\n            'total_cost': total_cost,\n            'total_savings': total_savings,\n            'net_savings': net_savings\n        })\n\n    return pd.DataFrame(results)", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 17, "titulo": "calculate_precision_recall_from_counts", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Calcula Precisão e Recall a partir de contagens agregadas (TP, predições positivas).", "codigo_funcao": "def calculate_precision_recall_from_counts(\n    n_true_pos: Union[int, np.ndarray],\n    n_pos_pred: Union[int, np.ndarray], \n    n_total_real_pos: int\n) -> Tuple[Union[float, np.ndarray], Union[float, np.ndarray]]:\n    \"\"\"\n    Calcula a Precisão e o Recall a partir de contagens agregadas (ex: por threshold).\n\n    Precisão = TP / (TP + FP) = n_true_pos / n_pos_pred\n    Recall = TP / (TP + FN) = n_true_pos / n_total_real_pos\n\n    Args:\n        n_true_pos (Union[int, np.ndarray]): Número de True Positives. Pode ser array (p/ vários thresholds).\n        n_pos_pred (Union[int, np.ndarray]): Número total de Predições Positivas (TP + FP). Pode ser array.\n        n_total_real_pos (int): Número total de positivos reais (TP + FN) no dataset inteiro. Deve ser > 0.\n\n    Returns:\n        Tuple[Union[float, np.ndarray], Union[float, np.ndarray]]: (precision, recall).\n        Retorna np.nan para precisão se n_pos_pred for 0.\n        Retorna np.nan para recall se n_total_real_pos for 0 (embora validado).\n\n    Raises:\n        ValueError: Se n_total_real_pos for <= 0.\n        TypeError: Se as entradas numéricas não forem int ou np.ndarray.\n    \"\"\"\n    if not isinstance(n_total_real_pos, int) or n_total_real_pos <= 0:\n        raise ValueError(\"'n_total_real_pos' deve ser um inteiro positivo.\")\n    if not isinstance(n_true_pos, (int, np.ndarray)):\n         raise TypeError(\"'n_true_pos' deve ser int ou np.ndarray.\")\n    if not isinstance(n_pos_pred, (int, np.ndarray)):\n         raise TypeError(\"'n_pos_pred' deve ser int ou np.ndarray.\")\n\n    # Garantir que sejam arrays numpy para divisão segura\n    n_tp = np.asarray(n_true_pos)\n    n_pp = np.asarray(n_pos_pred)\n\n    # Recall\n    # Se n_total_real_pos é 0 \n    if n_total_real_pos == 0:\n        recall = np.full_like(n_tp, np.nan, dtype=np.float64)\n    else:\n        recall = n_tp / n_total_real_pos\n\n    # Precision - Usar np.divide para tratar n_pp == 0\n    precision = np.divide(n_tp, n_pp,\n                          out=np.full_like(n_tp, np.nan, dtype=np.float64), # Output padrão se where=False\n                          where=(n_pp != 0))\n\n    # Se a entrada foi int, retorna float, senão retorna array\n    if isinstance(n_true_pos, int) and isinstance(n_pos_pred, int):\n        return float(precision), float(recall)\n    else:\n        return precision, recall", "bibliotecas": ["numpy", "typing"], "versao": "0.1.0"}
{"id_funcao": 18, "titulo": "fit_calibrated_classifier_cv", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Treina um CalibratedClassifierCV para calibrar as probabilidades de um modelo base.", "codigo_funcao": "def fit_calibrated_classifier_cv(\n    base_estimator: BaseEstimator,\n    X_train: Union[np.ndarray, pd.DataFrame],\n    y_train: Union[np.ndarray, pd.Series],\n    method: Literal['sigmoid', 'isotonic'] = 'sigmoid',\n    cv: Union[int, Literal['prefit']] = 5 #\n) -> CalibratedClassifierCV:\n    \"\"\"\n    Treina (fit) um CalibratedClassifierCV, que envolve um classificador base\n    para calibrar suas probabilidades preditas (usando Sigmoid/Platt ou Isotonic).\n\n    Args:\n        base_estimator (BaseEstimator): O modelo classificador base (ex: RandomForestClassifier).\n            - Se cv for um inteiro, este estimador NÃO deve ser treinado (será clonado e treinado internamente).\n            - Se cv='prefit', este estimador JÁ deve ter sido treinado nos dados X_train, y_train.\n        X_train (Union[np.ndarray, pd.DataFrame]): Dados de treino (features).\n        y_train (Union[np.ndarray, pd.Series]): Dados de treino (alvo).\n        method (Literal['sigmoid', 'isotonic'], optional): O método de calibração.\n            'sigmoid': Calibração de Platt (assume curva em S).\n            'isotonic': Regressão isotônica (não paramétrica, requer mais dados).\n            Defaults to 'sigmoid'.\n        cv (Union[int, Literal['prefit']], optional): Estratégia de validação cruzada para treinar\n            o calibrador.\n            - int: Número de folds (o estimador base é treinado em k-1 folds e o calibrador no k-ésimo).\n            - 'prefit': Assume que `base_estimator` já está treinado e usa todos os dados (X_train, y_train)\n                       para treinar apenas o calibrador.\n            Defaults to 5.\n\n    Returns:\n        CalibratedClassifierCV: O objeto classificador calibrado e treinado. Pode ser usado\n                                para `.predict()` e `.predict_proba()` como um classificador normal.\n\n    Raises:\n        TypeError: Se `base_estimator` não for um classificador ou `X_train`/`y_train` não forem array-like.\n        ValueError: Se `method` for inválido, `cv` for inválido, ou se `cv='prefit'` e o\n                    estimador base não parecer treinado.\n        Exception: Para outros erros durante o treinamento interno.\n    \"\"\"\n    if not is_classifier(base_estimator):\n        raise TypeError(\"'base_estimator' deve ser um classificador do scikit-learn.\")\n    if method not in ['sigmoid', 'isotonic']:\n        raise ValueError(f\"Método de calibração '{method}' inválido. Escolha 'sigmoid' ou 'isotonic'.\")\n    if not isinstance(cv, int) and cv != 'prefit':\n         raise ValueError(f\"Argumento 'cv' inválido: {cv}. Deve ser um inteiro >= 2 ou 'prefit'.\")\n    if isinstance(cv, int) and cv < 2:\n         raise ValueError(\"Se 'cv' for um inteiro, deve ser >= 2.\")\n\n    # Validação básica de X e y (tipos e compatibilidade de tamanho serão checados pelo .fit())\n    if not hasattr(X_train, 'shape') or not hasattr(y_train, 'shape'):\n         raise TypeError(\"'X_train' e 'y_train' devem ser array-like (NumPy array, pandas DataFrame/Series).\")\n    # Checagem mínima de tamanho (mais robusta seria feita pelo fit)\n    try:\n         if X_train.shape[0] != len(y_train):\n              raise ValueError(f\"Número de amostras inconsistente entre X_train ({X_train.shape[0]}) e y_train ({len(y_train)}).\")\n    except Exception: # Se len() não for aplicável a y_train por algum motivo\n         pass\n\n    # Se cv='prefit', verificar minimamente se o estimador parece treinado\n    if cv == 'prefit':\n         is_likely_fitted = hasattr(base_estimator, \"classes_\") or \\\n                           hasattr(base_estimator, \"coef_\") or \\\n                           hasattr(base_estimator, \"feature_importances_\")\n         if not is_likely_fitted:\n              print(\"Aviso: cv='prefit' foi usado, mas 'base_estimator' não parece \"\n                    \"ter atributos comuns de um modelo treinado (ex: 'classes_'). \"\n                    \"Certifique-se de que ele foi treinado corretamente antes.\")\n\n\n    try:\n        # ensemble=False significa que um único calibrador é treinado (com CV se cv=int)\n        # ensemble=True treinaria múltiplos calibradores e faria a média (não é o padrão)\n        calib_clf = CalibratedClassifierCV(base_estimator, method=method, cv=cv, ensemble=False, n_jobs=-1)\n        calib_clf.fit(X_train, y_train)\n        return calib_clf\n\n    except ValueError as e:\n        if \"Needs to be required by the estimator\" in str(e) and cv=='prefit':\n             raise ValueError(\"Erro com cv='prefit'. O estimador base pode não ter sido treinado ou não suporta predict_proba/decision_function.\")\n        elif \"should be an integer\" in str(e):\n             raise ValueError(f\"Erro com o argumento 'cv': {e}\")\n        else:\n             raise ValueError(f\"Erro ao treinar CalibratedClassifierCV. Verifique \"\n                             f\"formatos de X/y, o estado 'cv'/'base_estimator', e se o estimador base \"\n                             f\"suporta predict_proba (para sigmoid) ou decision_function. Erro original: {e}\")\n    except Exception as e:\n        raise RuntimeError(f\"Erro inesperado ao treinar CalibratedClassifierCV: {e}\")", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 19, "titulo": "plot_classification_scatter", "categoria": "Visualização de Dados", "subcategoria": "Relações Bivariadas e Multivariadas", "descricao": "Plota um scatter plot 2D para visualização de classificação binária.", "codigo_funcao": "def plot_classification_scatter(\n    X: np.ndarray,\n    y_true: np.ndarray,\n    y_pred: Optional[np.ndarray] = None, \n    decision_boundary: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n    feature_names: Optional[Tuple[str, str]] = ('Feature 1', 'Feature 2'),\n    target_names: Optional[Tuple[str, str]] = ('Classe 0', 'Classe 1'),\n    ax: Optional[plt.Axes] = None\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Plota um scatter plot 2D para visualização de classificação binária,\n               mostrando classes reais e, opcionalmente, predições e fronteira de decisão.\n\n    Args:\n        X (np.ndarray): Array NumPy de features com exatamente 2 colunas (shape: [n_samples, 2]).\n        y_true (np.ndarray): Array NumPy 1D com os rótulos verdadeiros (0 ou 1).\n        y_pred (Optional[np.ndarray], optional): Array NumPy 1D com os rótulos preditos (0 ou 1).\n            Se fornecido, marca predições corretas/incorretas ou plota círculos de predição.\n            Defaults to None.\n        decision_boundary (Optional[Tuple[np.ndarray, np.ndarray]], optional): Tupla contendo\n            (x_coords, y_coords) da linha da fronteira de decisão. Defaults to None.\n        feature_names (Optional[Tuple[str, str]], optional): Nomes para os eixos X e Y.\n            Defaults to ('Feature 1', 'Feature 2').\n        target_names (Optional[Tuple[str, str]], optional): Nomes para as classes 0 e 1 na legenda.\n            Defaults to ('Classe 0', 'Classe 1').\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o scatter plot.\n\n    Raises:\n        TypeError: Se X, y_true, y_pred não forem arrays NumPy.\n        ValueError: Se X não tiver 2 colunas, ou se os arrays tiverem tamanhos incompatíveis.\n    \"\"\"\n    if not isinstance(X, np.ndarray) or not isinstance(y_true, np.ndarray):\n        raise TypeError(\"'X' e 'y_true' devem ser arrays NumPy.\")\n    if X.ndim != 2 or X.shape[1] != 2:\n        raise ValueError(\"'X' deve ter exatamente 2 colunas (shape: [n_samples, 2]).\")\n    if X.shape[0] != len(y_true):\n        raise ValueError(f\"Número de amostras inconsistente: X ({X.shape[0]}) vs y_true ({len(y_true)}).\")\n    if y_pred is not None:\n        if not isinstance(y_pred, np.ndarray):\n             raise TypeError(\"'y_pred' deve ser um array NumPy ou None.\")\n        if X.shape[0] != len(y_pred):\n             raise ValueError(f\"Número de amostras inconsistente: X ({X.shape[0]}) vs y_pred ({len(y_pred)}).\")\n    if decision_boundary is not None:\n         if not (isinstance(decision_boundary, tuple) and len(decision_boundary) == 2 and\n                 all(isinstance(arr, np.ndarray) for arr in decision_boundary)):\n             raise TypeError(\"'decision_boundary' deve ser uma tupla de dois arrays NumPy (x_coords, y_coords) ou None.\")\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 6))\n\n    colors = ['blue', 'red']\n    markers_true = ['o', 's'] \n    markers_pred_correct = 'v' \n    markers_pred_wrong = 'x'\n\n    # Plota pontos reais\n    for cls, marker in zip([0, 1], markers_true):\n        mask_true = (y_true == cls)\n        ax.scatter(X[mask_true, 0], X[mask_true, 1],\n                   color=colors[cls], marker=marker, label=f\"Real: {target_names[cls]}\",\n                   alpha=0.6, s=50) # s é o tamanho\n\n    # Plota indicações de predição (se fornecido)\n    if y_pred is not None:\n        correct_mask = (y_true == y_pred)\n        wrong_mask = ~correct_mask\n\n        # Marca predições incorretas com 'x' sobre o ponto real\n        ax.scatter(X[wrong_mask, 0], X[wrong_mask, 1],\n                   color='black', marker=markers_pred_wrong, s=100, # Tamanho maior para destacar erro\n                   label='Predição Incorreta')\n\n    # Plota fronteira de decisão (se fornecida)\n    if decision_boundary is not None:\n        ax.plot(decision_boundary[0], decision_boundary[1], 'k--', label='Fronteira de Decisão') # Linha tracejada preta\n\n    ax.set_xlabel(feature_names[0])\n    ax.set_ylabel(feature_names[1])\n    ax.set_title('Visualização da Classificação 2D')\n    ax.legend(loc='best')\n    ax.grid(True, linestyle='--', alpha=0.5)\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "typing"], "versao": "0.1.0"}
{"id_funcao": 20, "titulo": "plot_cv_splits", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Cria uma visualização gráfica de como um splitter de validação cruzada divide os dados.", "codigo_funcao": "def plot_cv_splits(\n    cv_splitter: Any,\n    X: Union[np.ndarray, pd.DataFrame, int],\n    y: Optional[Union[np.ndarray, pd.Series]] = None,\n    ax: Optional[plt.Axes] = None\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Cria uma visualização gráfica de como um splitter de validação\n               cruzada (ex: KFold) divide os dados em folds de treino e teste.\n\n    Args:\n        cv_splitter (Any): Um objeto splitter de validação cruzada do scikit-learn\n                           que possui o método `.split(X, y)`.\n        X (Union[np.ndarray, pd.DataFrame, int]): Os dados de features (features) ou\n                           simplesmente o número total de amostras (se for int).\n        y (Optional[Union[np.ndarray, pd.Series]], optional): Os rótulos/alvo. Necessário\n                           se o splitter for estratificado (ex: StratifiedKFold).\n                           Defaults to None.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            uma nova figura/eixo é criada. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo a visualização dos splits.\n\n    Raises:\n        TypeError: Se `cv_splitter` não tiver o método `split` ou `get_n_splits`.\n        ValueError: Se `X` for int e `y` for fornecido (não faz sentido), ou se `y`\n                    for necessário (splitter estratificado) mas não fornecido.\n    \"\"\"\n    if not hasattr(cv_splitter, 'split') or not hasattr(cv_splitter, 'get_n_splits'):\n        raise TypeError(\"'cv_splitter' deve ser um objeto splitter do scikit-learn com métodos 'split' e 'get_n_splits'.\")\n\n    n_splits = cv_splitter.get_n_splits()\n\n    if isinstance(X, int):\n        n_samples = X\n        if y is not None:\n            raise ValueError(\"Se 'X' é um inteiro (n_samples), 'y' deve ser None.\")\n        # Criar X e y dummy apenas para o método split, se necessário\n        X_dummy = np.arange(n_samples).reshape(-1, 1) # Dummy X\n        y_dummy = y # y é None\n    elif hasattr(X, 'shape'):\n        n_samples = X.shape[0]\n        X_dummy = X # Usar X real\n        y_dummy = y # Usar y real\n    else:\n        raise TypeError(\"'X' deve ser um array NumPy, DataFrame pandas ou um inteiro (n_samples).\")\n\n    # Verificar se y é necessário\n    from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold # Import local\n    is_stratified = isinstance(cv_splitter, (StratifiedKFold, StratifiedGroupKFold)) # Adicionar outros se necessário\n    if is_stratified and y_dummy is None:\n         raise ValueError(\"Splitters estratificados (como StratifiedKFold) requerem que 'y' seja fornecido.\")\n\n    if ax is None:\n        # Cria uma figura com altura proporcional ao número de splits\n        fig, ax = plt.subplots(figsize=(10, n_splits * 0.8)) # Ajustar altura\n\n    cmap_data = plt.cm.Paired # Usar um colormap diferente\n    cmap_cv = plt.cm.coolwarm\n\n    ax.set_yticks(np.arange(n_splits) + 0.5)\n    ax.set_yticklabels([f\"Fold {i+1}\" for i in range(n_splits)])\n    ax.set_ylabel(\"Fold CV\")\n    ax.set_xticks([]) # Remover ticks do eixo x (índice da amostra)\n    ax.set_xlabel(\"Índice da Amostra\")\n    ax.set_xlim(0, n_samples)\n    ax.set_ylim(n_splits, 0) # Inverter eixo y para Fold 1 ficar no topo\n    ax.set_title(f'Visualização dos Splits ({cv_splitter.__class__.__name__}, n_splits={n_splits})')\n\n    # Desenha retângulos para cada fold\n    for i, (train_idx, test_idx) in enumerate(cv_splitter.split(X_dummy, y_dummy)):\n        # Preenche fundo com cor de treino\n        ax.add_patch(patches.Rectangle((0, i), n_samples, 1, facecolor=cmap_cv(0.1), edgecolor='k'))\n        # Desenha blocos de teste sobre o fundo\n        for idx in test_idx:\n             # Desenha um patch estreito para cada índice de teste\n             ax.add_patch(patches.Rectangle((idx, i), 1, 1, facecolor=cmap_cv(0.9), edgecolor='k'))\n\n    # Adiciona legenda manualmente (patches não criam legendas automaticamente)\n    train_patch = patches.Patch(color=cmap_cv(0.1), label='Dados de Treino')\n    test_patch = patches.Patch(color=cmap_cv(0.9), label='Dados de Teste')\n    ax.legend(handles=[train_patch, test_patch], bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Ajusta para caber a legenda fora\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 21, "titulo": "plot_grid_search_results", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Plota os resultados de um GridSearchCV em relação a um hiperparâmetro específico.", "codigo_funcao": "def plot_grid_search_results(\n    search_cv: Union[GridSearchCV, RandomizedSearchCV],\n    param_name: str,\n    score_key: str = 'mean_test_score',\n    std_key: Optional[str] = 'std_test_score',\n    log_scale: bool = True,\n    ax: Optional[plt.Axes] = None\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Plota os resultados de um GridSearchCV ou RandomizedSearchCV\n               em relação a um hiperparâmetro específico. Mostra a pontuação média\n               e, opcionalmente, o desvio padrão.\n\n    Args:\n        search_cv (Union[GridSearchCV, RandomizedSearchCV]): O objeto GridSearchCV ou\n            RandomizedSearchCV já treinado (.fit() executado), que contém o atributo `cv_results_`.\n        param_name (str): O nome exato do hiperparâmetro (como aparece nas chaves\n            'param_<nome>' em `cv_results_`) a ser plotado no eixo X.\n        score_key (str, optional): A chave em `cv_results_` que contém a pontuação\n            média a ser plotada no eixo Y. Defaults to 'mean_test_score'.\n        std_key (Optional[str], optional): A chave em `cv_results_` que contém o desvio\n            padrão da pontuação. Se fornecida, plota uma área sombreada (±1 std).\n            Defaults to 'std_test_score'.\n        log_scale (bool, optional): Se True, o eixo X (hiperparâmetro) será plotado\n            em escala logarítmica (útil para parâmetros como 'C' ou 'gamma').\n            Defaults to True.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico.\n\n    Raises:\n        AttributeError: Se `search_cv` não tiver o atributo `cv_results_`.\n        KeyError: Se `param_name`, `score_key` ou `std_key` não forem encontrados\n                  em `search_cv.cv_results_`.\n        TypeError: Se `search_cv` não for um objeto GridSearchCV ou RandomizedSearchCV.\n    \"\"\"\n    if not isinstance(search_cv, (GridSearchCV, RandomizedSearchCV)):\n         raise TypeError(\"O argumento 'search_cv' deve ser um objeto GridSearchCV ou RandomizedSearchCV treinado.\")\n    if not hasattr(search_cv, 'cv_results_'):\n        raise AttributeError(\"'search_cv' não possui o atributo 'cv_results_'. O .fit() foi executado?\")\n\n    cv_results = search_cv.cv_results_\n    param_key = f'param_{param_name}'\n\n    # Validar chaves\n    required_keys = [param_key, score_key]\n    if std_key:\n        required_keys.append(std_key)\n    missing_keys = [key for key in required_keys if key not in cv_results]\n    if missing_keys:\n        raise KeyError(f\"Chaves não encontradas em cv_results_: {missing_keys}. \"\n                       f\"Chaves disponíveis: {list(cv_results.keys())}\")\n\n    param_values = cv_results[param_key]\n    mean_scores = cv_results[score_key]\n\n    # Tentar converter param_values para numérico se possível (para plotar)\n    try:\n        param_values_num = np.array(param_values, dtype=float)\n    except ValueError:\n        print(f\"Aviso: Não foi possível converter os valores do parâmetro '{param_name}' para numérico. \"\n              f\"Plotando como categórico.\")\n        param_values_num = np.arange(len(param_values)) # Usar índice como x\n        log_scale = False # Desativar escala log para categórico/índice\n        x_labels = param_values # Guardar labels originais\n\n    # Ordenar pelos valores do parâmetro para um plot limpo\n    sorted_indices = np.argsort(param_values_num)\n    param_values_sorted = param_values_num[sorted_indices]\n    mean_scores_sorted = mean_scores[sorted_indices]\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 5))\n\n    # Plotar score médio\n    line, = ax.plot(param_values_sorted, mean_scores_sorted, 'o-', label=f'{score_key} Média')\n\n    # Plotar desvio padrão (se fornecido)\n    if std_key:\n        std_scores = cv_results[std_key]\n        std_scores_sorted = std_scores[sorted_indices]\n        ax.fill_between(param_values_sorted,\n                        mean_scores_sorted - std_scores_sorted,\n                        mean_scores_sorted + std_scores_sorted,\n                        alpha=0.2, color=line.get_color(), label=f'± 1 {std_key}')\n\n    ax.set_xlabel(param_name)\n    ax.set_ylabel(score_key.replace('_', ' ').title()) \n    ax.set_title(f'Resultado do Search CV vs {param_name}')\n    ax.legend()\n    ax.grid(True, linestyle='--', alpha=0.6)\n\n    if log_scale:\n        ax.set_xscale('log')\n\n    # Ajustar labels do eixo X se plotamos como categórico\n    if 'x_labels' in locals():\n         ax.set_xticks(param_values_num)\n         ax.set_xticklabels(x_labels[sorted_indices], rotation=45, ha='right')\n\n    plt.tight_layout()\n    return ax", "bibliotecas": ["matplotlib", "numpy", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 22, "titulo": "plot_roc_curves_cv", "categoria": "Modelagem e Machine Learning", "subcategoria": "Avaliação e Interpretabilidade", "descricao": "Plota as curvas ROC para cada fold de uma validação cruzada, com a média e desvio padrão.", "codigo_funcao": "def plot_roc_curves_cv(\n    model: BaseEstimator,\n    X: np.ndarray,\n    y: np.ndarray,\n    cv_splitter: Union[KFold, StratifiedKFold], # Recebe o splitter inicializado\n    ax: Optional[plt.Axes] = None\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Plota as curvas ROC para cada fold de uma validação cruzada,\n               juntamente com a curva ROC média e um intervalo de ±1 desvio padrão.\n\n    Args:\n        model (BaseEstimator): O modelo classificador scikit-learn (ou compatível)\n                               a ser avaliado. Será clonado e treinado em cada fold.\n        X (np.ndarray): Array NumPy de features (dados completos).\n        y (np.ndarray): Array NumPy do alvo (dados completos).\n        cv_splitter (Union[KFold, StratifiedKFold]): Um objeto splitter de validação\n            cruzada (ex: KFold(n_splits=5)) já inicializado.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo as curvas ROC.\n\n    Raises:\n        TypeError: Se model, X, y, ou cv_splitter forem de tipos inválidos.\n        ValueError: Se X e y tiverem shapes incompatíveis.\n        Exception: Se o modelo falhar ao treinar ou prever probabilidades.\n    \"\"\"\n    if not isinstance(model, BaseEstimator):\n        raise TypeError(\"'model' deve ser um estimador scikit-learn.\")\n    if not all(isinstance(arr, np.ndarray) for arr in [X, y]):\n        raise TypeError(\"'X' e 'y' devem ser arrays NumPy.\")\n    if X.shape[0] != y.shape[0]:\n         raise ValueError(f\"Incompatibilidade de amostras: X ({X.shape[0]}) e y ({y.shape[0]})\")\n    if not hasattr(cv_splitter, 'split') or not hasattr(cv_splitter, 'get_n_splits'):\n        raise TypeError(\"'cv_splitter' deve ser um objeto splitter CV do scikit-learn.\")\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 8))\n\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100) # Base comum de FPR para interpolação\n\n    n_splits = cv_splitter.get_n_splits()\n    fold_counter = 0\n\n    for train_idx, test_idx in cv_splitter.split(X, y):\n        fold_counter += 1\n        X_train_fold, X_test_fold = X[train_idx], X[test_idx]\n        y_train_fold, y_test_fold = y[train_idx], y[test_idx]\n\n        try:\n            # Clona e treina o modelo no fold de treino\n            model_fold = clone(model)\n            model_fold.fit(X_train_fold, y_train_fold)\n\n            # Prever probabilidades no fold de teste\n            if hasattr(model_fold, \"predict_proba\"):\n                 y_proba_fold = model_fold.predict_proba(X_test_fold)[:, 1]\n            elif hasattr(model_fold, \"decision_function\"):\n                 # Usa decision_function se predict_proba não estiver disponível (ex: alguns SVMs)\n                 y_proba_fold = model_fold.decision_function(X_test_fold)\n                 # Escala decision_function para [0, 1] (aproximação)\n                 y_proba_fold = (y_proba_fold - y_proba_fold.min()) / (y_proba_fold.max() - y_proba_fold.min())\n            else:\n                 raise AttributeError(f\"Modelo {model.__class__.__name__} não possui 'predict_proba' ou 'decision_function'.\")\n\n            # Calcula ROC curve e AUC para o fold atual\n            fpr, tpr, _ = roc_curve(y_test_fold, y_proba_fold)\n            roc_auc = auc(fpr, tpr)\n            aucs.append(roc_auc)\n\n            # Plota a curva ROC do fold atual\n            ax.plot(fpr, tpr, lw=1, alpha=0.4, label=f'Fold {fold_counter} (AUC = {roc_auc:.2f})')\n\n            # Interpola TPR para a base comum de FPR\n            interp_tpr = np.interp(mean_fpr, fpr, tpr)\n            interp_tpr[0] = 0.0 # Garante que começa em 0\n            tprs.append(interp_tpr)\n\n        except Exception as e:\n            print(f\"Aviso: Erro ao processar fold {fold_counter}: {e}. Pulando este fold.\")\n            continue # Pula para o próximo fold\n\n    # Plotar linha de chance (aleatório)\n    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='grey', label='Chance (AUC = 0.5)', alpha=.8)\n\n    # Calcular e plotar média e desvio padrão das curvas\n    if tprs: # Apenas se pelo menos um fold funcionou\n        mean_tpr = np.mean(tprs, axis=0)\n        mean_tpr[-1] = 1.0 # Garante que termina em 1\n        mean_auc = auc(mean_fpr, mean_tpr)\n        std_auc = np.std(aucs)\n        ax.plot(mean_fpr, mean_tpr, color='b',\n                label=f'ROC Média (AUC = {mean_auc:.2f} ± {std_auc:.2f})',\n                lw=2, alpha=.8)\n\n        std_tpr = np.std(tprs, axis=0)\n        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n        ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='b', alpha=.2,\n                        label=r'± 1 desvio padrão')\n    else:\n        print(\"Aviso: Nenhuma curva ROC pôde ser calculada ou plotada.\")\n\n\n    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n           xlabel='Taxa de Falsos Positivos (FPR)',\n           ylabel='Taxa de Verdadeiros Positivos (TPR)',\n           title=f'Curvas ROC da Validação Cruzada ({n_splits} folds)')\n    ax.legend(loc=\"lower right\", fontsize='small') # Ajustar tamanho da legenda\n    ax.grid(True, linestyle='--', alpha=0.5)\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 23, "titulo": "plot_comparative_histogram", "categoria": "Visualização de Dados", "subcategoria": "Distribuições Univariadas", "descricao": "Plota histogramas sobrepostos de uma coluna numérica, separados pelos valores de uma coluna alvo.", "codigo_funcao": "def plot_comparative_histogram(\n    df: pd.DataFrame,\n    num_col: str,\n    target_col: str,\n    bins: Union[int, List[float], str] = 'auto',\n    density: bool = False,\n    ax: Optional[plt.Axes] = None,\n    **hist_kwargs\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Plota histogramas sobrepostos de uma coluna numérica, separados\n               pelos valores de uma coluna alvo (geralmente binária),\n               para comparação visual das distribuições.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        num_col (str): Nome da coluna numérica a ser plotada no histograma.\n        target_col (str): Nome da coluna (geralmente categórica ou binária)\n                          usada para separar os dados em grupos.\n        bins (Union[int, List[float], str], optional): Argumento 'bins' a ser passado\n            para `plt.hist` (número de bins, lista de bordas, ou estratégia como 'auto').\n            Defaults to 'auto'.\n        density (bool, optional): Se True, normaliza o histograma para formar uma\n            densidade de probabilidade. Se False (padrão), mostra contagens brutas.\n            Defaults to False.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n        **hist_kwargs: Argumentos adicionais a serem passados para `plt.hist()`\n            (ex: alpha=0.6, color=['blue', 'red'], label=['Grupo 0', 'Grupo 1']).\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo os histogramas.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se 'num_col' ou 'target_col' não existirem no DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if num_col not in df.columns:\n        raise ValueError(f\"A coluna numérica '{num_col}' não existe no DataFrame.\")\n    if target_col not in df.columns:\n        raise ValueError(f\"A coluna alvo '{target_col}' não existe no DataFrame.\")\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=hist_kwargs.pop('figsize', (8, 5)))\n\n    # Identifica os grupos únicos na coluna alvo\n    target_values = df[target_col].unique()\n    target_values = sorted([v for v in target_values if pd.notna(v)]) # Ordena e remove NaN\n\n    # Pega cores/labels dos kwargs ou define defaults\n    colors = hist_kwargs.pop('color', plt.cm.viridis(np.linspace(0, 1, len(target_values))))\n    labels = hist_kwargs.pop('label', [f'{target_col} = {val}' for val in target_values])\n    alpha = hist_kwargs.pop('alpha', 0.6) # Default alpha para sobreposição\n\n    if len(colors) < len(target_values):\n         raise ValueError(f\"Fornecidas {len(colors)} cores, mas existem {len(target_values)} grupos em '{target_col}'.\")\n    if len(labels) < len(target_values):\n         raise ValueError(f\"Fornecidos {len(labels)} labels, mas existem {len(target_values)} grupos em '{target_col}'.\")\n\n    # Plota um histograma para cada grupo\n    plotted_data = []\n    for i, value in enumerate(target_values):\n        data_subset = df.loc[df[target_col] == value, num_col].dropna()\n        if not data_subset.empty:\n            ax.hist(data_subset, bins=bins, density=density,\n                    alpha=alpha, color=colors[i], label=labels[i], **hist_kwargs)\n            plotted_data.append(data_subset)\n        else:\n             print(f\"Aviso: Nenhum dado encontrado para {target_col} = {value}.\")\n\n    # Ajusta o título e labels\n    ylabel = 'Densidade de Probabilidade' if density else 'Contagem (Frequência)'\n    ax.set_xlabel(num_col)\n    ax.set_ylabel(ylabel)\n    ax.set_title(hist_kwargs.pop('title', f'Distribuição de {num_col} por {target_col}'))\n    ax.legend()\n    ax.grid(True, axis='y', linestyle='--', alpha=0.5) # Grid apenas no eixo y\n\n    plt.tight_layout()\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 24, "titulo": "remove_items_from_list", "categoria": "Funções Utilitárias", "subcategoria": "Manipulação de Estruturas", "descricao": "Remove um conjunto de itens de uma lista, retornando uma nova lista.", "codigo_funcao": "def remove_items_from_list(original_list: List[Any], items_to_remove: List[Any]) -> List[Any]:\n    \"\"\"\n    Descrição: Remove um conjunto de itens de uma lista, retornando uma nova lista.\n\n    Args:\n        original_list (List[Any]): A lista original.\n        items_to_remove (List[Any]): A lista de itens a serem removidos da lista original.\n\n    Returns:\n        List[Any]: Uma nova lista contendo apenas os itens que NÃO estavam em 'items_to_remove'.\n\n    Raises:\n        TypeError: Se 'original_list' ou 'items_to_remove' não forem listas.\n    \"\"\"\n    if not isinstance(original_list, list):\n        raise TypeError(\"O argumento 'original_list' deve ser uma lista.\")\n    if not isinstance(items_to_remove, list):\n        raise TypeError(\"O argumento 'items_to_remove' deve ser uma lista.\")\n\n    # Usar um set para 'items_to_remove' otimiza a verificação (item not in ...)\n    items_set: Set[Any] = set(items_to_remove)\n\n    # List comprehension para criar a nova lista filtrada\n    new_list = [item for item in original_list if item not in items_set]\n\n    return new_list", "bibliotecas": ["typing"], "versao": "0.1.0"}
{"id_funcao": 25, "titulo": "concatenate_numpy_arrays", "categoria": "Funções Utilitárias", "subcategoria": "Manipulação de Estruturas", "descricao": "Concatena dois arrays NumPy ao longo de um eixo especificado (linhas ou colunas).", "codigo_funcao": "def concatenate_numpy_arrays(arr1: np.ndarray, arr2: np.ndarray, axis: int = 0) -> np.ndarray:\n    \"\"\"\n    Concatena dois arrays NumPy ao longo de um eixo especificado.\n\n    Args:\n        arr1 (np.ndarray): Primeiro array NumPy.\n        arr2 (np.ndarray): Segundo array NumPy.\n        axis (int, optional): O eixo ao longo do qual concatenar (0 = linhas, 1 = colunas).\n                             Defaults to 0.\n\n    Returns:\n        np.ndarray: O array NumPy resultante da concatenação.\n\n    Raises:\n        TypeError: Se 'arr1' ou 'arr2' não forem arrays NumPy.\n        ValueError: Se os arrays tiverem shapes incompatíveis para concatenação\n                    ao longo do eixo especificado.\n    \"\"\"\n    if not isinstance(arr1, np.ndarray) or not isinstance(arr2, np.ndarray):\n        raise TypeError(\"As entradas 'arr1' e 'arr2' devem ser arrays NumPy.\")\n\n    try:\n        concatenated_array = np.concatenate((arr1, arr2), axis=axis)\n        return concatenated_array\n    except ValueError as e:\n        shape1 = arr1.shape\n        shape2 = arr2.shape\n        raise ValueError(f\"Não foi possível concatenar arrays com shapes {shape1} e {shape2} \"\n                         f\"ao longo do eixo {axis}. Verifique a compatibilidade das dimensões. \"\n                         f\"Erro original: {e}\")", "bibliotecas": ["numpy", "typing"], "versao": "0.1.0"}
{"id_funcao": 26, "titulo": "generate_random_sample_from_array", "categoria": "Funções Utilitárias", "subcategoria": "Geração de Dados Sintéticos", "descricao": "Gera uma amostra aleatória (com ou sem reposição) de um array ou Série pandas.", "codigo_funcao": "def generate_random_sample_from_array(\n    source_array: Union[np.ndarray, pd.Series],\n    sample_size: int,\n    replace: bool = True, \n    random_state: Optional[int] = None \n) -> np.ndarray:\n    \"\"\"\n    Gera uma amostra aleatória (com ou sem reposição) de um array ou Série pandas.\n\n    Args:\n        source_array (Union[np.ndarray, pd.Series]): O array ou Série de onde tirar a amostra.\n                                                    Valores NaN na fonte serão incluídos na\n                                                    amostragem se presentes.\n        sample_size (int): O número de amostras a serem geradas (tamanho do resultado).\n        replace (bool, optional): Se True (padrão), a amostragem é feita com reposição.\n                                  Se False, é sem reposição (sample_size não pode ser\n                                  maior que o tamanho da fonte). Defaults to True.\n        random_state (Optional[int], optional): Seed opcional para o gerador de números\n                                                aleatórios para reprodutibilidade. Defaults to None.\n\n    Returns:\n        np.ndarray: Um novo array numpy contendo a amostra aleatória.\n\n    Raises:\n        TypeError: Se 'source_array' não for um np.ndarray ou pd.Series.\n        ValueError: Se 'sample_size' não for um inteiro positivo, ou se replace=False\n                    e sample_size > len(source_array).\n    \"\"\"\n    if not isinstance(source_array, (np.ndarray, pd.Series)):\n        raise TypeError(\"O argumento 'source_array' deve ser um np.ndarray ou pd.Series.\")\n    if not isinstance(sample_size, int) or sample_size <= 0:\n        raise ValueError(\"'sample_size' deve ser um inteiro positivo.\")\n\n    # Converte para numpy array caso seja uma Série\n    if isinstance(source_array, pd.Series):\n        source_values = source_array.values\n    else:\n        source_values = source_array # Já é ndarray\n\n    if not replace and sample_size > len(source_values):\n         raise ValueError(\"sample_size não pode ser maior que o tamanho da fonte \"\n                          \"quando replace=False.\")\n\n    # Configura o gerador de números aleatórios para reprodutibilidade\n    rng = np.random.default_rng(seed=random_state)\n\n    # Usa o método choice do gerador\n    return rng.choice(source_values, size=sample_size, replace=replace)", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 27, "titulo": "initialize_kfold_splitter", "categoria": "Funções Utilitárias", "subcategoria": "Configuração de ML", "descricao": "Inicializa e retorna um objeto KFold ou StratifiedKFold do scikit-learn.", "codigo_funcao": "def initialize_kfold_splitter(\n    n_splits: int,\n    shuffle: bool = True, \n    random_state: Optional[int] = 42,\n    stratify: bool = False \n) -> Union[KFold, StratifiedKFold]:\n    \"\"\"\n    Inicializa e retorna um objeto KFold ou StratifiedKFold do scikit-learn.\n\n    Args:\n        n_splits (int): O número de folds (divisões). Deve ser >= 2.\n        shuffle (bool, optional): Se deve embaralhar os dados antes de dividir.\n                                  Defaults to True.\n        random_state (Optional[int], optional): Seed para reprodutibilidade se shuffle=True.\n                                               Defaults to 42.\n        stratify (bool, optional): Se True, retorna um StratifiedKFold. Se False (padrão),\n                                   retorna um KFold. Defaults to False.\n\n    Returns:\n        Union[KFold, StratifiedKFold]: O objeto splitter configurado.\n\n    Raises:\n        ValueError: Se n_splits for menor que 2.\n    \"\"\"\n    if not isinstance(n_splits, int) or n_splits < 2:\n        raise ValueError(\"'n_splits' deve ser um inteiro maior ou igual a 2.\")\n\n    state_to_pass = random_state if shuffle else None\n\n    if stratify:\n        return StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=state_to_pass)\n    else:\n        return KFold(n_splits=n_splits, shuffle=shuffle, random_state=state_to_pass)", "bibliotecas": ["scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 28, "titulo": "generate_uniform_data_1d", "categoria": "Funções Utilitárias", "subcategoria": "Geração de Dados Sintéticos", "descricao": "Gera um array NumPy 1D de dados com distribuição uniforme.", "codigo_funcao": "def generate_uniform_data_1d(\n    n_samples: int,\n    low: float,\n    high: float,\n    random_state: Optional[int] = None \n) -> np.ndarray:\n    \"\"\"\n    Gera um array NumPy 1D de dados com distribuição uniforme.\n\n    Args:\n        n_samples (int): O número de amostras a serem geradas (tamanho do array). Deve ser >= 0.\n        low (float): O limite inferior (inclusivo) da distribuição.\n        high (float): O limite superior (exclusivo) da distribuição. Deve ser > low.\n        random_state (Optional[int]): Seed opcional para o gerador de números\n                                        aleatórios para reprodutibilidade. Defaults to None.\n\n    Returns:\n        np.ndarray: Array NumPy 1D com os dados gerados.\n\n    Raises:\n        TypeError: Se 'n_samples' não for int, ou 'low'/'high' não forem numéricos.\n        ValueError: Se 'n_samples' for negativo ou 'high' <= 'low'.\n    \"\"\"\n    if not isinstance(n_samples, int):\n        raise TypeError(\"O argumento 'n_samples' deve ser um inteiro.\")\n    if n_samples < 0:\n        raise ValueError(\"O argumento 'n_samples' não pode ser negativo.\")\n    if not all(isinstance(v, (int, float)) for v in [low, high]):\n        raise TypeError(\"Os argumentos 'low' e 'high' devem ser numéricos (int ou float).\")\n    if high <= low:\n        raise ValueError(\"O argumento 'high' deve ser maior que 'low'.\")\n\n    # Configura o gerador de números aleatórios\n    rng = np.random.default_rng(seed=random_state)\n\n    # Gera dados uniformes\n    return rng.uniform(low=low, high=high, size=n_samples) # size pode ser int direto para 1D", "bibliotecas": ["numpy", "typing"], "versao": "0.1.0"}
{"id_funcao": 29, "titulo": "generate_linear_regression_data", "categoria": "Funções Utilitárias", "subcategoria": "Geração de Dados Sintéticos", "descricao": "Gera dados sintéticos (X, y) 1D para regressão linear com ruído gaussiano.", "codigo_funcao": "def generate_linear_regression_data(\n    n_samples: int,\n    slope: float,\n    intercept: float,\n    x_low: float = 0.0,\n    x_high: float = 10.0,\n    noise_mean: float = 0.0, \n    noise_scale: float = 1.0,\n    random_state: Optional[int] = None\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Gera dados sintéticos (X, y) 1D para regressão linear com ruído gaussiano adicionado a y.\n\n    Args:\n        n_samples (int): O número de amostras (pontos) a serem geradas. Deve ser >= 0.\n        slope (float): O coeficiente angular (inclinação) da linha de regressão verdadeira.\n        intercept (float): O intercepto da linha de regressão verdadeira.\n        x_low (float, optional): O limite inferior para gerar os valores de X (distribuição uniforme). Defaults to 0.0.\n        x_high (float, optional): O limite superior para gerar os valores de X. Deve ser > x_low. Defaults to 10.0.\n        noise_mean (float, optional): A média do ruído normal (gaussiano) adicionado a y. Defaults to 0.0.\n        noise_scale (float, optional): A escala (desvio padrão) do ruído normal. Deve ser >= 0. Defaults to 1.0.\n        random_state (Optional[int], optional): Seed opcional para reprodutibilidade. Defaults to None.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Uma tupla contendo (X, y) como arrays NumPy 1D.\n                                       X contém os valores da variável independente.\n                                       y contém os valores da variável dependente (slope * X + intercept + noise).\n\n    Raises:\n        TypeError: Se os argumentos numéricos não forem int ou float.\n        ValueError: Se 'n_samples' for negativo, 'x_high' <= 'x_low', ou 'noise_scale' for negativo.\n    \"\"\"\n    if not isinstance(n_samples, int) or n_samples < 0:\n        raise ValueError(\"O argumento 'n_samples' deve ser um inteiro não negativo.\")\n    numerics = [slope, intercept, x_low, x_high, noise_mean, noise_scale]\n    if not all(isinstance(v, (int, float)) for v in numerics):\n        raise TypeError(\"Argumentos 'slope', 'intercept', 'x_low', 'x_high', 'noise_mean', 'noise_scale' devem ser numéricos.\")\n    if x_high <= x_low:\n        raise ValueError(\"'x_high' deve ser maior que 'x_low'.\")\n    if noise_scale < 0:\n        raise ValueError(\"'noise_scale' (desvio padrão do ruído) não pode ser negativo.\")\n\n    # Configura o gerador de números aleatórios\n    rng = np.random.default_rng(seed=random_state)\n\n    # 1. Gerar X (uniforme)\n    X = rng.uniform(low=x_low, high=x_high, size=n_samples)\n\n    # 2. Gerar ruído (normal/gaussiano)\n    noise = rng.normal(loc=noise_mean, scale=noise_scale, size=n_samples)\n\n    # 3. Gerar y (relação linear + ruído)\n    y = slope * X + intercept + noise\n\n    return X, y", "bibliotecas": ["numpy", "typing"], "versao": "0.1.0"}
{"id_funcao": 30, "titulo": "find_outlier_indices", "categoria": "Exploração e Limpeza de Dados", "subcategoria": "Limpeza e Tratamento de Dados", "descricao": "Identifica os índices de linhas que contêm outliers usando o método IQR ou Desvio Padrão.", "codigo_funcao": "def find_outlier_indices(\n    df: pd.DataFrame,\n    cols: Optional[List[str]] = None,\n    method: Literal['std', 'iqr'] = 'iqr', \n    threshold: float = 1.5 \n) -> pd.Index:\n    \"\"\"\n    Descrição: Identifica e retorna os índices de linhas que contêm outliers em\n               colunas numéricas, usando o método do Desvio Padrão (Z-Score) ou IQR.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols (List[str], optional): Lista de colunas numéricas a serem verificadas.\n            Se None, usa todas as colunas numéricas. Defaults to None.\n        method (Literal['std', 'iqr'], optional): Método de detecção.\n            'std': Usa Z-Score (média ± threshold * desvio padrão). Sensível a outliers existentes.\n            'iqr': Usa Intervalo Interquartil (Q1 - threshold*IQR, Q3 + threshold*IQR). Mais robusto.\n            Defaults to 'iqr'.\n        threshold (float, optional): O multiplicador para o desvio padrão (se method='std')\n            ou para o IQR (se method='iqr').\n            Valores comuns: 3.0 para 'std', 1.5 para 'iqr'. Defaults to 1.5.\n\n    Returns:\n        pd.Index: Um índice pandas contendo as labels (índices) únicas das linhas\n                  identificadas como contendo pelo menos um outlier em qualquer\n                  das colunas verificadas.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se 'method' for inválido, 'cols' contiver colunas inexistentes,\n                    ou 'threshold' for negativo.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if method not in ['std', 'iqr']:\n        raise ValueError(\"Método inválido. Escolha 'std' ou 'iqr'.\")\n    if threshold < 0:\n        raise ValueError(\"'threshold' não pode ser negativo.\")\n\n    if cols is None:\n        cols_to_process = df.select_dtypes(include=np.number).columns.tolist()\n        if not cols_to_process:\n             print(\"Aviso: Nenhuma coluna numérica encontrada para verificação de outliers.\")\n             return pd.Index([]) # Retorna índice vazio\n    else:\n        if not isinstance(cols, list):\n             raise TypeError(\"'cols' deve ser uma lista de nomes de colunas.\")\n        missing = [c for c in cols if c not in df.columns]\n        if missing:\n            raise ValueError(f\"As seguintes colunas não existem no df: {missing}\")\n        # Filtrar para garantir que são numéricas\n        numeric_cols_in_list = df[cols].select_dtypes(include=np.number).columns.tolist()\n        if len(numeric_cols_in_list) < len(cols):\n             non_numeric = [c for c in cols if c not in numeric_cols_in_list]\n             print(f\"Aviso: Ignorando colunas não numéricas especificadas em 'cols': {non_numeric}\")\n        cols_to_process = numeric_cols_in_list\n        if not cols_to_process:\n             print(\"Aviso: Nenhuma coluna numérica válida encontrada na lista 'cols'.\")\n             return pd.Index([])\n\n    outlier_indices = pd.Index([])\n\n    for col in cols_to_process:\n        data_col = df[col].dropna() # Ignora NaNs na detecção\n        if data_col.empty:\n            continue\n\n        if method == 'std':\n            mean = data_col.mean()\n            std = data_col.std()\n            if std == 0: continue # Coluna constante não tem outliers por std\n            lower_bound = mean - threshold * std\n            upper_bound = mean + threshold * std\n        else: # method == 'iqr'\n            Q1 = data_col.quantile(0.25)\n            Q3 = data_col.quantile(0.75)\n            IQR = Q3 - Q1\n            if IQR == 0: continue # Se IQR é zero, bounds podem ser iguais a Q1/Q3\n            lower_bound = Q1 - threshold * IQR\n            upper_bound = Q3 + threshold * IQR\n\n        # Identifica outliers na coluna atual (usando o DataFrame original para obter índices corretos)\n        col_outliers = df.index[(df[col] < lower_bound) | (df[col] > upper_bound)]\n        outlier_indices = outlier_indices.union(col_outliers) # Acumula índices únicos\n\n    return outlier_indices", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 31, "titulo": "impute_missing_values_simple", "categoria": "Exploração e Limpeza de Dados", "subcategoria": "Limpeza e Tratamento de Dados", "descricao": "Imputa valores ausentes (NaN) usando estratégias simples (média, mediana, moda, constante).", "codigo_funcao": "def impute_missing_values_simple(\n    df: pd.DataFrame,\n    cols: Optional[List[str]] = None,\n    strategy: Literal['mean', 'median', 'mode', 'constant'] = 'mean',\n    fill_value: Optional[Any] = None, \n    inplace: bool = False\n) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Descrição: Imputa valores ausentes (NaN) em colunas especificadas usando\n               estratégias simples (média, mediana, moda, constante).\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols (List[str], optional): Colunas onde a imputação será aplicada. Se None,\n            tenta aplicar em todas as colunas com NaNs. Defaults to None.\n        strategy (Literal['mean', 'median', 'mode', 'constant'], optional): A estratégia\n            de imputação simples. Defaults to 'mean'.\n        fill_value (Optional[Any], optional): Valor a ser usado quando strategy='constant'.\n            Ignorado para outras estratégias. Defaults to None.\n        inplace (bool, optional): Se True, modifica o DataFrame original. Defaults to False.\n\n    Returns:\n        Optional[pd.DataFrame]: DataFrame com valores imputados, ou None se inplace=True.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame.\n        ValueError: Se a estratégia for inválida, colunas não existirem, ou `fill_value`\n                    não for fornecido quando strategy='constant'.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n\n    valid_strategies = ['mean', 'median', 'mode', 'constant']\n    if strategy not in valid_strategies:\n        raise ValueError(f\"Estratégia inválida para imputação simples. Escolha entre: {valid_strategies}\")\n\n    df_out = df if inplace else df.copy()\n\n    if cols is None:\n        cols_to_process = df_out.columns[df_out.isnull().any()].tolist()\n        if not cols_to_process:\n             print(\"Aviso: Nenhuma coluna com valores ausentes encontrada.\")\n             return df_out if not inplace else None\n    else:\n        if not isinstance(cols, list):\n             raise TypeError(\"'cols' deve ser uma lista de nomes de colunas.\")\n        missing = [c for c in cols if c not in df_out.columns]\n        if missing:\n            raise ValueError(f\"As seguintes colunas não existem no df: {missing}\")\n        cols_to_process = [c for c in cols if df_out[c].isnull().any()]\n        if not cols_to_process:\n             print(f\"Aviso: Nenhuma das colunas especificadas em 'cols' possui valores ausentes.\")\n             return df_out if not inplace else None\n\n    numeric_cols = df_out[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n    non_numeric_cols = [c for c in cols_to_process if c not in numeric_cols]\n\n    if strategy in ['mean', 'median'] and non_numeric_cols:\n        print(f\"Aviso: Estratégia '{strategy}' só aplicável a colunas numéricas. Ignorando: {non_numeric_cols}\")\n        cols_for_impute = numeric_cols\n    else:\n         cols_for_impute = cols_to_process\n\n    if not cols_for_impute:\n         print(f\"Aviso: Nenhuma coluna aplicável encontrada para a estratégia '{strategy}'.\")\n         return df_out if not inplace else None\n\n    if strategy == 'constant' and fill_value is None:\n         raise ValueError(\"`fill_value` deve ser fornecido para strategy='constant'.\")\n\n    sklearn_strategy = 'most_frequent' if strategy == 'mode' else strategy\n    \n    imputer = SimpleImputer(strategy=sklearn_strategy, fill_value=fill_value)\n\n    try:\n        imputed_data = imputer.fit_transform(df_out[cols_for_impute])\n        df_imputed_temp = pd.DataFrame(imputed_data, columns=cols_for_impute, index=df_out.index)\n        df_out[cols_for_impute] = df_imputed_temp\n    except Exception as e:\n        raise RuntimeError(f\"Erro ao aplicar SimpleImputer (strategy='{strategy}') nas colunas {cols_for_impute}: {e}\")\n\n    if inplace:\n        return None\n    return df_out", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 32, "titulo": "apply_pca", "categoria": "Engenharia e Transformação de Features", "subcategoria": "Redução de Dimensionalidade (PCA)", "descricao": "Aplica Análise de Componentes Principais (PCA) em um DataFrame, com padronização opcional.", "codigo_funcao": "def apply_pca(\n    df: pd.DataFrame,\n    n_components: int,\n    cols: Optional[List[str]] = None,\n    standardize: bool = True\n) -> Tuple[pd.DataFrame, PCA]:\n    \"\"\"\n    Descrição: Aplica a Análise de Componentes Principais (PCA) em um DataFrame,\n               após opcionalmente padronizar os dados (Z-score).\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        n_components (int): O número de componentes principais a serem retornados.\n        cols (Optional[List[str]], optional): Lista de colunas a serem usadas na análise.\n            Se None, utiliza todas as colunas numéricas do DataFrame. Defaults to None.\n        standardize (bool, optional): Se True, aplica a padronização Z-score (StandardScaler)\n            antes da PCA, o que é altamente recomendado. Defaults to True.\n\n    Returns:\n        Tuple[pd.DataFrame, PCA]: Uma tupla contendo:\n            - Um novo DataFrame com os componentes principais como colunas.\n            - O objeto PCA treinado (fitted).\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame do pandas.\n        ValueError: Se 'cols' contiver colunas inexistentes ou n_components for inválido.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame do pandas.\")\n\n    if cols:\n        if not all(isinstance(c, str) for c in cols):\n            raise TypeError(\"'cols' deve ser uma lista de strings.\")\n        missing = [c for c in cols if c not in df.columns]\n        if missing:\n            raise ValueError(f\"As seguintes colunas não existem no df: {missing}\")\n        data_to_process = df[cols]\n    else:\n        data_to_process = df.select_dtypes(include=np.number)\n\n    if n_components <= 0 or n_components > data_to_process.shape[1]:\n        raise ValueError(f\"'n_components' ({n_components}) deve ser um inteiro positivo \"\n                         f\"e não maior que o número de features ({data_to_process.shape[1]}).\")\n\n    # Garante que não há NaNs\n    if data_to_process.isnull().values.any():\n        raise ValueError(\"Os dados para PCA não podem conter valores ausentes (NaN).\")\n\n    X = data_to_process.values\n\n    if standardize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    pca = PCA(n_components=n_components)\n    principal_components = pca.fit_transform(X)\n\n    pc_cols = [f'PC_{i+1}' for i in range(n_components)]\n    df_pca = pd.DataFrame(data=principal_components, columns=pc_cols, index=df.index)\n\n    return df_pca, pca", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 33, "titulo": "get_pca_explained_variance", "categoria": "Engenharia e Transformação de Features", "subcategoria": "Redução de Dimensionalidade (PCA)", "descricao": "Extrai a variância explicada individual e acumulada de um modelo PCA treinado.", "codigo_funcao": "def get_pca_explained_variance(pca_model: PCA) -> pd.DataFrame:\n    \"\"\"\n    Descrição: Extrai a variância explicada individual e acumulada de um\n               modelo PCA treinado e a retorna em um DataFrame.\n\n    Args:\n        pca_model (PCA): O objeto PCA já treinado (fitted).\n\n    Returns:\n        pd.DataFrame: DataFrame com as colunas ['Component', 'Explained Variance',\n                      'Cumulative Variance'].\n\n    Raises:\n        AttributeError: Se o objeto pca_model não parecer estar treinado (não tiver\n                        o atributo 'explained_variance_ratio_').\n    \"\"\"\n    if not hasattr(pca_model, 'explained_variance_ratio_'):\n        raise AttributeError(\"O modelo PCA fornecido não parece estar treinado.\")\n\n    explained_variance = pca_model.explained_variance_ratio_\n    cumulative_variance = np.cumsum(explained_variance)\n    \n    n_components = len(explained_variance)\n    component_names = [f'PC_{i+1}' for i in range(n_components)]\n\n    df_variance = pd.DataFrame({\n        'Component': component_names,\n        'Explained Variance': explained_variance,\n        'Cumulative Variance': cumulative_variance\n    })\n\n    return df_variance", "bibliotecas": ["numpy", "pandas", "scikit-learn"], "versao": "0.1.0"}
{"id_funcao": 34, "titulo": "plot_pca_component_heatmap", "categoria": "Aprendizado Não Supervisionado", "subcategoria": "Redução de Dimensionalidade (PCA)", "descricao": "Plota um heatmap da contribuição de cada feature original em cada componente principal.", "codigo_funcao": "def plot_pca_component_heatmap(\n    pca_model: PCA,\n    feature_names: List[str],\n    ax: Optional[Axes] = None, \n    **heatmap_kwargs\n) -> Axes: \n    \"\"\"\n    Descrição: Plota um heatmap dos componentes de um modelo PCA para visualizar\n               a contribuição de cada feature original em cada componente principal.\n\n    Args:\n        pca_model (PCA): O objeto PCA já treinado (fitted).\n        feature_names (List[str]): Lista dos nomes das features originais, na mesma\n                                   ordem em que foram usadas para treinar o PCA.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n        **heatmap_kwargs: Argumentos adicionais a serem passados para sns.heatmap()\n            (ex: cmap='viridis', annot=True).\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o heatmap.\n\n    Raises:\n        AttributeError: Se o modelo PCA não estiver treinado.\n        ValueError: Se o número de feature_names não corresponder ao número de\n                    features no modelo.\n    \"\"\"\n    if not hasattr(pca_model, 'components_'):\n        raise AttributeError(\"O modelo PCA fornecido não parece estar treinado.\")\n    if len(feature_names) != pca_model.n_features_in_:\n        raise ValueError(f\"O número de 'feature_names' ({len(feature_names)}) não corresponde \"\n                         f\"ao número de features no modelo ({pca_model.n_features_in_}).\")\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=heatmap_kwargs.pop('figsize', (12, max(4, pca_model.n_components_ * 0.8))))\n\n    component_labels = [f'PC_{i+1}' for i in range(pca_model.n_components_)]\n    \n    heatmap_kwargs.setdefault('cmap', 'viridis')\n    heatmap_kwargs.setdefault('annot', True)\n    heatmap_kwargs.setdefault('fmt', '.2f')\n\n    sns.heatmap(\n        pca_model.components_,\n        xticklabels=feature_names,\n        yticklabels=component_labels,\n        ax=ax,\n        **heatmap_kwargs\n    )\n    ax.set_title(heatmap_kwargs.pop('title', 'Contribuição das Features nos Componentes Principais'))\n    plt.tight_layout()\n    \n    return ax", "bibliotecas": ["matplotlib", "pandas", "scikit-learn", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 35, "titulo": "plot_pca_scatter", "categoria": "Aprendizado Não Supervisionado", "subcategoria": "Redução de Dimensionalidade (PCA)", "descricao": "Plota um gráfico de dispersão 2D ou 3D dos componentes principais (PCA).", "codigo_funcao": "def plot_pca_scatter(\n    df_pca: pd.DataFrame,\n    components_to_plot: List[int],\n    color_by: Optional[Union[pd.Series, str]] = None,\n    ax: Optional[plt.Axes] = None,\n    **scatter_kwargs\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Plota um gráfico de dispersão 2D ou 3D dos componentes principais.\n\n    Args:\n        df_pca (pd.DataFrame): DataFrame contendo os componentes principais\n            (geralmente a saída de apply_pca).\n        components_to_plot (List[int]): Lista com 2 ou 3 inteiros indicando quais\n            componentes principais plotar (ex: [1, 2] para 2D, [1, 2, 3] para 3D).\n        color_by (Optional[Union[pd.Series, str]], optional): Uma Série (ou nome de coluna\n            no df original) para colorir os pontos. O índice deve ser compatível\n            com df_pca. Defaults to None.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Para 3D, deve ser um eixo de projeção 3D.\n            Defaults to None.\n        **scatter_kwargs: Argumentos adicionais a serem passados para a função de plot\n            (ex: s=50, alpha=0.7, cmap='viridis').\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico.\n\n    Raises:\n        ValueError: Se o número de componentes para plotar não for 2 ou 3.\n    \"\"\"\n    n_dims = len(components_to_plot)\n    if n_dims not in [2, 3]:\n        raise ValueError(\"'components_to_plot' deve conter 2 ou 3 componentes.\")\n\n    pc_names = [f'PC_{i}' for i in components_to_plot]\n    missing_cols = [c for c in pc_names if c not in df_pca.columns]\n    if missing_cols:\n        raise ValueError(f\"As colunas de componente a seguir não foram encontradas: {missing_cols}\")\n\n    if ax is None:\n        fig = plt.figure(figsize=scatter_kwargs.pop('figsize', (10, 8)))\n        if n_dims == 3:\n            ax = fig.add_subplot(111, projection='3d')\n        else:\n            ax = fig.add_subplot(111)\n\n    x_data = df_pca[pc_names[0]]\n    y_data = df_pca[pc_names[1]]\n    \n    if n_dims == 3:\n        z_data = df_pca[pc_names[2]]\n        scatter = ax.scatter(x_data, y_data, z_data, c=color_by, **scatter_kwargs)\n        ax.set_zlabel(pc_names[2])\n    else:\n        scatter = ax.scatter(x_data, y_data, c=color_by, **scatter_kwargs)\n\n    ax.set_xlabel(pc_names[0])\n    ax.set_ylabel(pc_names[1])\n    ax.set_title(scatter_kwargs.pop('title', f'Gráfico de Dispersão PCA ({n_dims}D)'))\n    ax.grid(True)\n\n    # Adiciona uma colorbar se os dados de cor forem numéricos\n    if color_by is not None and pd.api.types.is_numeric_dtype(color_by):\n        legend_label = color_by.name if hasattr(color_by, 'name') else 'Color'\n        plt.colorbar(scatter, ax=ax, label=legend_label)\n\n    plt.tight_layout()\n    return ax", "bibliotecas": ["matplotlib", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 36, "titulo": "remove_rows_with_zero_values", "categoria": "Exploração e Limpeza de Dados", "subcategoria": "Limpeza e Tratamento de Dados", "descricao": "Remove linhas de um DataFrame onde qualquer uma das colunas especificadas contenha o valor zero.", "codigo_funcao": "def remove_rows_with_zero_values(\n    df: pd.DataFrame,\n    cols: Optional[List[str]] = None,\n    inplace: bool = False\n) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Descrição: Remove linhas de um DataFrame onde qualquer uma das colunas especificadas\n               contenha o valor zero.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols (Optional[List[str]], optional): Lista de colunas a serem verificadas.\n            Se None, todas as colunas numéricas são utilizadas. Defaults to None.\n        inplace (bool, optional): Se True, modifica o DataFrame original.\n            Se False, retorna uma cópia modificada. Defaults to False.\n\n    Returns:\n        Optional[pd.DataFrame]: DataFrame sem as linhas que continham zeros,\n                                 ou None se inplace=True.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame do pandas.\n        ValueError: Se alguma das colunas em 'cols' não existir no DataFrame ou\n                    se nenhuma coluna aplicável for encontrada.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame do pandas.\")\n\n    df_out = df if inplace else df.copy()\n\n    if cols is None:\n        cols_to_process = df_out.select_dtypes(include=np.number).columns.tolist()\n        if not cols_to_process:\n            print(\"Aviso: Nenhuma coluna numérica encontrada para verificar zeros.\")\n            return df_out if not inplace else None\n    else:\n        if not isinstance(cols, list) or not all(isinstance(c, str) for c in cols):\n            raise TypeError(\"'cols' deve ser uma lista de strings.\")\n        missing = [c for c in cols if c not in df_out.columns]\n        if missing:\n            raise ValueError(f\"As seguintes colunas não existem no df: {missing}\")\n        # Filtrar para garantir que as colunas selecionadas são numéricas para comparação com 0\n        numeric_cols_in_list = df_out[cols].select_dtypes(include=np.number).columns.tolist()\n        if len(numeric_cols_in_list) < len(cols):\n            non_numeric = [c for c in cols if c not in numeric_cols_in_list]\n            print(f\"Aviso: Ignorando colunas não numéricas especificadas em 'cols': {non_numeric}\")\n        cols_to_process = numeric_cols_in_list\n        if not cols_to_process:\n             print(\"Aviso: Nenhuma coluna numérica válida encontrada na lista 'cols' para verificar zeros.\")\n             return df_out if not inplace else None\n\n    # Cria uma máscara para linhas onde QUALQUER uma das colunas numéricas processadas é 0\n    zero_mask = (df_out[cols_to_process] == 0).any(axis=1)\n\n    # Obtém os índices das linhas a serem removidas\n    rows_to_drop = df_out.index[zero_mask]\n\n    if not rows_to_drop.empty:\n        df_out.drop(index=rows_to_drop, inplace=True) # drop modifica df_out\n\n    if inplace:\n        # Se inplace=True, df original já foi modificado (pois df_out era df)\n        return None\n    else:\n        # Se inplace=False, df_out é a cópia modificada\n        return df_out", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 37, "titulo": "calculate_trimmed_statistic", "categoria": "Estatística e Probabilidade", "subcategoria": "Estatísticas de Regressão e Tendência", "descricao": "Calcula uma estatística de tendência central (média ou mediana) após remover outliers.", "codigo_funcao": "def calculate_trimmed_statistic(\n    df: pd.DataFrame,\n    col: str,\n    method: Literal['std', 'iqr'] = 'iqr', # Adicionado método IQR\n    threshold: float = 1.5, # Default para IQR (ou 3.0 para std)\n    statistic: Literal['mean', 'median'] = 'median'\n) -> float:\n    \"\"\"\n    Descrição: Calcula uma estatística de tendência central (média ou mediana)\n               após remover outliers ('trimming') com base no método Z-Score (std) ou IQR.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        col (str): Nome da coluna numérica a ser analisada.\n        method (Literal['std', 'iqr'], optional): Método de detecção de outliers.\n            'std': Usa Z-Score (média ± threshold * desvio padrão).\n            'iqr': Usa Intervalo Interquartil (Q1 - threshold*IQR, Q3 + threshold*IQR).\n            Defaults to 'iqr'.\n        threshold (float, optional): O multiplicador para o desvio padrão (se method='std')\n            ou para o IQR (se method='iqr'). Defaults to 1.5 (comum para IQR).\n            Para 'std', um valor comum é 3.0.\n        statistic (Literal['mean', 'median'], optional): A estatística a ser calculada\n            sobre os dados aparados ('trimmed'). Defaults to 'median'.\n\n    Returns:\n        float: O valor da estatística calculada sobre os dados aparados.\n               Retorna np.nan se não houver dados após o trimming.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame ou coluna não for numérica.\n        ValueError: Se a coluna não existir, método for inválido, threshold for negativo,\n                    ou statistic for inválido.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame do pandas.\")\n    if col not in df.columns:\n        raise ValueError(f\"A coluna '{col}' não existe no DataFrame.\")\n    if not pd.api.types.is_numeric_dtype(df[col]):\n        raise TypeError(f\"A coluna '{col}' deve ser numérica.\")\n    if method not in ['std', 'iqr']:\n        raise ValueError(\"Método inválido. Escolha 'std' ou 'iqr'.\")\n    if threshold < 0:\n        raise ValueError(\"'threshold' não pode ser negativo.\")\n    if statistic not in ['mean', 'median']:\n        raise ValueError(\"Statistic inválido. Escolha 'mean' ou 'median'.\")\n\n    series = df[col].dropna()\n    if series.empty:\n        return np.nan # Retorna NaN se não houver dados não nulos\n\n    # Define os limites para trimming\n    if method == 'std':\n        mean = series.mean()\n        std = series.std()\n        if std == 0: # Coluna constante\n            lower_bound = upper_bound = mean\n        else:\n            lower_bound = mean - threshold * std\n            upper_bound = mean + threshold * std\n    else: # method == 'iqr'\n        Q1 = series.quantile(0.25)\n        Q3 = series.quantile(0.75)\n        IQR = Q3 - Q1\n        if IQR == 0: # Se IQR é zero, bounds são Q1 e Q3\n             lower_bound = Q1\n             upper_bound = Q3\n        else:\n            lower_bound = Q1 - threshold * IQR\n            upper_bound = Q3 + threshold * IQR\n\n    # Filtra (apara) a série\n    trimmed_series = series[(series >= lower_bound) & (series <= upper_bound)]\n\n    if trimmed_series.empty:\n        return np.nan # Retorna NaN se trimming remover todos os dados\n\n    # Calcula a estatística desejada\n    if statistic == 'mean':\n        return float(trimmed_series.mean())\n    else: # median\n        return float(trimmed_series.median())", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 38, "titulo": "get_linear_regression_stats", "categoria": "Estatística e Probabilidade", "subcategoria": "Estatísticas de Regressão e Tendência", "descricao": "Calcula as estatísticas de uma regressão linear simples (OLS) entre duas variáveis.", "codigo_funcao": "def get_linear_regression_stats(\n    x: pd.Series,\n    y: pd.Series\n) -> Dict[str, float]:\n    \"\"\"\n    Descrição: Calcula as estatísticas de uma regressão linear simples (OLS)\n               entre duas variáveis usando scipy.stats.linregress.\n\n    Args:\n        x (pd.Series): A variável independente (preditora, eixo X).\n        y (pd.Series): A variável dependente (resposta, eixo Y).\n\n    Returns:\n        Dict[str, float]: Um dicionário contendo:\n            'slope': Coeficiente angular da linha de regressão.\n            'intercept': Intercepto da linha de regressão.\n            'r_value': Coeficiente de correlação de Pearson (R).\n            'p_value': Valor-p do teste de hipótese (H0: slope = 0).\n            'stderr': Erro padrão da estimativa da inclinação.\n            'r_squared': Coeficiente de determinação (R²).\n\n    Raises:\n        TypeError: Se 'x' ou 'y' não forem Séries do pandas.\n        ValueError: Se as Séries tiverem comprimentos diferentes ou dados\n                    insuficientes (< 2 pontos) após remover NaNs alinhados.\n    \"\"\"\n    if not all(isinstance(s, pd.Series) for s in [x, y]):\n        raise TypeError(\"As entradas 'x' e 'y' devem ser Séries do pandas.\")\n    if len(x) != len(y):\n        raise ValueError(f\"As séries 'x' (len={len(x)}) e 'y' (len={len(y)}) devem ter o mesmo comprimento.\")\n\n    # Remove NaNs alinhados para garantir que usemos os mesmos pontos\n    combined = pd.concat([x.rename('x_val'), y.rename('y_val')], axis=1).dropna()\n    if len(combined) < 2:\n        raise ValueError(f\"Dados insuficientes ({len(combined)} pontos) para calcular a regressão após remover NaNs.\")\n\n    x_clean = combined['x_val']\n    y_clean = combined['y_val']\n\n    # Calcula a regressão linear usando SciPy\n    try:\n        slope, intercept, r_value, p_value, stderr = linregress(x=x_clean, y=y_clean)\n    except Exception as e:\n         raise RuntimeError(f\"Erro ao calcular linregress: {e}\")\n\n\n    # Retorna as estatísticas em um dicionário\n    return {\n        \"slope\": float(slope),\n        \"intercept\": float(intercept),\n        \"r_value\": float(r_value),\n        \"p_value\": float(p_value),\n        \"stderr\": float(stderr),\n        \"r_squared\": float(r_value**2) # Calcula R²\n    }", "bibliotecas": ["pandas", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 39, "titulo": "plot_countplot", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Cria um gráfico de contagem (countplot) para visualizar a frequência de categorias.", "codigo_funcao": "def plot_countplot(\n    df: pd.DataFrame,\n    col: str,\n    hue: Optional[str] = None,\n    ax: Optional[plt.Axes] = None,\n    **countplot_kwargs\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Cria um gráfico de contagem (countplot) usando Seaborn para visualizar\n               a frequência de cada categoria em uma coluna, opcionalmente separada\n               por outra coluna categórica ('hue').\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        col (str): Nome da coluna categórica cujas frequências serão plotadas no eixo X.\n        hue (Optional[str], optional): Nome da coluna categórica para diferenciar as\n                                       contagens por cor dentro de cada barra. Defaults to None.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n        **countplot_kwargs: Argumentos adicionais a serem passados diretamente para\n            `sns.countplot()` (ex: order=['A', 'B'], palette='viridis').\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se 'col' ou 'hue' (se fornecido) não existirem no DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame do pandas.\")\n    if col not in df.columns:\n        raise ValueError(f\"A coluna '{col}' não existe no DataFrame.\")\n    if hue and hue not in df.columns:\n        raise ValueError(f\"A coluna 'hue' ('{hue}') não existe no DataFrame.\")\n\n    if ax is None:\n        # Define um tamanho de figura padrão que pode ser sobrescrito por figsize nos kwargs\n        fig_width = countplot_kwargs.pop('fig_width', 8)\n        fig_height = countplot_kwargs.pop('fig_height', 6)\n        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    # Plota usando Seaborn\n    sns.countplot(data=df, x=col, hue=hue, ax=ax, **countplot_kwargs)\n\n    # Configurações adicionais\n    ax.set_title(countplot_kwargs.pop('title', f'Distribuição de {col}' + (f' por {hue}' if hue else '')))\n    ax.set_xlabel(col) # Label do X é o nome da coluna\n    ax.set_ylabel('Contagem')\n    # Rotacionar labels do eixo X se houver muitas categorias ou nomes longos\n    try: # Tenta obter número de categorias, pode falhar se col não for categórica ainda\n         if df[col].nunique() > 10:\n              ax.tick_params(axis='x', rotation=45, ha='right')\n    except:\n         pass # Ignora se não conseguir obter nunique\n\n    plt.tight_layout() # Ajusta layout\n\n    return ax", "bibliotecas": ["matplotlib", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 40, "titulo": "plot_boxplot_by_category", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Plota boxplots de uma coluna numérica, agrupados pelas categorias de outra coluna.", "codigo_funcao": "def plot_boxplot_by_category(\n    df: pd.DataFrame,\n    category_col: str,\n    numeric_col: str,\n    remove_fliers: bool = False,\n    ax: Optional[plt.Axes] = None,\n    **boxplot_kwargs\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Plota boxplots de uma coluna numérica usando Seaborn, agrupados\n               pelas categorias de outra coluna, para comparar distribuições.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        category_col (str): Coluna categórica a ser usada para o eixo X (grupos).\n        numeric_col (str): Coluna numérica a ser usada para o eixo Y (valores).\n        remove_fliers (bool, optional): Se True, os outliers (fliers) não são\n            plotados no boxplot. Defaults to False.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n        **boxplot_kwargs: Argumentos adicionais a serem passados diretamente para\n            `sns.boxplot()` (ex: order=['A', 'B'], palette='Set2').\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo os boxplots.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se 'category_col' ou 'numeric_col' não existirem no DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame do pandas.\")\n    if category_col not in df.columns:\n         raise ValueError(f\"A coluna de categoria '{category_col}' não existe.\")\n    if numeric_col not in df.columns:\n         raise ValueError(f\"A coluna numérica '{numeric_col}' não existe.\")\n    if not pd.api.types.is_numeric_dtype(df[numeric_col]):\n        print(f\"Aviso: A coluna '{numeric_col}' pode não ser numérica, o boxplot pode falhar ou ser inadequado.\")\n\n\n    if ax is None:\n        # Define um tamanho de figura padrão\n        fig_width = boxplot_kwargs.pop('fig_width', 10)\n        fig_height = boxplot_kwargs.pop('fig_height', 7)\n        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    # Plota usando Seaborn\n    sns.boxplot(\n        data=df,\n        x=category_col,\n        y=numeric_col,\n        showfliers=(not remove_fliers), \n        ax=ax,\n        **boxplot_kwargs\n    )\n\n    # Configurações adicionais\n    ax.set_title(boxplot_kwargs.pop('title', f'Distribuição de {numeric_col} por {category_col}'))\n    ax.set_xlabel(category_col) # Label do X\n    ax.set_ylabel(numeric_col) # Label do Y\n    # Rotacionar labels do eixo X se houver muitas categorias\n    try:\n         if df[category_col].nunique() > 8:\n              ax.tick_params(axis='x', rotation=45, ha='right')\n    except:\n         pass\n\n    plt.tight_layout() # Ajusta layout\n\n    return ax", "bibliotecas": ["matplotlib", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 41, "titulo": "plot_linear_regression_scatter", "categoria": "Visualização de Dados", "subcategoria": "Relações Bivariadas e Multivariadas", "descricao": "Plota um gráfico de dispersão com uma linha de regressão linear opcional.", "codigo_funcao": "def plot_linear_regression_scatter(\n    df: pd.DataFrame,\n    x_col: str,\n    y_col: str,\n    show_regression_line: bool = True,\n    ax: Optional[Axes] = None,\n    **regplot_kwargs\n) -> Axes: \n    \"\"\"\n    Descrição: Plota um gráfico de dispersão (scatter plot) entre duas variáveis\n               numéricas usando Seaborn, com uma linha de regressão linear opcional\n               e intervalo de confiança.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        x_col (str): Nome da coluna numérica para o eixo X.\n        y_col (str): Nome da coluna numérica para o eixo Y.\n        show_regression_line (bool, optional): Se True (padrão), exibe a linha de\n            regressão linear estimada e o intervalo de confiança. Se False,\n            mostra apenas os pontos (equivalente a sns.scatterplot). Defaults to True.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n        **regplot_kwargs: Argumentos adicionais a serem passados diretamente para\n            `sns.regplot()` (ex: color='red', scatter_kws={'s': 10, 'alpha': 0.5},\n            line_kws={'linestyle': '--'}).\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se 'x_col' ou 'y_col' não existirem no DataFrame ou não forem numéricas.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame do pandas.\")\n    if x_col not in df.columns:\n         raise ValueError(f\"A coluna X '{x_col}' não existe.\")\n    if y_col not in df.columns:\n         raise ValueError(f\"A coluna Y '{y_col}' não existe.\")\n    if not pd.api.types.is_numeric_dtype(df[x_col]):\n        print(f\"Aviso: A coluna X '{x_col}' pode não ser numérica.\")\n    if not pd.api.types.is_numeric_dtype(df[y_col]):\n        print(f\"Aviso: A coluna Y '{y_col}' pode não ser numérica.\")\n\n    if ax is None:\n        fig_width = regplot_kwargs.pop('fig_width', 8)\n        fig_height = regplot_kwargs.pop('fig_height', 6)\n        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    sns.regplot(\n        data=df,\n        x=x_col,\n        y=y_col,\n        fit_reg=show_regression_line,\n        ax=ax,\n        **regplot_kwargs\n    )\n\n    ax.set_title(regplot_kwargs.pop('title', f'Relação entre {x_col} e {y_col}' + (' com Regressão' if show_regression_line else '')))\n    ax.set_xlabel(x_col)\n    ax.set_ylabel(y_col)\n    ax.grid(True, linestyle='--', alpha=0.5)\n\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 42, "titulo": "plot_histogram", "categoria": "Visualização de Dados", "subcategoria": "Distribuições Univariadas", "descricao": "Cria um histograma ou densidade para uma coluna, com opções de separação (hue) e KDE.", "codigo_funcao": "def plot_histogram(\n    df: pd.DataFrame,\n    col: str,\n    hue: Optional[str] = None,\n    bins: Union[int, List[float], str] = 'auto', \n    binwidth: Optional[float] = None, \n    discrete: Optional[bool] = None,\n    stat: Literal['count', 'frequency', 'probability', 'percent', 'density'] = 'count',\n    multiple: Literal['layer', 'dodge', 'stack', 'fill'] = 'layer', \n    kde: bool = False, \n    ax: Optional[plt.Axes] = None,\n    **histplot_kwargs \n) -> plt.Axes:\n    \"\"\"\n    Descrição: Cria e exibe uma distribuição de frequências (histograma) ou densidade\n               para uma coluna usando sns.histplot, com opções de separação e KDE.\n\n    Args:\n        df (pd.DataFrame): O DataFrame que contém os dados.\n        col (str): O nome da coluna para a qual a distribuição será plotada (eixo X).\n        hue (Optional[str], optional): Nome da coluna para separar os dados por cor. Default: None.\n        bins (Union[int, List[float], str], optional): Especificação dos bins. Pode ser\n            um inteiro (número de bins), uma lista de bordas, ou uma string\n            (ex: 'auto', 'fd', 'doane', 'scott', 'stone', 'rice', 'sturges', 'sqrt').\n            Defaults to 'auto'.\n        binwidth (Optional[float], optional): Largura de cada intervalo (bin). Sobrepõe 'bins' se fornecido.\n            Defaults to None.\n        discrete (Optional[bool], optional): Se True, trata a variável `col` como discreta\n            (centraliza barras nos valores). Se None (padrão), infere. Defaults to None.\n        stat (Literal['count', 'frequency', 'probability', 'percent', 'density'], optional):\n            Estatística a agregar em cada bin. Defaults to 'count'.\n        multiple (Literal['layer', 'dodge', 'stack', 'fill'], optional): Como plotar múltiplas\n            distribuições (quando `hue` é usado). Defaults to 'layer'.\n        kde (bool, optional): Se True, adiciona uma curva de estimativa de densidade de kernel (KDE).\n            Defaults to False.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib para desenhar o gráfico. Se None,\n            um novo eixo é criado. Defaults to None.\n        **histplot_kwargs: Outros argumentos de palavra-chave a serem passados diretamente\n            para `sns.histplot` (ex: palette, alpha, log_scale).\n\n    Returns:\n        plt.Axes: O objeto de eixos Matplotlib com o gráfico.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se a coluna `col` ou `hue` (se fornecida) não for encontrada no DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if col not in df.columns:\n        raise ValueError(f\"A coluna '{col}' não foi encontrada no DataFrame.\")\n    if hue and hue not in df.columns:\n        raise ValueError(f\"A coluna de hue '{hue}' não foi encontrada no DataFrame.\")\n\n    # Validação dos parâmetros stat e multiple\n    valid_stats = ['count', 'frequency', 'probability', 'percent', 'density']\n    valid_multiples = ['layer', 'dodge', 'stack', 'fill']\n    if stat not in valid_stats:\n        raise ValueError(f\"'stat' inválido. Escolha entre: {valid_stats}\")\n    if multiple not in valid_multiples:\n        raise ValueError(f\"'multiple' inválido. Escolha entre: {valid_multiples}\")\n\n\n    if ax is None:\n        # Define um tamanho de figura padrão\n        fig_width = histplot_kwargs.pop('fig_width', 8)\n        fig_height = histplot_kwargs.pop('fig_height', 5)\n        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    # Plota usando histplot (nível de eixos)\n    sns.histplot(\n        data=df,\n        x=col,\n        hue=hue,\n        bins=bins,\n        binwidth=binwidth,\n        discrete=discrete,\n        stat=stat,\n        multiple=multiple,\n        kde=kde,\n        ax=ax,\n        **histplot_kwargs\n    )\n\n    # Configurações adicionais\n    ax.set_title(histplot_kwargs.pop('title', f'Distribuição de {col}' + (f' por {hue}' if hue else '')))\n    ax.set_xlabel(col)\n    ax.set_ylabel(stat.capitalize()) # Label do Y reflete a estatística\n\n    # Adiciona legenda apenas se hue for usado e não houver legenda nos kwargs\n    if hue and 'legend' not in histplot_kwargs:\n         handles, labels = ax.get_legend_handles_labels()\n         # Evitar legendas duplicadas se histplot já criou uma\n         if handles: # Verifica se a legenda já existe\n             ax.legend(title=hue) # Adiciona título à legenda existente\n\n    plt.tight_layout() # Ajusta layout\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 43, "titulo": "calculate_descriptive_statistics", "categoria": "Exploração e Limpeza de Dados", "subcategoria": "Análise Descritiva", "descricao": "Calcula estatísticas descritivas para uma coluna numérica e retorna como um dicionário.", "codigo_funcao": "def calculate_descriptive_statistics(\n    df: pd.DataFrame,\n    col: str\n) -> Dict[str, Any]:\n    \"\"\"\n    Descrição: Calcula e retorna as principais estatísticas descritivas para uma\n               coluna numérica usando o método .describe() do pandas.\n\n    Args:\n        df (pd.DataFrame): O DataFrame que contém os dados.\n        col (str): O nome da coluna numérica a ser analisada.\n\n    Returns:\n        Dict[str, Any]: Um dicionário contendo as estatísticas calculadas pelo\n                        método .describe() (count, mean, std, min, 25%, 50%, 75%, max).\n                        Os valores são convertidos para tipos nativos Python.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas ou se `col` não for numérica.\n        ValueError: Se `col` não for encontrada no DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if col not in df.columns:\n        raise ValueError(f\"A coluna '{col}' não foi encontrada no DataFrame.\")\n    if not pd.api.types.is_numeric_dtype(df[col]):\n        raise TypeError(f\"A coluna '{col}' deve ser numérica para calcular estatísticas descritivas.\")\n\n    # Calcula as estatísticas e converte para dicionário com tipos nativos\n    stats_series = df[col].describe()\n    stats_dict = stats_series.to_dict()\n\n    # Converter tipos numpy (ex: np.float64) para tipos Python nativos (float)\n    for key, value in stats_dict.items():\n        if isinstance(value, (np.generic, np.ndarray)): # Checa por tipos numpy\n             stats_dict[key] = value.item() # Converte para tipo Python nativo\n\n    return stats_dict", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 44, "titulo": "plot_kernel_density", "categoria": "Visualização de Dados", "subcategoria": "Distribuições Univariadas", "descricao": "Plota a Estimativa de Densidade Kernel (KDE) para uma coluna numérica, com 'hue' opcional.", "codigo_funcao": "def plot_kernel_density(\n    df: pd.DataFrame,\n    col: str,\n    hue: Optional[str] = None,\n    ax: Optional[Axes] = None, # <-- TIPO CORRIGIDO\n    **kdeplot_kwargs\n) -> Axes: # <-- TIPO CORRIGIDO\n    \"\"\"\n    Descrição: Plota a Estimativa de Densidade Kernel (KDE) para uma coluna numérica\n               usando sns.kdeplot, opcionalmente segmentada por 'hue'.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        col (str): Nome da coluna numérica a ser plotada.\n        hue (Optional[str], optional): Nome da coluna categórica para segmentar a densidade. Defaults to None.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib para desenhar o gráfico. Se None, um novo é criado.\n        **kdeplot_kwargs: Outros argumentos de palavra-chave a serem passados diretamente\n            para `sns.kdeplot` (ex: fill=True, palette='viridis', bw_adjust=0.5).\n\n    Returns:\n        plt.Axes: O objeto de eixos Matplotlib com o gráfico KDE.\n\n    Raises:\n        TypeError: Se 'df' não for DataFrame ou 'col' não for numérica.\n        ValueError: Se 'col' ou 'hue' não forem encontrados no DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if col not in df.columns:\n        raise ValueError(f\"A coluna '{col}' não foi encontrada no DataFrame.\")\n    if hue and hue not in df.columns:\n        raise ValueError(f\"A coluna de hue '{hue}' não foi encontrada no DataFrame.\")\n    if not pd.api.types.is_numeric_dtype(df[col]):\n        raise TypeError(f\"A coluna '{col}' deve ser numérica para plotar KDE.\")\n\n    if ax is None:\n        fig_width = kdeplot_kwargs.pop('fig_width', 8)\n        fig_height = kdeplot_kwargs.pop('fig_height', 5)\n        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    sns.kdeplot(data=df, x=col, hue=hue, ax=ax, **kdeplot_kwargs)\n\n    ax.set_title(kdeplot_kwargs.pop('title', f'Densidade Kernel de {col}' + (f' por {hue}' if hue else '')))\n    ax.set_xlabel(col)\n    ax.set_ylabel('Densidade')\n    if hue:\n        handles, labels = ax.get_legend_handles_labels()\n        if handles: ax.legend(title=hue)\n\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 45, "titulo": "plot_violin", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Cria um gráfico de violino para visualizar distribuições numéricas agrupadas por categorias.", "codigo_funcao": "def plot_violin(\n    df: pd.DataFrame,\n    y_col: str,\n    x_col: Optional[str] = None,\n    hue: Optional[str] = None,\n    ax: Optional[Axes] = None, \n    **violinplot_kwargs\n) -> Axes:\n    \"\"\"\n    Descrição: Cria um gráfico de violino usando sns.violinplot para visualizar\n               a distribuição de dados numéricos, opcionalmente agrupados por\n               categorias em 'x_col' e/ou 'hue'.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        y_col (str): Nome da coluna numérica para o eixo Y.\n        x_col (Optional[str], optional): Nome da coluna categórica para o eixo X. Defaults to None.\n        hue (Optional[str], optional): Nome da coluna para segmentação por cor. Defaults to None.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib para o gráfico.\n        **violinplot_kwargs: Outros argumentos para `sns.violinplot`.\n\n    Returns:\n        plt.Axes: O objeto de eixos Matplotlib com o gráfico de violino.\n\n    Raises:\n        TypeError: Se 'df' não for DataFrame ou 'y_col' não for numérica.\n        ValueError: Se 'y_col', 'x_col', ou 'hue' não forem encontrados no DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if y_col not in df.columns:\n        raise ValueError(f\"A coluna y '{y_col}' não foi encontrada.\")\n    if x_col and x_col not in df.columns:\n        raise ValueError(f\"A coluna x '{x_col}' não foi encontrada.\")\n    if hue and hue not in df.columns:\n        raise ValueError(f\"A coluna de hue '{hue}' não foi encontrada.\")\n    if not pd.api.types.is_numeric_dtype(df[y_col]):\n        raise TypeError(f\"A coluna y '{y_col}' deve ser numérica.\")\n\n    if ax is None:\n        fig_width = violinplot_kwargs.pop('fig_width', 10)\n        fig_height = violinplot_kwargs.pop('fig_height', 6)\n        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    sns.violinplot(data=df, x=x_col, y=y_col, hue=hue, ax=ax, **violinplot_kwargs)\n\n    title = f'Distribuição de {y_col}'\n    if x_col: title += f' por {x_col}'\n    if hue: title += f' (segmentado por {hue})'\n    ax.set_title(violinplot_kwargs.pop('title', title))\n    ax.set_xlabel(x_col if x_col else '')\n    ax.set_ylabel(y_col)\n    try:\n         if x_col and df[x_col].nunique() > 8:\n              ax.tick_params(axis='x', rotation=45, ha='right')\n    except:\n         pass\n\n    if hue:\n        handles, labels = ax.get_legend_handles_labels()\n        if handles: ax.legend(title=hue)\n\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 46, "titulo": "plot_pairplot", "categoria": "Visualização de Dados", "subcategoria": "Relações Bivariadas e Multivariadas", "descricao": "Cria uma matriz de gráficos de dispersão (pairplot) para análise multivariada.", "codigo_funcao": "def plot_pairplot(\n    df: pd.DataFrame,\n    cols: Optional[List[str]] = None,\n    hue: Optional[str] = None,\n    corner: bool = False,\n    **pairplot_kwargs\n) -> sns.PairGrid:\n    \"\"\"\n    Descrição: Cria uma matriz de gráficos de dispersão (pairplot) usando sns.pairplot\n               para análise exploratória multivariada, mostrando relações par-a-par\n               e distribuições univariadas.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols (Optional[List[str]], optional): Lista de colunas a serem incluídas no pairplot.\n            Se None, utiliza todas as colunas numéricas do DataFrame. Defaults to None.\n        hue (Optional[str], optional): Nome da coluna categórica para segmentar os plots por cor.\n            Defaults to None.\n        corner (bool, optional): Se True, omite os gráficos redundantes do triângulo superior\n            e as diagonais correspondentes para economizar espaço. Defaults to False.\n        **pairplot_kwargs: Outros argumentos de palavra-chave a serem passados diretamente\n            para `sns.pairplot` (ex: kind='reg', diag_kind='kde', palette='husl', markers=['o', 's']).\n\n    Returns:\n        sns.PairGrid: O objeto PairGrid retornado pelo Seaborn, que permite customizações posteriores.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se 'cols' ou 'hue' contiverem colunas inexistentes, ou se nenhuma\n                    coluna aplicável for encontrada.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if hue and hue not in df.columns:\n        raise ValueError(f\"A coluna de hue '{hue}' não foi encontrada no DataFrame.\")\n\n    df_to_plot = df # Por padrão, usa o df todo (pairplot seleciona numéricas)\n\n    if cols:\n        if not isinstance(cols, list):\n             raise TypeError(\"'cols' deve ser uma lista de nomes de colunas.\")\n        missing = [c for c in cols if c not in df.columns]\n        if missing:\n            raise ValueError(f\"As seguintes colunas não existem no df: {missing}\")\n        # Seleciona apenas as colunas especificadas\n        df_to_plot = df[cols + ([hue] if hue and hue not in cols else [])] # Inclui hue se necessário\n        # Verifica se há colunas numéricas suficientes\n        numeric_in_cols = df_to_plot.select_dtypes(include=np.number).columns\n        if len(numeric_in_cols) < 2 and 'vars' not in pairplot_kwargs: # `vars` pode sobrescrever `cols`\n             raise ValueError(\"Pelo menos duas colunas numéricas são necessárias para o pairplot \"\n                              \"nas colunas selecionadas ou inferidas.\")\n    elif 'vars' not in pairplot_kwargs: # Se cols é None e vars não foi passado\n         numeric_cols = df.select_dtypes(include=np.number).columns\n         if len(numeric_cols) < 2:\n              raise ValueError(\"Pelo menos duas colunas numéricas são necessárias para o pairplot \"\n                               \"quando 'cols' ou 'vars' não são especificados.\")\n         # pairplot já seleciona numéricas por padrão se nada for passado\n\n    # Plota usando pairplot\n    # pairplot é a nível de figura, retorna o PairGrid\n    g = sns.pairplot(\n        data=df_to_plot, # Passa o df \n        vars=cols if cols else None, # Usa `vars` para garantir que apenas `cols` sejam plotados se fornecido\n        hue=hue,\n        corner=corner,\n        **pairplot_kwargs\n    )\n\n    # Adiciona um título geral se não houver um nos kwargs\n    if 'fig_kws' not in pairplot_kwargs or 'suptitle' not in pairplot_kwargs.get('fig_kws', {}):\n        title = 'Pairplot das Variáveis'\n        if hue: title += f' (Colorido por {hue})'\n        g.figure.suptitle(title, y=1.02) # y > 1 para colocar acima dos subplots\n\n    return g", "bibliotecas": ["matplotlib", "numpy", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 47, "titulo": "calculate_binomial_pmf", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Calcula a Função Massa de Probabilidade (PMF) para uma distribuição binomial.", "codigo_funcao": "def calculate_binomial_pmf(k: int, n: int, p: float) -> float:\n    \"\"\"\n    Calcula a Função Massa de Probabilidade (PMF) para uma distribuição binomial.\n\n    Retorna a probabilidade de se obter exatamente 'k' sucessos em 'n' tentativas,\n    dada uma probabilidade de sucesso 'p' em cada tentativa. P(X=k).\n\n    Args:\n        k (int): O número de sucessos (valor pontual >= 0).\n        n (int): O número total de tentativas (>= 0).\n        p (float): A probabilidade de sucesso em uma única tentativa (entre 0 e 1).\n\n    Returns:\n        float: A probabilidade P(X=k).\n\n    Raises:\n        ValueError: Se os parâmetros 'n', 'k' ou 'p' forem inválidos (n<0, k<0, k>n, 0<=p<=1).\n        TypeError: Se k ou n não forem inteiros.\n    \"\"\"\n    if not isinstance(k, int) or not isinstance(n, int):\n         raise TypeError(\"'k' e 'n' devem ser inteiros.\")\n    if not isinstance(p, (int, float)): # Permite p=0 ou p=1 como int\n         raise TypeError(\"'p' deve ser um número (float ou int).\")\n    if not (0 <= p <= 1):\n        raise ValueError(\"A probabilidade 'p' deve estar no intervalo [0, 1].\")\n    if n < 0 or k < 0:\n        raise ValueError(\"O número de tentativas 'n' e de sucessos 'k' não pode ser negativo.\")\n    if k > n:\n        # PMF é 0 se k>n, mas levantamos erro para consistência com outras validações\n        raise ValueError(\"O número de sucessos 'k' não pode ser maior que o número de tentativas 'n'.\")\n\n    # Scipy lida bem com k>n (retorna 0), mas mantemos a validação acima\n    return float(binom.pmf(k=k, n=n, p=p))", "bibliotecas": ["scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 48, "titulo": "calculate_binomial_cdf", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Calcula a Função de Distribuição Acumulada (CDF) para uma distribuição binomial.", "codigo_funcao": "def calculate_binomial_cdf(k: int, n: int, p: float) -> float:\n    \"\"\"\n    Calcula a Função de Distribuição Acumulada (CDF) para uma distribuição binomial.\n\n    Retorna a probabilidade de se obter 'k' ou menos sucessos em 'n' tentativas. P(X <= k).\n\n    Args:\n        k (int): O número de sucessos (>= 0).\n        n (int): O número total de tentativas (>= 0).\n        p (float): A probabilidade de sucesso em uma única tentativa (entre 0 e 1).\n\n    Returns:\n        float: A probabilidade P(X <= k).\n\n    Raises:\n        ValueError: Se os parâmetros 'n', 'k' ou 'p' forem inválidos.\n        TypeError: Se k ou n não forem inteiros.\n    \"\"\"\n    if not isinstance(k, int) or not isinstance(n, int):\n         raise TypeError(\"'k' e 'n' devem ser inteiros.\")\n    if not isinstance(p, (int, float)):\n         raise TypeError(\"'p' deve ser um número (float ou int).\")\n    if not (0 <= p <= 1):\n        raise ValueError(\"A probabilidade 'p' deve estar no intervalo [0, 1].\")\n    if n < 0 or k < 0:\n        raise ValueError(\"O número de tentativas 'n' e de sucessos 'k' não pode ser negativo.\")\n    # k pode ser > n, a CDF será 1 nesses casos, o que é correto.\n\n    return float(binom.cdf(k=k, n=n, p=p))", "bibliotecas": ["scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 49, "titulo": "calculate_binomial_sf", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Calcula a Função de Sobrevivência (SF) para uma distribuição binomial.", "codigo_funcao": "def calculate_binomial_sf(k: int, n: int, p: float) -> float:\n    \"\"\"\n    Calcula a Função de Sobrevivência (SF) para uma distribuição binomial.\n\n    Retorna a probabilidade de se obter mais de 'k' sucessos em 'n' tentativas. P(X > k).\n\n    Args:\n        k (int): O número de sucessos (>= 0).\n        n (int): O número total de tentativas (>= 0).\n        p (float): A probabilidade de sucesso em uma única tentativa (entre 0 e 1).\n\n    Returns:\n        float: A probabilidade P(X > k).\n\n    Raises:\n        ValueError: Se os parâmetros 'n', 'k' ou 'p' forem inválidos.\n        TypeError: Se k ou n não forem inteiros.\n    \"\"\"\n    if not isinstance(k, int) or not isinstance(n, int):\n         raise TypeError(\"'k' e 'n' devem ser inteiros.\")\n    if not isinstance(p, (int, float)):\n         raise TypeError(\"'p' deve ser um número (float ou int).\")\n    if not (0 <= p <= 1):\n        raise ValueError(\"A probabilidade 'p' deve estar no intervalo [0, 1].\")\n    if n < 0 or k < 0:\n        raise ValueError(\"O número de tentativas 'n' e de sucessos 'k' não pode ser negativo.\")\n    # k pode ser >= n, a SF será 0 nesses casos, o que é correto.\n\n    return float(binom.sf(k=k, n=n, p=p))", "bibliotecas": ["scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 50, "titulo": "plot_binomial_distribution", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Plota um gráfico de barras da Função Massa de Probabilidade (PMF) de uma distribuição binomial.", "codigo_funcao": "def plot_binomial_distribution(\n    n: int,\n    p: float,\n    ax: Optional[Axes] = None,  \n    **barplot_kwargs\n) -> Axes:  \n    \"\"\"\n    Gera e plota um gráfico de barras da Função Massa de Probabilidade (PMF)\n    de uma distribuição binomial.\n\n    Args:\n        n (int): O número total de tentativas (>= 0).\n        p (float): A probabilidade de sucesso em cada tentativa (entre 0 e 1).\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib para desenhar o gráfico.\n            Se None, um novo eixo é criado. Defaults to None.\n        **barplot_kwargs: Outros argumentos de palavra-chave a serem passados\n            diretamente para `sns.barplot` (ex: color='skyblue', alpha=0.8).\n\n    Returns:\n        plt.Axes: O objeto de eixos Matplotlib com o gráfico.\n\n    Raises:\n        ValueError: Se os parâmetros 'n' ou 'p' forem inválidos.\n        TypeError: Se n não for inteiro.\n    \"\"\"\n    if not isinstance(n, int): raise TypeError(\"'n' deve ser inteiro.\")\n    if n < 0:\n        raise ValueError(\"O número de tentativas 'n' não pode ser negativo.\")\n    if not isinstance(p, (int, float)): raise TypeError(\"'p' deve ser numérico.\")\n    if not (0 <= p <= 1):\n        raise ValueError(\"A probabilidade 'p' deve estar no intervalo [0, 1].\")\n\n    if ax is None:\n        fig_width = barplot_kwargs.pop('fig_width', 10)\n        fig_height = barplot_kwargs.pop('fig_height', 6)\n        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    k_values = np.arange(0, n + 1)\n    probabilities = binom.pmf(k=k_values, n=n, p=p)\n\n    sns.barplot(x=k_values, y=probabilities, ax=ax, **barplot_kwargs)\n\n    ax.set_title(barplot_kwargs.pop('title', f'Distribuição Binomial (n={n}, p={p:.2f})'))\n    ax.set_xlabel('Número de Sucessos (k)')\n    ax.set_ylabel('Probabilidade P(X=k)')\n    ax.grid(axis='y', linestyle='--', alpha=0.6)\n\n    if n > 20:\n         ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=10))\n\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "scipy", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 51, "titulo": "calculate_poisson_pmf", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Calcula a Função Massa de Probabilidade (PMF) para uma distribuição de Poisson.", "codigo_funcao": "def calculate_poisson_pmf(k: int, mu: float) -> float:\n    \"\"\"\n    Calcula a Função Massa de Probabilidade (PMF) para uma distribuição de Poisson.\n\n    Retorna a probabilidade de um evento ocorrer exatamente 'k' vezes em um intervalo,\n    dado que a taxa média de ocorrência nesse intervalo é 'mu' (lambda). P(X=k).\n\n    Args:\n        k (int): O número de ocorrências (valor pontual >= 0).\n        mu (float): A taxa média de ocorrências (lambda, >= 0).\n\n    Returns:\n        float: A probabilidade P(X=k).\n\n    Raises:\n        ValueError: Se 'k' ou 'mu' forem negativos.\n        TypeError: Se k não for inteiro ou mu não for numérico.\n    \"\"\"\n    if not isinstance(k, int): raise TypeError(\"'k' deve ser inteiro.\")\n    if not isinstance(mu, (int, float)): raise TypeError(\"'mu' deve ser numérico.\")\n    if k < 0 or mu < 0:\n        raise ValueError(\"O número de ocorrências 'k' e a taxa média 'mu' não podem ser negativos.\")\n\n    return float(poisson.pmf(k=k, mu=mu))", "bibliotecas": ["scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 52, "titulo": "calculate_poisson_cdf", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Calcula a Função de Distribuição Acumulada (CDF) para uma distribuição de Poisson.", "codigo_funcao": "def calculate_poisson_cdf(k: int, mu: float) -> float:\n    \"\"\"\n    Calcula a Função de Distribuição Acumulada (CDF) para uma distribuição de Poisson.\n\n    Retorna a probabilidade de um evento ocorrer 'k' vezes ou menos. P(X <= k).\n\n    Args:\n        k (int): O número de ocorrências (>= 0).\n        mu (float): A taxa média de ocorrências (lambda, >= 0).\n\n    Returns:\n        float: A probabilidade P(X <= k).\n\n    Raises:\n        ValueError: Se 'k' ou 'mu' forem negativos.\n        TypeError: Se k não for inteiro ou mu não for numérico.\n    \"\"\"\n    if not isinstance(k, int): raise TypeError(\"'k' deve ser inteiro.\")\n    if not isinstance(mu, (int, float)): raise TypeError(\"'mu' deve ser numérico.\")\n    if k < 0 or mu < 0:\n        raise ValueError(\"O número de ocorrências 'k' e a taxa média 'mu' não podem ser negativos.\")\n\n    return float(poisson.cdf(k=k, mu=mu))", "bibliotecas": ["scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 53, "titulo": "calculate_poisson_sf", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Calcula a Função de Sobrevivência (SF) para uma distribuição de Poisson.", "codigo_funcao": "def calculate_poisson_sf(k: int, mu: float) -> float:\n    \"\"\"\n    Calcula a Função de Sobrevivência (SF) para uma distribuição de Poisson.\n\n    Retorna a probabilidade de um evento ocorrer mais de 'k' vezes. P(X > k).\n\n    Args:\n        k (int): O número de ocorrências (>= 0).\n        mu (float): A taxa média de ocorrências (lambda, >= 0).\n\n    Returns:\n        float: A probabilidade P(X > k).\n\n    Raises:\n        ValueError: Se 'k' ou 'mu' forem negativos.\n        TypeError: Se k não for inteiro ou mu não for numérico.\n    \"\"\"\n    if not isinstance(k, int): raise TypeError(\"'k' deve ser inteiro.\")\n    if not isinstance(mu, (int, float)): raise TypeError(\"'mu' deve ser numérico.\")\n    if k < 0 or mu < 0:\n        raise ValueError(\"O número de ocorrências 'k' e a taxa média 'mu' não podem ser negativos.\")\n\n    return float(poisson.sf(k=k, mu=mu))", "bibliotecas": ["scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 54, "titulo": "plot_poisson_distribution", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Plota um gráfico de barras da Função Massa de Probabilidade (PMF) de uma distribuição de Poisson.", "codigo_funcao": "def plot_poisson_distribution(\n    mu: float,\n    max_k: Optional[int] = None,\n    ax: Optional[Axes] = None, \n    **barplot_kwargs\n) -> Axes: \n    \"\"\"\n    Gera e plota um gráfico de barras da Função Massa de Probabilidade (PMF)\n    de uma distribuição de Poisson.\n\n    Args:\n        mu (float): A taxa média de ocorrências (lambda, >= 0).\n        max_k (Optional[int], optional): O valor máximo de 'k' (ocorrências) a ser\n            plotado no eixo X. Se None, um valor razoável é estimado.\n            Defaults to None.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib para desenhar o gráfico.\n            Se None, um novo eixo é criado. Defaults to None.\n        **barplot_kwargs: Outros argumentos de palavra-chave a serem passados\n            diretamente para `sns.barplot` (ex: color='purple', alpha=0.7).\n\n    Returns:\n        plt.Axes: O objeto de eixos Matplotlib com o gráfico.\n\n    Raises:\n        ValueError: Se 'mu' for negativo ou 'max_k' (se fornecido) for negativo.\n        TypeError: Se mu não for numérico ou max_k (se fornecido) não for inteiro.\n    \"\"\"\n    if not isinstance(mu, (int, float)): raise TypeError(\"'mu' deve ser numérico.\")\n    if mu < 0:\n        raise ValueError(\"A taxa média 'mu' não pode ser negativa.\")\n\n    if max_k is None:\n        if mu == 0:\n             max_k = 5\n        else:\n             max_k = int(np.ceil(mu + 3 * np.sqrt(mu)))\n             max_k = max(max_k, 10)\n    else:\n        if not isinstance(max_k, int): raise TypeError(\"'max_k' deve ser um inteiro ou None.\")\n        if max_k < 0: raise ValueError(\"'max_k' não pode ser negativo.\")\n\n    if ax is None:\n        fig_width = barplot_kwargs.pop('fig_width', 10)\n        fig_height = barplot_kwargs.pop('fig_height', 6)\n        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    k_values = np.arange(0, max_k + 1)\n    probabilities = poisson.pmf(k=k_values, mu=mu)\n\n    sns.barplot(x=k_values, y=probabilities, ax=ax, **barplot_kwargs)\n\n    ax.set_title(barplot_kwargs.pop('title', f'Distribuição de Poisson (μ={mu:.2f})'))\n    ax.set_xlabel('Número de Ocorrências (k)')\n    ax.set_ylabel('Probabilidade P(X=k)')\n    ax.grid(axis='y', linestyle='--', alpha=0.6)\n\n    if max_k > 20:\n         ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=10))\n\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "scipy", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 55, "titulo": "calculate_normal_probability", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Calcula a probabilidade em uma distribuição normal para um dado intervalo (P(X<=a), P(X>b), P(a<X<b)).", "codigo_funcao": "def calculate_normal_probability(\n    mean: float,\n    std_dev: float,\n    x_lower: Optional[float] = None,\n    x_upper: Optional[float] = None\n) -> float:\n    \"\"\"\n    Calcula a probabilidade em uma distribuição normal para um dado intervalo.\n\n    A função pode calcular P(X <= x_upper), P(X > x_lower) ou P(x_lower < X < x_upper).\n    - Para P(X <= x_upper), forneça apenas x_upper.\n    - Para P(X > x_lower), forneça apenas x_lower.\n    - Para P(x_lower < X < x_upper), forneça ambos.\n\n    Args:\n        mean (float): A média (μ) da distribuição normal.\n        std_dev (float): O desvio padrão (σ) da distribuição normal (deve ser > 0).\n        x_lower (Optional[float], optional): O limite inferior do intervalo. Default: None.\n        x_upper (Optional[float], optional): O limite superior do intervalo. Default: None.\n\n    Returns:\n        float: A probabilidade calculada para o intervalo especificado.\n\n    Raises:\n        TypeError: Se mean ou std_dev não forem numéricos.\n        ValueError: Se nenhum limite (x_lower, x_upper) for fornecido, se std_dev <= 0,\n                    ou se x_lower >= x_upper quando ambos são fornecidos.\n    \"\"\"\n    if not isinstance(mean, (int, float)): raise TypeError(\"'mean' deve ser numérico.\")\n    if not isinstance(std_dev, (int, float)): raise TypeError(\"'std_dev' deve ser numérico.\")\n    if std_dev <= 0:\n        raise ValueError(\"O desvio padrão (std_dev) deve ser um valor positivo.\")\n    if x_lower is None and x_upper is None:\n        raise ValueError(\"Pelo menos um dos limites (x_lower ou x_upper) deve ser fornecido.\")\n\n    # Usar -np.inf e np.inf implicitamente se limites não forem dados\n    cdf_lower = 0.0\n    cdf_upper = 1.0\n\n    if x_lower is not None:\n         if not isinstance(x_lower, (int, float)): raise TypeError(\"'x_lower' deve ser numérico ou None.\")\n         cdf_lower = norm.cdf(x=x_lower, loc=mean, scale=std_dev)\n\n    if x_upper is not None:\n         if not isinstance(x_upper, (int, float)): raise TypeError(\"'x_upper' deve ser numérico ou None.\")\n         cdf_upper = norm.cdf(x=x_upper, loc=mean, scale=std_dev)\n\n    # Caso P(X <= x_upper)\n    if x_lower is None and x_upper is not None:\n        return float(cdf_upper)\n    # Caso P(X > x_lower)\n    elif x_lower is not None and x_upper is None:\n        return float(1.0 - cdf_lower) # Equivalente a norm.sf\n    # Caso P(x_lower < X < x_upper)\n    elif x_lower is not None and x_upper is not None:\n        if x_lower >= x_upper:\n            raise ValueError(\"O limite inferior (x_lower) deve ser estritamente menor que o limite superior (x_upper).\")\n        return float(cdf_upper - cdf_lower)\n    else:\n         # Inalcançável devido à validação inicial, mas garante retorno\n         raise RuntimeError(\"Lógica interna inválida na função calculate_normal_probability.\")", "bibliotecas": ["scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 56, "titulo": "find_normal_quantile", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Encontra o valor (quantil) que corresponde a uma probabilidade acumulada em uma distribuição normal.", "codigo_funcao": "def find_normal_quantile(\n    quantile: float,\n    mean: float,\n    std_dev: float\n) -> float:\n    \"\"\"\n    Encontra o valor 'x' que corresponde a um quantil (probabilidade acumulada)\n    em uma distribuição normal, usando a Função Ponto Percentual (PPF).\n\n    Args:\n        quantile (float): A probabilidade acumulada (estritamente entre 0 e 1)\n                          para a qual se deseja encontrar o valor de 'x'.\n        mean (float): A média (μ) da distribuição normal.\n        std_dev (float): O desvio padrão (σ) da distribuição normal (deve ser > 0).\n\n    Returns:\n        float: O valor de 'x' tal que P(X <= x) = quantile.\n\n    Raises:\n        TypeError: Se quantile, mean ou std_dev não forem numéricos.\n        ValueError: Se o quantil não estiver no intervalo (0, 1) ou se std_dev <= 0.\n    \"\"\"\n    if not isinstance(quantile, (int, float)): raise TypeError(\"'quantile' deve ser numérico.\")\n    if not isinstance(mean, (int, float)): raise TypeError(\"'mean' deve ser numérico.\")\n    if not isinstance(std_dev, (int, float)): raise TypeError(\"'std_dev' deve ser numérico.\")\n    if not (0 < quantile < 1):\n        raise ValueError(\"O quantil (quantile) deve estar no intervalo estrito (0, 1).\")\n    if std_dev <= 0:\n        raise ValueError(\"O desvio padrão (std_dev) deve ser um valor positivo.\")\n\n    return float(norm.ppf(q=quantile, loc=mean, scale=std_dev))", "bibliotecas": ["scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 57, "titulo": "find_normal_central_interval", "categoria": "Estatística e Probabilidade", "subcategoria": "Distribuições de Probabilidade", "descricao": "Calcula o intervalo central simétrico que contém uma dada porcentagem dos dados em uma distribuição normal.", "codigo_funcao": "def find_normal_central_interval(\n    confidence_level: float,\n    mean: float,\n    std_dev: float\n) -> Tuple[float, float]:\n    \"\"\"\n    Calcula o intervalo central [a, b] simétrico em torno da média que contém\n    uma dada porcentagem (nível de confiança) dos dados em uma distribuição normal.\n\n    Args:\n        confidence_level (float): O nível de confiança desejado (estritamente entre 0 e 1),\n                                  representando a proporção de dados dentro do intervalo.\n        mean (float): A média (μ) da distribuição normal.\n        std_dev (float): O desvio padrão (σ) da distribuição normal (deve ser > 0).\n\n    Returns:\n        Tuple[float, float]: Uma tupla contendo o limite inferior (a) e superior (b) do intervalo.\n\n    Raises:\n        TypeError: Se confidence_level, mean ou std_dev não forem numéricos.\n        ValueError: Se confidence_level não estiver no intervalo (0, 1) ou se std_dev <= 0.\n    \"\"\"\n    if not isinstance(confidence_level, (int, float)): raise TypeError(\"'confidence_level' deve ser numérico.\")\n    if not isinstance(mean, (int, float)): raise TypeError(\"'mean' deve ser numérico.\")\n    if not isinstance(std_dev, (int, float)): raise TypeError(\"'std_dev' deve ser numérico.\")\n    if not (0 < confidence_level < 1):\n        raise ValueError(\"O nível de confiança (confidence_level) deve estar no intervalo estrito (0, 1).\")\n    if std_dev <= 0:\n        raise ValueError(\"O desvio padrão (std_dev) deve ser um valor positivo.\")\n\n    # Probabilidade em cada cauda\n    tail_prob = (1.0 - confidence_level) / 2.0\n\n    # Calcula os quantis correspondentes às caudas\n    lower_bound = norm.ppf(q=tail_prob, loc=mean, scale=std_dev)\n    upper_bound = norm.ppf(q=1.0 - tail_prob, loc=mean, scale=std_dev)\n\n    return (float(lower_bound), float(upper_bound))", "bibliotecas": ["scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 58, "titulo": "calculate_sample_mean_probability", "categoria": "Estatística e Probabilidade", "subcategoria": "Inferência e Testes de Hipótese", "descricao": "Calcula a probabilidade da média amostral estar em um certo intervalo (Teorema do Limite Central).", "codigo_funcao": "def calculate_sample_mean_probability(\n    pop_mean: float,\n    pop_std: float,\n    sample_size: int,\n    x_lower: Optional[float] = None,\n    x_upper: Optional[float] = None\n) -> float:\n    \"\"\"\n    Calcula a probabilidade da média amostral (x̄) de uma amostra de tamanho 'n'\n    estar em um certo intervalo [x_lower, x_upper], assumindo que a média amostral\n    segue uma distribuição normal (pelo Teorema do Limite Central).\n\n    Usa a média da população (μ) como média da distribuição amostral e o erro padrão\n    (σ/√n) como desvio padrão da distribuição amostral.\n\n    Args:\n        pop_mean (float): A média (μ) da população da qual a amostra foi retirada.\n        pop_std (float): O desvio padrão (σ) da população (deve ser >= 0).\n        sample_size (int): O tamanho 'n' da amostra (deve ser > 0).\n        x_lower (Optional[float], optional): O limite inferior para a média amostral (x̄). Default: None.\n        x_upper (Optional[float], optional): O limite superior para a média amostral (x̄). Default: None.\n\n    Returns:\n        float: A probabilidade calculada para a média amostral P(x̄ <= x_upper),\n               P(x̄ > x_lower) ou P(x_lower < x̄ < x_upper).\n\n    Raises:\n        TypeError: Se pop_mean, pop_std não forem numéricos ou sample_size não for int.\n        ValueError: Se sample_size <= 0, pop_std < 0, ou limites inválidos forem passados\n                    para a função `calculate_normal_probability` interna.\n    \"\"\"\n    if not isinstance(pop_mean, (int, float)): raise TypeError(\"'pop_mean' deve ser numérico.\")\n    if not isinstance(pop_std, (int, float)): raise TypeError(\"'pop_std' deve ser numérico.\")\n    if not isinstance(sample_size, int): raise TypeError(\"'sample_size' deve ser inteiro.\")\n    if sample_size <= 0:\n        raise ValueError(\"O tamanho da amostra (sample_size) deve ser positivo.\")\n    if pop_std < 0:\n         raise ValueError(\"O desvio padrão da população (pop_std) não pode ser negativo.\")\n\n    # Calcula o erro padrão da média\n    if pop_std == 0:\n         standard_error = 0.0 # Se a população é constante, a média amostral também é\n    else:\n         standard_error = pop_std / np.sqrt(sample_size)\n\n    if standard_error == 0:\n         # Se o erro padrão é zero, a média amostral será exatamente pop_mean.\n         # A probabilidade é 1 se pop_mean estiver dentro do intervalo [lower, upper), 0 caso contrário.\n         prob = 0.0\n         if x_lower is None and x_upper is not None: # P(X <= upper)\n              if pop_mean <= x_upper: prob = 1.0\n         elif x_lower is not None and x_upper is None: # P(X > lower)\n              if pop_mean > x_lower: prob = 1.0\n         elif x_lower is not None and x_upper is not None: # P(lower < X < upper)\n              if x_lower < pop_mean < x_upper: prob = 1.0\n         return prob\n    else:\n         # Chama a função KB existente para calcular a probabilidade\n         return calculate_normal_probability(\n             mean=pop_mean,\n             std_dev=standard_error,\n             x_lower=x_lower,\n             x_upper=x_upper\n         )", "bibliotecas": ["numpy", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 59, "titulo": "calculate_proportion_confidence_interval", "categoria": "Estatística e Probabilidade", "subcategoria": "Inferência e Testes de Hipótese", "descricao": "Calcula o intervalo de confiança para uma proporção populacional (p) usando a aproximação Normal.", "codigo_funcao": "def calculate_proportion_confidence_interval(\n    n_successes: int,\n    sample_size: int,\n    confidence_level: float = 0.95,\n    method: str = 'normal' # Reservado para futuros métodos (ex: 'wilson', 'agresti_coull')\n) -> Tuple[float, float]:\n    \"\"\"\n    Calcula o intervalo de confiança para uma proporção populacional (p).\n\n    Por padrão, utiliza a aproximação da distribuição Normal (Intervalo de Wald),\n    válida para amostras suficientemente grandes (geralmente np >= 5 e n(1-p) >= 5).\n\n    Args:\n        n_successes (int): O número de 'sucessos' (observações da categoria de interesse)\n                           na amostra (x). Deve ser >= 0.\n        sample_size (int): O tamanho total da amostra (n). Deve ser > 0.\n        confidence_level (float, optional): O nível de confiança desejado (estritamente\n                                           entre 0 e 1). Defaults to 0.95.\n        method (str, optional): Método de cálculo. Atualmente suporta apenas 'normal'.\n                                Defaults to 'normal'.\n\n    Returns:\n        Tuple[float, float]: Uma tupla (limite_inferior, limite_superior) do intervalo\n                             de confiança para a proporção populacional. Os limites são\n                             garantidos entre [0, 1].\n\n    Raises:\n        TypeError: Se n_successes ou sample_size não forem inteiros, ou confidence_level não for numérico.\n        ValueError: Se os parâmetros forem inválidos (sample_size<=0, n_successes<0 ou > sample_size,\n                    confidence_level fora de (0,1)), ou método desconhecido.\n    \"\"\"\n    if not isinstance(n_successes, int): raise TypeError(\"'n_successes' deve ser inteiro.\")\n    if not isinstance(sample_size, int): raise TypeError(\"'sample_size' deve ser inteiro.\")\n    if not isinstance(confidence_level, (int, float)): raise TypeError(\"'confidence_level' deve ser numérico.\")\n    if method not in ['normal']: raise ValueError(\"Método inválido. Atualmente suporta apenas 'normal'.\")\n\n    if sample_size <= 0:\n        raise ValueError(\"O tamanho da amostra (sample_size) deve ser um inteiro positivo.\")\n    if not (0 <= n_successes <= sample_size):\n        raise ValueError(f\"O número de sucessos ({n_successes}) deve estar entre 0 e o tamanho da amostra ({sample_size}).\")\n    if not (0 < confidence_level < 1):\n        raise ValueError(\"O nível de confiança (confidence_level) deve estar no intervalo (0, 1).\")\n\n    # Calcula a proporção amostral\n    p_hat = n_successes / sample_size\n\n    if method == 'normal':\n        # Verifica as condições para a aproximação normal (aviso)\n        check_np = sample_size * p_hat\n        check_n1p = sample_size * (1 - p_hat)\n        if not (check_np >= 5 and check_n1p >= 5):\n            print(f\"Aviso: As condições para a aproximação normal (np>=5 e n(1-p)>=5) \"\n                  f\"podem não ser satisfeitas (np={check_np:.2f}, n(1-p)={check_n1p:.2f}). \"\n                  \"O intervalo pode ser impreciso, especialmente perto de p=0 ou p=1.\")\n\n        # Calcula o erro padrão da proporção\n        # Adiciona pequena tolerância no denominador para evitar divisão por zero se n=0 (já validado)\n        standard_error = np.sqrt(p_hat * (1 - p_hat) / sample_size) if sample_size > 0 else 0\n\n        if standard_error == 0:\n             # Se p_hat é 0 ou 1, o intervalo normal colapsa.\n             # Outros métodos (como Wilson) são melhores aqui, mas para Wald:\n             return (float(p_hat), float(p_hat))\n\n        # Calcula o intervalo usando a Z-score\n        # norm.interval(confidence, loc=mean, scale=std_error)\n        interval = norm.interval(confidence=confidence_level, loc=p_hat, scale=standard_error)\n\n        # Garante que os limites do intervalo estejam entre [0, 1]\n        lower_bound = max(0.0, float(interval[0]))\n        upper_bound = min(1.0, float(interval[1]))\n\n        return (lower_bound, upper_bound)", "bibliotecas": ["numpy", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 60, "titulo": "calculate_min_sample_size_for_mean", "categoria": "Estatística e Probabilidade", "subcategoria": "Inferência e Testes de Hipótese", "descricao": "Calcula o tamanho mínimo da amostra para estimar uma média populacional com uma margem de erro definida.", "codigo_funcao": "def calculate_min_sample_size_for_mean(\n    confidence_level: float,\n    margin_of_error: float,\n    population_std_estimate: float\n) -> int:\n    \"\"\"\n    Calcula o tamanho mínimo da amostra (n) necessário para estimar uma média\n    populacional (μ) com um nível de confiança e margem de erro especificados.\n\n    Args:\n        confidence_level (float): O nível de confiança desejado para a estimativa\n                                  (estritamente entre 0 e 1, ex: 0.95 para 95%).\n        margin_of_error (float): A margem de erro máxima aceitável (E) na estimativa\n                                 da média (metade da largura do IC, deve ser > 0).\n        population_std_estimate (float): Uma estimativa do desvio padrão\n                                         populacional (σ). Deve ser >= 0.\n\n    Returns:\n        int: O tamanho mínimo da amostra necessário, arredondado para o próximo\n             inteiro superior (usando np.ceil).\n\n    Raises:\n        TypeError: Se confidence_level, margin_of_error ou population_std_estimate\n                   não forem numéricos.\n        ValueError: Se confidence_level não estiver em (0, 1), margin_of_error <= 0,\n                    ou population_std_estimate < 0.\n    \"\"\"\n    if not isinstance(confidence_level, (int, float)): raise TypeError(\"'confidence_level' deve ser numérico.\")\n    if not isinstance(margin_of_error, (int, float)): raise TypeError(\"'margin_of_error' deve ser numérico.\")\n    if not isinstance(population_std_estimate, (int, float)): raise TypeError(\"'population_std_estimate' deve ser numérico.\")\n\n    if not (0 < confidence_level < 1):\n        raise ValueError(\"O nível de confiança (confidence_level) deve estar no intervalo (0, 1).\")\n    if margin_of_error <= 0:\n        raise ValueError(\"A margem de erro (margin_of_error) deve ser positiva.\")\n    if population_std_estimate < 0:\n        raise ValueError(\"A estimativa do desvio padrão populacional (population_std_estimate) não pode ser negativa.\")\n\n    # Se std_estimate é 0, a população é constante, tecnicamente n=1 seria suficiente,\n    # mas a fórmula daria 0. Retornar 1 parece mais razoável.\n    if population_std_estimate == 0:\n        return 1\n\n    # Calcula o z-score crítico (zc) para o nível de confiança bilateral\n    q_for_ppf = 1.0 - (1.0 - confidence_level) / 2.0\n    z_critical = norm.ppf(q=q_for_ppf)\n\n    # Calcula o tamanho da amostra usando a fórmula: n = (zc * σ / E)^2\n    n_float = (z_critical * population_std_estimate / margin_of_error) ** 2\n\n    # Arredonda para cima, pois o tamanho da amostra deve ser inteiro\n    return int(np.ceil(n_float))", "bibliotecas": ["numpy", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 61, "titulo": "get_sorted_eigenpairs", "categoria": "Aprendizado Não Supervisionado", "subcategoria": "Redução de Dimensionalidade (PCA)", "descricao": "Calcula os autovalores e autovetores de uma matriz quadrada e os retorna em ordem decrescente.", "codigo_funcao": "def get_sorted_eigenpairs(\n    matrix: Union[np.ndarray, pd.DataFrame]\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calcula os autovalores e autovetores de uma matriz quadrada e os retorna em ordem decrescente.\n\n    Args:\n        matrix (Union[np.ndarray, pd.DataFrame]): A matriz de entrada. Deve ser quadrada.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Uma tupla contendo:\n            - Um array NumPy 1D com os autovalores ordenados de forma decrescente.\n            - Uma matriz NumPy 2D onde cada coluna é o autovetor correspondente ao\n              autovalor na mesma posição.\n\n    Raises:\n        TypeError: Se a entrada não for um array NumPy ou DataFrame pandas.\n        ValueError: Se a matriz de entrada não for quadrada.\n    \"\"\"\n    if not isinstance(matrix, (np.ndarray, pd.DataFrame)):\n        raise TypeError(\"A entrada 'matrix' deve ser um array NumPy ou DataFrame pandas.\")\n\n    # Converte para array NumPy se for DataFrame\n    if isinstance(matrix, pd.DataFrame):\n        matrix = matrix.values\n\n    if matrix.shape[0] != matrix.shape[1]:\n        raise ValueError(f\"A matriz de entrada deve ser quadrada, mas tem shape {matrix.shape}.\")\n\n    # Calcula autovalores e autovetores\n    try:\n        eigenvalues, eigenvectors = np.linalg.eig(matrix)\n    except np.linalg.LinAlgError as e:\n        raise np.linalg.LinAlgError(f\"Falha no cálculo de autovalores/autovetores: {e}\")\n\n    # Ordena os autovalores em ordem decrescente e obtém os índices\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n\n    # Reordena os autovalores e autovetores com base nos índices\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    return sorted_eigenvalues, sorted_eigenvectors", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 62, "titulo": "calculate_distance_matrix", "categoria": "Aprendizado Não Supervisionado", "subcategoria": "Redução de Dimensionalidade (PCA)", "descricao": "Calcula a matriz de distâncias par a par para um conjunto de dados usando várias métricas.", "codigo_funcao": "def calculate_distance_matrix(\n    data: Union[np.ndarray, pd.DataFrame],\n    metric: str = 'euclidean',\n    labels: Optional[Union[List[str], pd.Series]] = None\n) -> pd.DataFrame:\n    \"\"\"\n    Calcula a matriz de distâncias par a par para um conjunto de dados.\n\n    Args:\n        data (Union[np.ndarray, pd.DataFrame]): Array ou DataFrame com as observações nas linhas\n            e as variáveis nas colunas. Se for um DataFrame, apenas as colunas\n            numéricas serão usadas.\n        metric (str, optional): A métrica de distância a ser usada. Exemplos: 'euclidean',\n            'seuclidean' (euclidiana padronizada), 'mahalanobis', 'cityblock' (manhattan).\n            Defaults to 'euclidean'.\n        labels (Optional[Union[List[str], pd.Series]], optional): Rótulos para as linhas e\n            colunas da matriz de distância resultante. Se None e a entrada for um\n            DataFrame, o índice do DataFrame será usado. Defaults to None.\n\n    Returns:\n        pd.DataFrame: Uma matriz de distâncias quadrada (DataFrame) com os rótulos\n                      fornecidos ou inferidos.\n\n    Raises:\n        TypeError: Se 'data' não for um array NumPy ou DataFrame.\n        ValueError: Se o número de rótulos não corresponder ao número de observações,\n                    ou se houver menos de 2 observações para calcular a distância.\n    \"\"\"\n    if not isinstance(data, (np.ndarray, pd.DataFrame)):\n        raise TypeError(\"O argumento 'data' deve ser um array NumPy ou DataFrame pandas.\")\n\n    if isinstance(data, pd.DataFrame):\n        if labels is None:\n            labels = data.index # Usa o índice do DataFrame como padrão\n        # Garante que apenas colunas numéricas sejam usadas para o cálculo\n        numeric_data = data.select_dtypes(include=np.number).values\n    else:\n        numeric_data = data\n\n    if numeric_data.shape[0] < 2:\n        raise ValueError(\"São necessárias pelo menos 2 observações para calcular distâncias.\")\n\n    if labels is not None and len(labels) != numeric_data.shape[0]:\n        raise ValueError(f\"O número de rótulos ({len(labels)}) não corresponde ao número de \"\n                         f\"observações ({numeric_data.shape[0]}).\")\n\n    # Calcula a matriz de distância condensada e a converte para a forma quadrada\n    try:\n        dist_matrix = squareform(pdist(numeric_data, metric=metric))\n    except Exception as e:\n        raise ValueError(f\"Erro ao calcular a distância com a métrica '{metric}': {e}\")\n\n    # Cria o DataFrame de resultado com os rótulos\n    dist_df = pd.DataFrame(dist_matrix, index=labels, columns=labels)\n\n    return dist_df", "bibliotecas": ["numpy", "pandas", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 63, "titulo": "find_extreme_distance_pairs", "categoria": "Aprendizado Não Supervisionado", "subcategoria": "Redução de Dimensionalidade (PCA)", "descricao": "Encontra o(s) par(es) de itens com a maior ou menor distância em uma matriz de distância.", "codigo_funcao": "def find_extreme_distance_pairs(\n    dist_df: pd.DataFrame,\n    op: Literal['max', 'min'] = 'max'\n) -> List[Tuple[str, str, float]]:\n    \"\"\"\n    Encontra o(s) par(es) de itens com a maior ou menor distância em uma matriz de distância.\n\n    Args:\n        dist_df (pd.DataFrame): Uma matriz de distância quadrada e simétrica, onde o\n                                índice e as colunas são os rótulos dos itens.\n        op (Literal['max', 'min'], optional): A operação a ser realizada.\n            'max': encontra os pares mais distantes (diferentes).\n            'min': encontra os pares mais próximos (similares), ignorando a diagonal.\n            Defaults to 'max'.\n\n    Returns:\n        List[Tuple[str, str, float]]: Uma lista de tuplas, onde cada tupla contém\n            (item1, item2, distância). A lista pode conter múltiplos pares se houver empates.\n\n    Raises:\n        TypeError: Se 'dist_df' não for um DataFrame pandas.\n        ValueError: Se 'dist_df' não for uma matriz quadrada ou se 'op' for inválido.\n    \"\"\"\n    if not isinstance(dist_df, pd.DataFrame):\n        raise TypeError(\"O argumento 'dist_df' deve ser um DataFrame pandas.\")\n    if dist_df.shape[0] != dist_df.shape[1]:\n        raise ValueError(f\"A matriz de distância deve ser quadrada, mas tem shape {dist_df.shape}.\")\n    if op not in ['max', 'min']:\n        raise ValueError(\"O argumento 'op' deve ser 'max' ou 'min'.\")\n\n    # Cria uma cópia para evitar modificar o original\n    dist_values = dist_df.values.copy()\n\n    if op == 'min':\n        # Para encontrar o mínimo, ignora a diagonal (distância de um item para si mesmo)\n        np.fill_diagonal(dist_values, np.inf)\n        extreme_val = np.nanmin(dist_values)\n    else: # op == 'max'\n        extreme_val = np.nanmax(dist_values)\n        \n    if np.isinf(extreme_val) or pd.isna(extreme_val):\n        return [] # Retorna lista vazia se não encontrar um valor válido\n\n    # Encontra as coordenadas de todos os pares com o valor extremo\n    # Usa np.triu para pegar apenas o triângulo superior e evitar pares duplicados (A,B) e (B,A)\n    indices = np.where(np.triu(dist_values) == extreme_val)\n    \n    # Extrai os rótulos e a distância para cada par encontrado\n    result = []\n    item_labels = dist_df.index\n    for r, c in zip(indices[0], indices[1]):\n        result.append((item_labels[r], item_labels[c], extreme_val))\n\n    return result", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 64, "titulo": "perform_hierarchical_clustering", "categoria": "Aprendizado Não Supervisionado", "subcategoria": "Clustering", "descricao": "Executa o agrupamento hierárquico, plota o dendrograma e opcionalmente define os grupos.", "codigo_funcao": "def perform_hierarchical_clustering(\n    df: pd.DataFrame,\n    method: str = 'ward',\n    metric: str = 'euclidean',\n    cut_height: Optional[float] = None,\n    ax: Optional[Axes] = None,\n    **dendrogram_kwargs\n) -> Tuple[np.ndarray, Axes, Optional[np.ndarray]]: \n    \"\"\"\n    Descrição: Executa a análise de agrupamento hierárquico, plota o dendrograma\n               e, opcionalmente, corta a árvore para definir os grupos.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada. Apenas colunas numéricas serão usadas.\n        method (str, optional): O método de ligação a ser usado. Defaults to 'ward'.\n        metric (str, optional): A métrica de distância. Defaults to 'euclidean'.\n        cut_height (Optional[float], optional): A altura para cortar o dendrograma. Defaults to None.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Defaults to None.\n        **dendrogram_kwargs: Argumentos adicionais para scipy.cluster.hierarchy.dendrogram().\n\n    Returns:\n        Tuple[np.ndarray, plt.Axes, Optional[np.ndarray]]: Uma tupla contendo:\n            - A matriz de ligação (linkage matrix) Z.\n            - O objeto Axes do Matplotlib com o dendrograma.\n            - Um array NumPy com os rótulos dos grupos se cut_height for fornecido, senão None.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame do pandas.\n        ValueError: Se 'df' tiver menos de duas linhas ou nenhuma coluna numérica.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame do pandas.\")\n\n    X = df.select_dtypes(include=np.number)\n    if X.shape[0] < 2:\n        raise ValueError(\"O DataFrame precisa de pelo menos duas observações para o agrupamento.\")\n    if X.shape[1] == 0:\n        raise ValueError(\"Nenhuma coluna numérica encontrada no DataFrame.\")\n\n    try:\n        Z = linkage(X, method=method, metric=metric)\n    except Exception as e:\n        raise RuntimeError(f\"Erro ao calcular a ligação (linkage): {e}\")\n\n    if ax is None:\n        fig_width = dendrogram_kwargs.pop('fig_width', 12)\n        fig_height = dendrogram_kwargs.pop('fig_height', 7)\n        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    dendrogram(\n        Z, ax=ax, labels=df.index.tolist(),\n        leaf_rotation=dendrogram_kwargs.pop('leaf_rotation', 90.),\n        leaf_font_size=dendrogram_kwargs.pop('leaf_font_size', 10.),\n        **dendrogram_kwargs\n    )\n    ax.set_title(dendrogram_kwargs.pop('title', f'Dendrograma Hierárquico (Método: {method})'))\n    ax.set_xlabel(dendrogram_kwargs.pop('xlabel', 'Observações'))\n    ax.set_ylabel(dendrogram_kwargs.pop('ylabel', 'Distância'))\n\n    cluster_labels = None\n    if cut_height is not None:\n        if not isinstance(cut_height, (int, float)) or cut_height <= 0:\n            print(f\"Aviso: 'cut_height' deve ser um número positivo. Nenhum corte será feito.\")\n        else:\n            ax.axhline(y=cut_height, c='grey', lw=1, linestyle='dashed')\n            cluster_labels = cut_tree(Z, height=cut_height).flatten()\n\n    plt.tight_layout()\n    return Z, ax, cluster_labels", "bibliotecas": ["matplotlib", "numpy", "pandas", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 65, "titulo": "plot_chi_square_qq", "categoria": "Estatística e Probabilidade", "subcategoria": "Inferência e Testes de Hipótese", "descricao": "Plota um gráfico Q-Q multivariado (Qui-Quadrado) para avaliar a normalidade multivariada.", "codigo_funcao": "def plot_chi_square_qq(\n    data: Union[pd.DataFrame, np.ndarray],\n    ax: Optional[Axes] = None\n) -> Axes: \n    \"\"\"\n    Descrição: Plota um gráfico Q-Q multivariado (Qui-Quadrado) para avaliar a\n               suposição de normalidade multivariada.\n\n    Args:\n        data (Union[pd.DataFrame, np.ndarray]): DataFrame ou array NumPy com as\n            observações nas linhas e as variáveis (features) nas colunas.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico Q-Q.\n\n    Raises:\n        TypeError: Se 'data' não for um DataFrame ou array NumPy.\n        ValueError: Se houver dados insuficientes ou se a matriz de covariância\n                    for singular (não invertível).\n    \"\"\"\n    if not isinstance(data, (pd.DataFrame, np.ndarray)):\n        raise TypeError(\"O argumento 'data' deve ser um DataFrame pandas ou array NumPy.\")\n\n    X = data.values if isinstance(data, pd.DataFrame) else data\n    n, p = X.shape\n\n    if n <= p:\n        raise ValueError(f\"O número de observações ({n}) deve ser maior que o número de variáveis ({p}).\")\n\n    mean_vec = np.mean(X, axis=0)\n    cov_matrix = np.cov(X.T)\n\n    try:\n        inv_cov_matrix = np.linalg.inv(cov_matrix)\n    except np.linalg.LinAlgError:\n        raise ValueError(\"A matriz de covariância é singular e não pode ser invertida.\")\n\n    mahalanobis_sq_dist = np.zeros(n)\n    for i in range(n):\n        diff = X[i] - mean_vec\n        mahalanobis_sq_dist[i] = diff.T @ inv_cov_matrix @ diff\n\n    sorted_dist = np.sort(mahalanobis_sq_dist)\n\n    theoretical_quantiles = np.zeros(n)\n    for i in range(n):\n        prob = ((i + 1) - 0.5) / n\n        theoretical_quantiles[i] = stats.chi2.ppf(prob, p)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(7, 7))\n\n    ax.scatter(sorted_dist, theoretical_quantiles, c='black', alpha=0.7)\n    \n    line_range = [min(np.min(sorted_dist), np.min(theoretical_quantiles)),\n                  max(np.max(sorted_dist), np.max(theoretical_quantiles))]\n    ax.plot(line_range, line_range, 'r--')\n\n    ax.set_xlabel('Distâncias de Mahalanobis Ordenadas ($d^2_i$)')\n    ax.set_ylabel(f'Quantis Teóricos da $\\\\chi^2_{{{p}}}$')\n    ax.set_title('Gráfico Q-Q Multivariado (Qui-Quadrado)')\n    ax.grid(True)\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 66, "titulo": "plot_pp", "categoria": "Estatística e Probabilidade", "subcategoria": "Inferência e Testes de Hipótese", "descricao": "Plota um gráfico P-P (Probabilidade-Probabilidade) para avaliar a normalidade univariada.", "codigo_funcao": "def plot_pp(\n    series: Union[pd.Series, np.ndarray],\n    ax: Optional[Axes] = None \n) -> Axes: \n    \"\"\"\n    Descrição: Plota um gráfico P-P (Probabilidade-Probabilidade) para avaliar a\n               suposição de normalidade univariada.\n\n    Args:\n        series (Union[pd.Series, np.ndarray]): Série ou array NumPy 1D contendo os dados.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico P-P.\n\n    Raises:\n        TypeError: Se 'series' não for uma Série pandas ou array NumPy.\n        ValueError: Se 'series' não for unidimensional ou tiver menos de 2 pontos\n                    após remover NaNs.\n    \"\"\"\n    if not isinstance(series, (pd.Series, np.ndarray)):\n        raise TypeError(\"O argumento 'series' deve ser uma Série pandas ou array NumPy.\")\n\n    data = np.asarray(series).flatten()\n    data = data[~np.isnan(data)]\n\n    if data.ndim != 1:\n        raise ValueError(\"A entrada 'series' deve ser unidimensional.\")\n    if len(data) < 2:\n        raise ValueError(\"São necessários pelo menos 2 pontos de dados não nulos.\")\n\n    sorted_data = np.sort(data)\n    n = len(sorted_data)\n    mean = np.mean(sorted_data)\n    std = np.std(sorted_data, ddof=1)\n\n    theoretical_probs = stats.norm.cdf((sorted_data - mean) / std)\n\n    empirical_probs = np.zeros(n)\n    for i in range(n):\n        empirical_probs[i] = (i + 1 - 0.5) / n\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(7, 7))\n\n    ax.scatter(empirical_probs, theoretical_probs, c='black', alpha=0.7)\n    ax.plot([0, 1], [0, 1], 'r--')\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xlabel('Probabilidades Empíricas Acumuladas')\n    ax.set_ylabel('Probabilidades Teóricas Acumuladas (Normal)')\n    ax.set_title('Gráfico P-P de Normalidade')\n    ax.grid(True)\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 67, "titulo": "perform_shapiro_wilk_test", "categoria": "Estatística e Probabilidade", "subcategoria": "Inferência e Testes de Hipótese", "descricao": "Realiza o teste de normalidade de Shapiro-Wilk em uma ou mais colunas de um DataFrame.", "codigo_funcao": "def perform_shapiro_wilk_test(\n    df: pd.DataFrame,\n    cols: Optional[List[str]] = None,\n    alpha: float = 0.05\n) -> pd.DataFrame:\n    \"\"\"\n    Descrição: Realiza o teste de normalidade de Shapiro-Wilk em uma ou mais colunas de um DataFrame.\n               A hipótese nula (H0) do teste é que os dados seguem uma distribuição Normal.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols (Optional[List[str]], optional): Lista de colunas numéricas a serem testadas.\n            Se None, testa todas as colunas numéricas do DataFrame. Defaults to None.\n        alpha (float, optional): Nível de significância para a decisão do teste.\n            Defaults to 0.05.\n\n    Returns:\n        pd.DataFrame: Um DataFrame contendo os resultados para cada coluna testada,\n                      incluindo a estatística do teste, o valor-p e a conclusão\n                      (se a hipótese nula de normalidade é rejeitada ou não).\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame do pandas.\n        ValueError: Se 'cols' contiver colunas inexistentes ou não numéricas, ou se\n                    'alpha' não estiver no intervalo (0, 1).\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if not (0 < alpha < 1):\n        raise ValueError(\"O nível de significância 'alpha' deve estar no intervalo (0, 1).\")\n\n    if cols is None:\n        cols_to_test = df.select_dtypes(include=np.number).columns.tolist()\n        if not cols_to_test:\n            print(\"Aviso: Nenhuma coluna numérica encontrada para testar.\")\n            return pd.DataFrame()\n    else:\n        if not isinstance(cols, list):\n            raise TypeError(\"'cols' deve ser uma lista de nomes de colunas.\")\n        missing = [c for c in cols if c not in df.columns]\n        if missing:\n            raise ValueError(f\"As seguintes colunas não existem no df: {missing}\")\n        \n        non_numeric = [c for c in cols if not pd.api.types.is_numeric_dtype(df[c])]\n        if non_numeric:\n            raise ValueError(f\"As seguintes colunas não são numéricas: {non_numeric}\")\n        cols_to_test = cols\n\n    results = []\n    for col in cols_to_test:\n        data = df[col].dropna()\n        if len(data) < 3:\n            print(f\"Aviso: Coluna '{col}' tem menos de 3 pontos de dados não nulos e será pulada.\")\n            continue\n        \n        stat, p_value = shapiro(data)\n        \n        # Conclusão do teste\n        reject_h0 = p_value < alpha\n        conclusion = f\"Rejeitar H0 (Não Normal)\" if reject_h0 else f\"Não Rejeitar H0 (Normal)\"\n        \n        results.append({\n            \"Coluna\": col,\n            \"Estatística W\": stat,\n            \"Valor-p\": p_value,\n            f\"Conclusão (alpha={alpha})\": conclusion\n        })\n    \n    return pd.DataFrame(results)", "bibliotecas": ["numpy", "pandas", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 68, "titulo": "plot_pca_explained_variance", "categoria": "Aprendizado Não Supervisionado", "subcategoria": "Redução de Dimensionalidade (PCA)", "descricao": "Plota um gráfico de barras (Scree Plot) da variância explicada por cada componente principal.", "codigo_funcao": "def plot_pca_explained_variance(\n    pca_model: PCA,\n    ax: Optional[Axes] = None \n) -> Axes:\n    \"\"\"\n    Descrição: Plota um gráfico de barras da variância explicada por cada componente\n               principal e uma linha da variância acumulada (Scree Plot).\n\n    Args:\n        pca_model (PCA): O objeto PCA já treinado (fitted).\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico.\n\n    Raises:\n        AttributeError: Se o objeto pca_model não parecer estar treinado.\n        TypeError: Se 'pca_model' não for uma instância de PCA.\n    \"\"\"\n    if not isinstance(pca_model, PCA):\n        raise TypeError(\"O argumento 'pca_model' deve ser um objeto PCA do scikit-learn.\")\n    if not hasattr(pca_model, 'explained_variance_ratio_'):\n        raise AttributeError(\"O modelo PCA fornecido não parece estar treinado (falta 'explained_variance_ratio_').\")\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n    var_exp = pca_model.explained_variance_ratio_ * 100\n    cum_var_exp = np.cumsum(var_exp)\n    n_components = len(var_exp)\n    component_names = [f'CP{i+1}' for i in range(n_components)]\n\n    plot_df = pd.DataFrame({\n        'Componente': component_names,\n        'Variância Explicada (%)': var_exp,\n        'Variância Acumulada (%)': cum_var_exp\n    })\n\n    plot_df.plot(kind='bar', x='Componente', y='Variância Explicada (%)',\n                 ax=ax, color='lightgray', legend=False, width=0.8)\n    ax.set_ylabel('Variância Explicada (%)')\n    ax.set_xlabel('Componentes Principais')\n\n    ax2 = ax.twinx()\n    ax2.plot(plot_df['Componente'], plot_df['Variância Acumulada (%)'],\n             color='black', marker='o', linestyle='-')\n    ax2.set_ylabel('Variância Acumulada (%)')\n    ax2.set_ylim(0, 105)\n\n    for index, value in enumerate(cum_var_exp):\n        ax2.text(index, value + 2, f\"{value:.1f}%\", ha=\"center\")\n\n    ax.set_title('Scree Plot - Variância Explicada pelos Componentes Principais')\n    ax.tick_params(axis='x', rotation=0)\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 69, "titulo": "plot_pca_biplot", "categoria": "Aprendizado Não Supervisionado", "subcategoria": "Redução de Dimensionalidade (PCA)", "descricao": "Cria um Biplot sobrepondo scores (observações) e loadings (variáveis) do PCA.", "codigo_funcao": "def plot_pca_biplot(\n    scores: np.ndarray,\n    pca_model: PCA,\n    feature_names: List[str],\n    score_labels: Optional[Union[List[str], pd.Index]] = None,\n    ax: Optional[Axes] = None \n) -> Axes:\n    \"\"\"\n    Descrição: Cria um Biplot que sobrepõe o gráfico de dispersão dos scores (observações)\n               com os loadings (variáveis) dos dois primeiros componentes principais.\n\n    Args:\n        scores (np.ndarray): Array 2D com os scores dos componentes principais.\n        pca_model (PCA): O objeto PCA já treinado.\n        feature_names (List[str]): Lista dos nomes das features originais.\n        score_labels (Optional[Union[List[str], pd.Index]], optional): Rótulos para as observações. Defaults to None.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o biplot.\n\n    Raises:\n        AttributeError: Se o modelo PCA não estiver treinado.\n        ValueError: Se o PCA tiver menos de 2 componentes ou se houver inconsistências de tamanho.\n    \"\"\"\n    if not isinstance(pca_model, PCA) or not hasattr(pca_model, 'components_'):\n        raise AttributeError(\"O modelo PCA fornecido não parece estar treinado.\")\n    if pca_model.n_components_ < 2:\n        raise ValueError(\"O PCA deve ter pelo menos 2 componentes para um biplot 2D.\")\n    if len(feature_names) != pca_model.n_features_in_:\n        raise ValueError(\"O número de 'feature_names' não corresponde ao número de features no modelo.\")\n    if score_labels is not None and len(score_labels) != scores.shape[0]:\n        raise ValueError(\"O número de 'score_labels' não corresponde ao número de observações (scores).\")\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n    loadings = pca_model.components_\n    xs = scores[:, 0]\n    ys = scores[:, 1]\n    \n    scalex = 1.0 / (xs.max() - xs.min())\n    scaley = 1.0 / (ys.max() - ys.min())\n\n    ax.scatter(xs * scalex, ys * scaley, alpha=0.7, edgecolors='k', s=50)\n    if score_labels is not None:\n        for i, label in enumerate(score_labels):\n            ax.annotate(label, (xs[i] * scalex, ys[i] * scaley), ha='center', va='bottom')\n\n    for i in range(len(feature_names)):\n        ax.arrow(0, 0, loadings[0, i], loadings[1, i],\n                 color='r', alpha=0.8, head_width=0.02)\n        ax.text(loadings[0, i] * 1.15, loadings[1, i] * 1.15,\n                feature_names[i], color='g', ha='center', va='center')\n\n    ax.set_xlabel(\"Componente Principal 1\")\n    ax.set_ylabel(\"Componente Principal 2\")\n    ax.set_title(\"Biplot PCA\")\n    ax.grid(True, linestyle='--', alpha=0.5)\n    \n    ax.axhline(0, color='grey', lw=1)\n    ax.axvline(0, color='grey', lw=1)\n    \n    plt.tight_layout()\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 70, "titulo": "apply_boxcox_transformation", "categoria": "Engenharia e Transformação de Features", "subcategoria": "Escalonamento e Normalização", "descricao": "Aplica a transformação de Box-Cox a uma série de dados para aproximá-la de uma distribuição normal.", "codigo_funcao": "def apply_boxcox_transformation(\n    series: Union[pd.Series, np.ndarray],\n    shift: float = 0.0\n) -> Tuple[np.ndarray, float]:\n    \"\"\"\n    Descrição: Aplica a transformação de Box-Cox a uma série de dados para aproximá-la\n             de uma distribuição normal. Opcionalmente, adiciona um valor de deslocamento\n             para tratar dados não positivos.\n\n    Args:\n        series (Union[pd.Series, np.ndarray]): A série de dados de entrada. Deve ser 1D.\n        shift (float, optional): Um valor constante a ser somado a todos os elementos da\n            série antes da transformação para garantir que sejam positivos. Defaults to 0.0.\n\n    Returns:\n        Tuple[np.ndarray, float]: Uma tupla contendo:\n            - O array NumPy com os dados transformados (sem NaNs).\n            - O valor lambda ótimo encontrado que maximiza a verossimilhança.\n\n    Raises:\n        TypeError: Se 'series' não for uma Série pandas ou array NumPy.\n        ValueError: Se a série, após o deslocamento, contiver valores não positivos.\n    \"\"\"\n    if not isinstance(series, (pd.Series, np.ndarray)):\n        raise TypeError(\"A entrada 'series' deve ser uma Série pandas ou array NumPy.\")\n\n    data = np.asarray(series).flatten()\n    data = data[~np.isnan(data)] \n\n    if shift != 0.0:\n        data = data + shift\n\n    if np.any(data <= 0):\n        raise ValueError(\"Os dados para a transformação Box-Cox devem ser estritamente positivos. \"\n                         \"Considere usar o parâmetro 'shift' para deslocar os dados.\")\n\n    try:\n        transformed_data, optimal_lambda = stats.boxcox(data)\n    except Exception as e:\n        raise RuntimeError(f\"Erro ao aplicar a transformação Box-Cox: {e}\")\n\n    return transformed_data, optimal_lambda", "bibliotecas": ["numpy", "pandas", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 71, "titulo": "plot_boxcox_lambda", "categoria": "Visualização de Dados", "subcategoria": "Distribuições Univariadas", "descricao": "Plota o gráfico de log-verossimilhança da transformação de Box-Cox para encontrar o lambda ótimo.", "codigo_funcao": "def plot_boxcox_lambda(\n    series: Union[pd.Series, np.ndarray],\n    lambda_range: Tuple[float, float] = (-2, 2),\n    ax: Optional[Axes] = None \n) -> Axes: \n    \"\"\"\n    Descrição: Plota o gráfico da função de log-verossimilhança da transformação\n             de Box-Cox em um intervalo de valores lambda, destacando o lambda ótimo.\n\n    Args:\n        series (Union[pd.Series, np.ndarray]): A série de dados de entrada (positivos).\n        lambda_range (Tuple[float, float], optional): Intervalo de valores lambda a serem plotados.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico.\n\n    Raises:\n        TypeError: Se 'series' não for uma Série pandas ou array NumPy.\n        ValueError: Se a série contiver valores não positivos.\n    \"\"\"\n    if not isinstance(series, (pd.Series, np.ndarray)):\n        raise TypeError(\"A entrada 'series' deve ser uma Série pandas ou array NumPy.\")\n\n    data = np.asarray(series).flatten()\n    data = data[~np.isnan(data)]\n\n    if np.any(data <= 0):\n        raise ValueError(\"Os dados para a transformação Box-Cox devem ser estritamente positivos.\")\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 6))\n\n    stats.boxcox_normplot(data, lambda_range[0], lambda_range[1], plot=ax)\n\n    optimal_lambda = stats.boxcox_normmax(data)\n    ax.axvline(optimal_lambda, color='r', linestyle='--', label=f'Lambda Ótimo = {optimal_lambda:.2f}')\n\n    ax.set_title('Gráfico de Log-Verossimilhança de Box-Cox')\n    ax.set_xlabel('Lambda (λ)')\n    ax.set_ylabel('Log-Verossimilhança')\n    ax.legend()\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 72, "titulo": "plot_kde_comparison", "categoria": "Visualização de Dados", "subcategoria": "Distribuições Univariadas", "descricao": "Cria uma comparação lado a lado de duas distribuições de dados usando gráficos KDE.", "codigo_funcao": "def plot_kde_comparison(\n    series_before: Union[pd.Series, np.ndarray],\n    series_after: Union[pd.Series, np.ndarray],\n    labels: Tuple[str, str] = (\"Antes\", \"Depois\"),\n    fig_size: Tuple[int, int] = (12, 6)\n) -> Tuple[plt.Figure, np.ndarray]:\n    \"\"\"\n    Descrição: Cria uma comparação lado a lado de duas distribuições de dados usando\n             gráficos de Estimativa de Densidade de Kernel (KDE).\n\n    Args:\n        series_before (Union[pd.Series, np.ndarray]): Os dados originais. NaNs são removidos.\n        series_after (Union[pd.Series, np.ndarray]): Os dados após alguma transformação. NaNs são removidos.\n        labels (Tuple[str, str], optional): Rótulos para os títulos dos gráficos.\n            Defaults to (\"Antes\", \"Depois\").\n        fig_size (Tuple[int, int], optional): Tamanho da figura. Defaults to (12, 6).\n\n    Returns:\n        Tuple[plt.Figure, np.ndarray]: Uma tupla contendo o objeto Figure e um\n            array de objetos Axes (contendo os dois subplots) do Matplotlib.\n\n    Raises:\n        TypeError: Se 'series_before' ou 'series_after' não forem Séries/arrays.\n        ValueError: Se 'labels' ou 'fig_size' não forem tuplas com 2 elementos.\n    \"\"\"\n    if not isinstance(series_before, (pd.Series, np.ndarray)):\n        raise TypeError(\"O argumento 'series_before' deve ser uma Série pandas ou array NumPy.\")\n    if not isinstance(series_after, (pd.Series, np.ndarray)):\n        raise TypeError(\"O argumento 'series_after' deve ser uma Série pandas ou array NumPy.\")\n    if not isinstance(labels, tuple) or len(labels) != 2:\n        raise ValueError(\"O argumento 'labels' deve ser uma tupla de duas strings.\")\n    if not isinstance(fig_size, tuple) or len(fig_size) != 2:\n         raise ValueError(\"O argumento 'fig_size' deve ser uma tupla de dois inteiros.\")\n        \n    fig, axes = plt.subplots(1, 2, figsize=fig_size)\n\n    # Função auxiliar para plotar ou lidar com dados inválidos\n    def plot_kde_safe(ax, data, label):\n        data_clean = data[~np.isnan(data)] if isinstance(data, np.ndarray) else data.dropna()\n        \n        if len(data_clean) < 2 or np.std(data_clean) == 0:\n            ax.text(0.5, 0.5, f\"Não é possível plotar KDE:\\nDados insuficientes ou constantes.\",\n                    horizontalalignment='center', verticalalignment='center',\n                    transform=ax.transAxes, color='red')\n            ax.set_title(f'Distribuição: {label} (Erro)')\n        else:\n            sns.kdeplot(data_clean, ax=ax, fill=True, linewidth=2)\n            ax.set_title(f'Distribuição: {label}')\n        \n        ax.set_xlabel('Valor')\n        ax.set_ylabel('Densidade')\n\n    # Plot dos KDEs\n    plot_kde_safe(axes[0], series_before, labels[0])\n    plot_kde_safe(axes[1], series_after, labels[1])\n\n    fig.suptitle('Comparação de Distribuições (KDE)', fontsize=16)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Ajusta para o suptitle\n\n    return fig, axes", "bibliotecas": ["matplotlib", "numpy", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 73, "titulo": "plot_relative_frequency_bar", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Plota um gráfico de barras com a frequência relativa (%) de uma categoria 'hue' dentro de uma categoria 'x'.", "codigo_funcao": "def plot_relative_frequency_bar(\n    df: pd.DataFrame,\n    x_col: str,\n    hue_col: str,\n    ax: Optional[Axes] = None,\n    **barplot_kwargs\n) -> Axes: \n    \"\"\"\n    Descrição: Plota um gráfico de barras mostrando a frequência relativa (porcentagem)\n             de uma categoria 'hue_col' dentro de cada categoria de 'x_col'.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        x_col (str): Nome da coluna para o eixo X (grupos principais).\n        hue_col (str): Nome da coluna para segmentação por cor (subgrupos).\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar.\n        **barplot_kwargs: Argumentos adicionais para sns.barplot().\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame.\n        ValueError: Se 'x_col' ou 'hue_col' não existirem no DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if x_col not in df.columns:\n        raise ValueError(f\"A coluna '{x_col}' não existe no DataFrame.\")\n    if hue_col not in df.columns:\n        raise ValueError(f\"A coluna '{hue_col}' não existe no DataFrame.\")\n\n    counts = df.groupby([x_col, hue_col]).size().unstack(fill_value=0)\n    total_counts = counts.sum(axis=1)\n    \n    total_counts[total_counts == 0] = 1 \n    percentages = counts.div(total_counts, axis=0) * 100\n\n    plot_data = percentages.reset_index().melt(id_vars=[x_col], value_name='percentage')\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=barplot_kwargs.pop('figsize', (10, 6)))\n\n    sns.barplot(ax=ax, x=x_col, y='percentage', hue=hue_col, data=plot_data, **barplot_kwargs)\n\n    ax.set_title(barplot_kwargs.pop('title', f'Frequência Relativa de {hue_col} por {x_col}'))\n    ax.set_ylabel('Porcentagem Relativa (%)')\n    ax.set_xlabel(x_col.capitalize())\n    ax.set_ylim(0, 100)\n    ax.legend(title=hue_col.capitalize())\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 74, "titulo": "plot_kmeans_elbow_curve", "categoria": "Aprendizado Não Supervisionado", "subcategoria": "Clustering", "descricao": "Plota a curva do cotovelo (Elbow Method) para ajudar a determinar o número ótimo de clusters (k).", "codigo_funcao": "def plot_kmeans_elbow_curve(\n    data: Union[pd.DataFrame, np.ndarray],\n    k_range: Union[List[int], range] = range(2, 11),\n    random_state: Optional[int] = 42,\n    ax: Optional[plt.Axes] = None\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Calcula e plota a curva do cotovelo (Elbow Method) para determinar o\n             número ótimo de clusters (k) para o algoritmo K-Means.\n\n    Args:\n        data (Union[pd.DataFrame, np.ndarray]): Os dados para clusterização. Se for\n            um DataFrame, apenas as colunas numéricas serão usadas.\n        k_range (Union[List[int], range], optional): Um intervalo ou lista de valores\n            de 'k' (número de clusters) para testar. Defaults to range(2, 11).\n        random_state (Optional[int], optional): Seed para reprodutibilidade. Defaults to 42.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico da curva do cotovelo.\n\n    Raises:\n        TypeError: Se 'data' não for um DataFrame ou array NumPy.\n        ValueError: Se não houver colunas numéricas, se 'k_range' for inválido (vazio\n                    ou com min < 2), ou se os dados tiverem menos amostras que o k mínimo.\n    \"\"\"\n    if not isinstance(data, (pd.DataFrame, np.ndarray)):\n        raise TypeError(\"O argumento 'data' deve ser um DataFrame pandas ou array NumPy.\")\n\n    if isinstance(data, pd.DataFrame):\n        X = data.select_dtypes(include=np.number).values\n        if X.shape[1] == 0:\n            raise ValueError(\"Nenhuma coluna numérica encontrada no DataFrame.\")\n    else:\n        X = data\n\n    if not k_range or min(k_range) < 2:\n        raise ValueError(\"'k_range' deve começar com um número de clusters de pelo menos 2.\")\n    if X.shape[0] < min(k_range):\n        raise ValueError(f\"Número de amostras ({X.shape[0]}) é menor que o k mínimo \"\n                         f\"em k_range ({min(k_range)}).\")\n\n    inertia_scores = []\n    k_list = list(k_range) # Garante que é uma lista para set_xticks\n    \n    for k in k_list:\n        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init='auto')\n        kmeans.fit(X)\n        inertia_scores.append(kmeans.inertia_)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n    ax.plot(k_list, inertia_scores, marker='o', linestyle='--')\n    ax.set_title('Curva do Cotovelo para K-Means')\n    ax.set_xlabel('Número de Clusters (k)')\n    ax.set_ylabel('Inércia (Soma das Distâncias Quadradas)')\n    ax.set_xticks(k_list)\n    ax.grid(True)\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 75, "titulo": "plot_pie_chart", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Plota gráfico de pizza a partir de uma pandas Series ou de uma coluna categórica de um DataFrame; oculta rótulos de fatias pequenas e adiciona legenda para fatias menores que um limiar percentual.", "codigo_funcao": "def plot_pie_chart(\n    data: Union[pd.Series, pd.DataFrame],\n    column: Optional[str] = None,\n    min_pct_to_show_label: float = 3.0,\n    ax: Optional[plt.Axes] = None,\n    **pie_kwargs\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Cria um gráfico de pizza a partir de uma série de dados ou coluna de um\n             DataFrame, com a opção de ocultar rótulos de fatias pequenas.\n\n    Args:\n        data (Union[pd.Series, pd.DataFrame]): A fonte dos dados. Pode ser uma Série\n            pandas de contagens (ex: value_counts()) ou um DataFrame.\n        column (Optional[str], optional): Se 'data' for um DataFrame, especifica a\n            coluna categórica a ser usada para o gráfico. Defaults to None.\n        min_pct_to_show_label (float, optional): A porcentagem mínima que uma fatia\n            deve ter para que seu rótulo seja exibido. Defaults to 3.0.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n        **pie_kwargs: Argumentos adicionais a serem passados para `plt.pie()`\n            (ex: colors, explode, startangle).\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico de pizza.\n        \n    Raises:\n        TypeError: Se 'data' não for uma Série pandas ou DataFrame.\n        ValueError: Se 'data' for um DataFrame e 'column' não for fornecido,\n                    ou se 'column' não existir.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        if column is None:\n            raise ValueError(\"O argumento 'column' é necessário quando 'data' é um DataFrame.\")\n        if column not in data.columns:\n             raise ValueError(f\"A coluna '{column}' não existe no DataFrame.\")\n        series = data[column].value_counts()\n    elif isinstance(data, pd.Series):\n        series = data\n    else:\n        raise TypeError(\"O argumento 'data' deve ser uma Série pandas ou DataFrame.\")\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=pie_kwargs.pop('figsize', (8, 8)))\n\n    # Função aninhada para formatar a porcentagem (autopct)\n    def make_autopct(values):\n        def my_autopct(pct):\n            return f'{pct:.1f}%' if pct >= min_pct_to_show_label else ''\n        return my_autopct\n\n    # Lógica para os rótulos (labels)\n    total = series.sum()\n    labels = [\n        label if (val / total * 100) >= min_pct_to_show_label else ''\n        for label, val in series.items()\n    ]\n\n    wedges, texts, autotexts = ax.pie(\n        series,\n        labels=labels,\n        autopct=make_autopct(series.values),\n        startangle=pie_kwargs.pop('startangle', 90),\n        **pie_kwargs\n    )\n    ax.axis('equal')\n    \n    title = pie_kwargs.pop('title', f'Distribuição de {series.name or (column or \"Dados\")}')\n    ax.set_title(title)\n\n    # Adiciona uma legenda para itens pequenos se houver algum\n    small_slices_mask = (series / total * 100) < min_pct_to_show_label\n    if small_slices_mask.any():\n        # Extrai os wedges e labels correspondentes às fatias pequenas\n        small_wedges = [w for w, mask in zip(wedges, small_slices_mask) if mask]\n        small_labels = series[small_slices_mask].index.tolist()\n        \n        ax.legend(small_wedges, small_labels, \n                  title=\"Categorias < \" + str(min_pct_to_show_label) + \"%\",\n                  loc=\"center left\", \n                  bbox_to_anchor=(1, 0, 0.5, 1))\n\n    plt.tight_layout()\n    return ax", "bibliotecas": ["matplotlib", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 76, "titulo": "plot_top_n_categories", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Agrupa e agrega valores por categoria, plotando um gráfico de barras com as N principais categorias (suporte a soma, média, etc.; orientação horizontal ou vertical).", "codigo_funcao": "def plot_top_n_categories(\n    df: pd.DataFrame,\n    category_col: str,\n    value_col: str,\n    n: int = 10,\n    agg_func: str = 'sum',\n    orientation: Literal['v', 'h'] = 'h',\n    ax: Optional[plt.Axes] = None,\n    **barplot_kwargs\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Agrega valores e plota um gráfico de barras com as 'n' principais\n             categorias.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        category_col (str): Coluna com as categorias a serem agrupadas.\n        value_col (str): Coluna com os valores a serem agregados.\n        n (int, optional): Número de categorias principais a serem exibidas. Defaults to 10.\n        agg_func (str, optional): Função de agregação ('sum', 'mean', 'count', etc.).\n            Defaults to 'sum'.\n        orientation (Literal['v', 'h'], optional): Orientação do gráfico ('v' para vertical,\n            'h' para horizontal). Defaults to 'h'.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Defaults to None.\n        **barplot_kwargs: Argumentos de palavra-chave para sns.barplot().\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico.\n        \n    Raises:\n        TypeError: Se 'df' não for um DataFrame.\n        ValueError: Se 'category_col' ou 'value_col' não existirem,\n                    'value_col' não for numérica (para 'sum'/'mean' etc.),\n                    'n' < 1, ou 'orientation' for inválido.\n        Exception: Se 'agg_func' for uma função de agregação inválida para o pandas.\n    \"\"\"\n    # Validação de Entradas (Regra 6)\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if category_col not in df.columns:\n        raise ValueError(f\"A coluna de categoria '{category_col}' não existe no DataFrame.\")\n    if value_col not in df.columns:\n        raise ValueError(f\"A coluna de valor '{value_col}' não existe no DataFrame.\")\n    if not isinstance(n, int) or n < 1:\n        raise ValueError(f\"O argumento 'n' deve ser um inteiro positivo (>= 1).\")\n    if orientation not in ['v', 'h']:\n        raise ValueError(\"O argumento 'orientation' deve ser 'v' (vertical) ou 'h' (horizontal).\")\n\n    # Validação de tipo para agg_func\n    numeric_agg_funcs = ['sum', 'mean', 'median', 'std', 'var']\n    if agg_func in numeric_agg_funcs and not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise ValueError(f\"A função de agregação '{agg_func}' requer que 'value_col' ('{value_col}') seja numérica.\")\n\n    # Lógica da função\n    try:\n        grouped_data = df.groupby(category_col)[value_col].agg(agg_func).nlargest(n)\n    except Exception as e:\n        raise ValueError(f\"Falha ao agregar dados. A função 'agg_func' ('{agg_func}') é válida? Erro: {e}\")\n        \n    plot_data = grouped_data.reset_index()\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=barplot_kwargs.pop('figsize', (10, 8)))\n\n    if orientation == 'h':\n        sns.barplot(data=plot_data, x=value_col, y=category_col, ax=ax, **barplot_kwargs)\n        ax.set_xlabel(f'Valor Agregado ({agg_func.capitalize()})')\n        ax.set_ylabel(category_col.capitalize())\n    else: # orientation == 'v'\n        sns.barplot(data=plot_data, x=category_col, y=value_col, ax=ax, **barplot_kwargs)\n        ax.set_xlabel(category_col.capitalize())\n        ax.set_ylabel(f'Valor Agregado ({agg_func.capitalize()})')\n        ax.tick_params(axis='x', rotation=45)\n\n    title = barplot_kwargs.pop('title', f'Top {n} {category_col.capitalize()} por {value_col.capitalize()}')\n    ax.set_title(title)\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 77, "titulo": "plot_pareto_chart", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Gera um gráfico de Pareto (barras de frequência + curva de porcentagem acumulada) a partir de uma Series ou de uma coluna categórica de um DataFrame; trata séries vazias exibindo mensagem apropriada.", "codigo_funcao": "def plot_pareto_chart(\n    series: Union[pd.Series, pd.DataFrame],\n    column: Optional[str] = None,\n    ax: Optional[plt.Axes] = None\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Cria um gráfico de Pareto para visualizar a frequência de categorias\n             e sua contribuição cumulativa.\n\n    Args:\n        series (Union[pd.Series, pd.DataFrame]): Uma Série ou DataFrame contendo os\n            dados categóricos.\n        column (Optional[str], optional): Se 'series' for um DataFrame, especifica\n            a coluna a ser usada. Defaults to None.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n\n    Returns:\n        plt.Axes: O objeto Axes principal do Matplotlib contendo o gráfico.\n        \n    Raises:\n        TypeError: Se 'series' não for uma Série pandas ou DataFrame.\n        ValueError: Se 'series' for um DataFrame e 'column' não for fornecido,\n                    ou se 'column' não existir.\n    \"\"\"\n    if isinstance(series, pd.DataFrame):\n        if column is None:\n            raise ValueError(\"O argumento 'column' é necessário para DataFrames.\")\n        if column not in series.columns:\n             raise ValueError(f\"A coluna '{column}' não existe no DataFrame.\")\n        data = series[column]\n    elif isinstance(series, pd.Series):\n        data = series\n    else:\n        raise TypeError(\"A entrada deve ser uma Série pandas ou DataFrame.\")\n\n    # Calcula a frequência e a porcentagem cumulativa\n    freq = data.value_counts().sort_values(ascending=False)\n    \n    if freq.empty:\n        print(f\"Aviso: Não há dados na coluna/série '{data.name}' para plotar.\")\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(12, 7))\n        ax.text(0.5, 0.5, \"Sem dados para plotar.\",\n                horizontalalignment='center', verticalalignment='center',\n                transform=ax.transAxes, color='red')\n        return ax\n\n    df_pareto = pd.DataFrame({'frequencia': freq})\n    df_pareto['pct_cumulativa'] = (df_pareto['frequencia'].cumsum() / df_pareto['frequencia'].sum()) * 100\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(12, 7))\n\n    # Plota o gráfico de barras\n    ax.bar(df_pareto.index.astype(str), df_pareto['frequencia'], color='skyblue')\n    ax.set_ylabel('Frequência')\n    ax.set_title(f'Gráfico de Pareto para {data.name}')\n    ax.tick_params(axis='x', rotation=45)\n\n    # Cria um segundo eixo Y para la porcentagem cumulativa\n    ax2 = ax.twinx()\n    ax2.plot(df_pareto.index.astype(str), df_pareto['pct_cumulativa'], color='red', marker='o', ms=5)\n    ax2.yaxis.set_major_formatter(PercentFormatter())\n    ax2.set_ylabel('Porcentagem Acumulada')\n    ax2.set_ylim(0, 110) # Adiciona um pouco de espaço no topo\n\n    plt.tight_layout()\n    return ax", "bibliotecas": ["matplotlib", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 78, "titulo": "create_graph_from_dataframe", "categoria": "Grafos e Redes", "subcategoria": "Construção e Visualização", "descricao": "Cria um grafo NetworkX (Graph ou DiGraph) a partir de um DataFrame de arestas e, opcionalmente, adiciona atributos de nós a partir de um DataFrame de nós; valida tipos e colunas essenciais.", "codigo_funcao": "def create_graph_from_dataframe(\n    edges_df: pd.DataFrame,\n    source_col: str,\n    target_col: str,\n    edge_attr: Optional[List[str]] = None, \n    nodes_df: Optional[pd.DataFrame] = None,\n    node_id_col: Optional[str] = None,\n    is_directed: bool = False\n) -> nx.Graph:\n    \"\"\"\n    Descrição: Cria um grafo NetworkX a partir de DataFrames de arestas e, opcionalmente, de nós.\n\n    Args:\n        edges_df (pd.DataFrame): DataFrame contendo as arestas do grafo.\n        source_col (str): Nome da coluna em 'edges_df' que representa o nó de origem.\n        target_col (str): Nome da coluna em 'edges_df' que representa o nó de destino.\n        edge_attr (Optional[List[str]], optional): Lista de colunas de 'edges_df' para usar\n            como atributos das arestas (ex: 'weight'). Defaults to None.\n        nodes_df (Optional[pd.DataFrame], optional): DataFrame contendo os atributos dos nós.\n            Defaults to None.\n        node_id_col (Optional[str], optional): Nome da coluna em 'nodes_df' que serve como\n            identificador do nó. Essencial se 'nodes_df' for fornecido. Defaults to None.\n        is_directed (bool, optional): Se True, cria um grafo direcionado (DiGraph).\n            Se False, cria um grafo não direcionado (Graph). Defaults to False.\n\n    Returns:\n        nx.Graph: O objeto de grafo NetworkX criado (pode ser Graph ou DiGraph).\n\n    Raises:\n        TypeError: Se 'edges_df' ou 'nodes_df' não forem DataFrames.\n        ValueError: Se colunas essenciais não forem encontradas nos DataFrames.\n    \"\"\"\n    if not isinstance(edges_df, pd.DataFrame):\n        raise TypeError(\"O argumento 'edges_df' deve ser um DataFrame pandas.\")\n    if nodes_df is not None and not isinstance(nodes_df, pd.DataFrame):\n         raise TypeError(\"O argumento 'nodes_df' deve ser um DataFrame pandas ou None.\")\n\n    if source_col not in edges_df.columns or target_col not in edges_df.columns:\n        raise ValueError(f\"As colunas de origem ('{source_col}') e destino ('{target_col}') devem estar no DataFrame de arestas.\")\n\n    GraphClass = nx.DiGraph if is_directed else nx.Graph\n    g = nx.from_pandas_edgelist(edges_df, source_col, target_col, edge_attr=edge_attr, create_using=GraphClass)\n\n    if nodes_df is not None:\n        if node_id_col is None:\n             raise ValueError(\"'node_id_col' deve ser fornecido quando 'nodes_df' é utilizado.\")\n        if node_id_col not in nodes_df.columns:\n            raise ValueError(f\"A coluna de ID do nó ('{node_id_col}') não foi encontrada no DataFrame de nós.\")\n        # Define o ID do nó como índice para fácil busca de atributos\n        try:\n             node_attributes = nodes_df.set_index(node_id_col).to_dict('index')\n             nx.set_node_attributes(g, node_attributes)\n        except Exception as e:\n             raise RuntimeError(f\"Erro ao definir atributos dos nós: {e}. Verifique se '{node_id_col}' tem valores únicos.\")\n\n    return g", "bibliotecas": ["list", "networkx", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 79, "titulo": "detect_communities", "categoria": "Grafos e Redes", "subcategoria": "Análise de Métricas de Rede", "descricao": "Detecta comunidades em grafos não direcionados usando Girvan-Newman ou Greedy Modularity e calcula a modularidade.", "codigo_funcao": "def detect_communities(\n    g: nx.Graph,\n    method: Literal['girvan_newman', 'greedy_modularity'] = 'greedy_modularity'\n) -> Tuple[pd.DataFrame, float]:\n    \"\"\"\n    Descrição: Detecta comunidades em um grafo usando algoritmos baseados em modularidade\n             e retorna a alocação dos nós e a pontuação de modularidade.\n             NOTA: Estes métodos são geralmente projetados para grafos NÃO direcionados.\n\n    Args:\n        g (nx.Graph): O objeto de grafo NetworkX. Idealmente não direcionado (nx.Graph).\n        method (Literal['girvan_newman', 'greedy_modularity'], optional): O algoritmo a\n            ser usado. 'girvan_newman' é baseado em intermediação de arestas,\n            'greedy_modularity' é um método hierárquico rápido. Defaults to 'greedy_modularity'.\n\n    Returns:\n        Tuple[pd.DataFrame, float]: Uma tupla contendo:\n            - Um DataFrame mapeando cada nó ao seu ID de comunidade.\n            - A pontuação de modularidade da partição encontrada.\n\n    Raises:\n        TypeError: Se 'g' não for um objeto de grafo NetworkX.\n        ValueError: Se o método for inválido, ou se o grafo for direcionado (pois os\n                    métodos podem não ser apropriados ou falhar).\n        ImportError: Se funções de comunidade não puderem ser importadas (raro).\n    \"\"\"\n    # Importa funções de comunidade aqui para evitar ImportError se networkx for antigo\n    try:\n        from networkx.algorithms.community import girvan_newman, greedy_modularity_communities\n        from networkx.algorithms.community import modularity\n    except ImportError as e:\n         raise ImportError(f\"Não foi possível importar funções de comunidade do networkx: {e}\")\n\n    if not isinstance(g, (nx.Graph, nx.DiGraph)):\n        raise TypeError(\"O argumento 'g' deve ser um objeto de grafo NetworkX.\")\n\n    # Validação crucial: Métodos geralmente assumem grafo não direcionado\n    if isinstance(g, nx.DiGraph):\n         raise ValueError(\"Métodos de comunidade implementados ('girvan_newman', 'greedy_modularity') \"\n                          \"e o cálculo de modularidade são geralmente definidos para grafos NÃO direcionados. \"\n                          \"Passe um nx.Graph ou converta seu DiGraph.\")\n         # Alternativa: print(\"Aviso: Aplicando detecção de comunidade a um grafo direcionado. \"\n         #                   \"Resultados podem ser inesperados.\")\n\n    partition: Optional[Dict[Any, int]] = None # type hint para partition\n\n    if method == 'girvan_newman':\n        # Girvan-Newman retorna um iterador de tuplas de comunidades\n        communities_generator = girvan_newman(g)\n        # Pega a melhor partição (geralmente a primeira com maior modularidade)\n        # Nota: encontrar a 'melhor' pode exigir iteração e cálculo de modularidade\n        try:\n             top_level_communities = next(communities_generator)\n             partition = {node: i for i, community in enumerate(top_level_communities) for node in community}\n        except StopIteration:\n             print(\"Aviso: O gerador Girvan-Newman não produziu partições.\")\n             # Retorna partição vazia? Ou erro? Retornar vazio é mais seguro.\n             return pd.DataFrame(columns=['community_id']), float('nan')\n             \n    elif method == 'greedy_modularity':\n        # Greedy modularity retorna uma lista de frozensets\n        communities = list(greedy_modularity_communities(g)) # Converte para lista\n        if not communities:\n              print(\"Aviso: Greedy modularity não encontrou comunidades.\")\n              return pd.DataFrame(columns=['community_id']), float('nan')\n        partition = {node: i for i, community in enumerate(communities) for node in community}\n    else:\n        raise ValueError(\"Método inválido. Escolha 'girvan_newman' ou 'greedy_modularity'.\")\n\n    if partition is None or not partition: # Se algo deu errado na criação da partição\n         print(\"Aviso: Não foi possível criar a partição de comunidade.\")\n         return pd.DataFrame(columns=['community_id']), float('nan')\n\n    # Calcula modularidade - precisa converter a partição de volta para lista de sets\n    # Gera a lista de sets das comunidades a partir do dicionário partition\n    communities_sets = []\n    if partition:\n        num_communities = max(partition.values()) + 1\n        communities_sets = [set() for _ in range(num_communities)]\n        for node, comm_id in partition.items():\n            communities_sets[comm_id].add(node)\n    \n    modularity_score = float('nan') # Default caso não consiga calcular\n    if communities_sets: # Só calcula se existirem comunidades\n        try:\n             # Usa a função importada\n             modularity_score = modularity(g, communities_sets)\n        except Exception as e:\n             print(f\"Aviso: Erro ao calcular a modularidade: {e}\")\n\n    df_membership = pd.DataFrame(partition.items(), columns=['node', 'community_id']).set_index('node')\n\n    return df_membership, modularity_score", "bibliotecas": ["networkx", "numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 80, "titulo": "plot_graph_with_communities", "categoria": "Grafos e Redes", "subcategoria": "Construção e Visualização", "descricao": "Plota um grafo NetworkX, colorindo os nós de acordo com um mapeamento de comunidades.", "codigo_funcao": "def plot_graph_with_communities(\n    g: nx.Graph, # Aceita Graph ou DiGraph\n    community_mapping: Union[pd.DataFrame, pd.Series, Dict[Any, int]], # Chave pode ser Any\n    layout_func: str = 'spring_layout',\n    ax: Optional[plt.Axes] = None,\n    **draw_kwargs\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Plota um grafo, colorindo os nós de acordo com as comunidades detectadas.\n\n    Args:\n        g (nx.Graph): O objeto de grafo NetworkX (Graph ou DiGraph).\n        community_mapping (Union[pd.DataFrame, pd.Series, Dict[Any, int]]): Um mapeamento\n            de nó para ID de comunidade. Pode ser um dicionário, Série pandas (com nós no índice)\n            ou DataFrame com nós no índice e uma coluna 'community_id'.\n        layout_func (str, optional): O nome da função de layout do NetworkX a ser usada\n            (ex: 'spring_layout', 'circular_layout', 'kamada_kawai_layout'). Defaults to 'spring_layout'.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Defaults to None.\n        **draw_kwargs: Argumentos de palavra-chave para nx.draw_networkx()\n            (ex: node_size=300, font_size=8).\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o gráfico.\n\n    Raises:\n        TypeError: Se 'g' não for grafo NetworkX ou 'community_mapping' tiver tipo inválido.\n        ValueError: Se 'community_mapping' for DataFrame e não tiver coluna 'community_id',\n                    ou se 'layout_func' não for um layout válido do NetworkX.\n    \"\"\"\n    if not isinstance(g, (nx.Graph, nx.DiGraph)):\n        raise TypeError(\"O argumento 'g' deve ser um objeto de grafo NetworkX.\")\n\n    node_to_community: Dict[Any, int]\n    if isinstance(community_mapping, pd.DataFrame):\n        if 'community_id' not in community_mapping.columns:\n             raise ValueError(\"Se 'community_mapping' for DataFrame, deve ter a coluna 'community_id'.\")\n        # Assume que o índice do DataFrame são os nós\n        node_to_community = community_mapping['community_id'].to_dict()\n    elif isinstance(community_mapping, pd.Series):\n        # Assume que o índice da Série são os nós\n        node_to_community = community_mapping.to_dict()\n    elif isinstance(community_mapping, dict):\n        node_to_community = community_mapping\n    else:\n        raise TypeError(\"O argumento 'community_mapping' deve ser um Dict, pd.Series ou pd.DataFrame.\")\n\n    # Gera uma paleta de cores para as comunidades\n    unique_communities = sorted(list(set(node_to_community.values())))\n    num_communities = len(unique_communities)\n    palette = sns.color_palette(\"hsv\", num_communities)\n    \n    # Mapeia ID de comunidade para cor (lida com IDs não sequenciais)\n    community_id_to_color = {comm_id: palette[i] for i, comm_id in enumerate(unique_communities)}\n    \n    # Define a cor de cada nó. Usa uma cor padrão (cinza) se nó não estiver no mapeamento.\n    default_color = (0.5, 0.5, 0.5) # Cinza\n    node_colors = [community_id_to_color.get(node_to_community.get(node), default_color) for node in g.nodes()]\n\n\n    # Define o layout\n    try:\n        pos_function = getattr(nx, layout_func)\n        pos = pos_function(g)\n    except AttributeError:\n         raise ValueError(f\"Função de layout '{layout_func}' não encontrada no NetworkX.\")\n    except Exception as e:\n         raise RuntimeError(f\"Erro ao calcular o layout '{layout_func}': {e}\")\n\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=draw_kwargs.pop('figsize', (15, 15)))\n\n    # Desenha o grafo\n    try:\n        nx.draw_networkx(\n            g,\n            pos,\n            ax=ax,\n            node_color=node_colors,\n            with_labels=draw_kwargs.pop('with_labels', True),\n            node_size=draw_kwargs.pop('node_size', 300),\n            width=draw_kwargs.pop('width', 0.5),\n            **draw_kwargs\n        )\n    except Exception as e:\n         raise RuntimeError(f\"Erro ao desenhar o grafo com nx.draw_networkx: {e}\")\n         \n    title = draw_kwargs.pop('title', f'Grafo com {num_communities} Comunidades Detectadas')\n    ax.set_title(title)\n    ax.axis('off') # Remove eixos para visualização de grafos\n    plt.tight_layout()\n\n    return ax", "bibliotecas": ["matplotlib", "networkx", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 81, "titulo": "plot_treemap", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Gera um Treemap Plotly para visualizar dados hierárquicos agregados.", "codigo_funcao": "def plot_treemap(\n    df: pd.DataFrame,\n    path_cols: List[str],\n    value_col: str,\n    root_node_label: Optional[str] = \"Total\",\n    title: Optional[str] = None,\n    textfont_size: int = 16,\n    fig_width: int = 1400,\n    fig_height: int = 800\n) -> go.Figure:\n    \"\"\"\n    Descrição: Gera um Treemap para visualizar dados hierárquicos agregados.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada, geralmente pré-agregado.\n        path_cols (List[str]): Lista de nomes de colunas que definem a hierarquia\n            (ex: ['Categoria', 'Subcategoria']).\n        value_col (str): Nome da coluna contendo os valores numéricos para o tamanho\n            dos retângulos.\n        root_node_label (Optional[str], optional): Rótulo para o nó raiz. Se None, não adiciona\n            um nó raiz explícito. Defaults to \"Total\".\n        title (Optional[str], optional): Título do gráfico. Se None, um título padrão é gerado. Defaults to None.\n        textfont_size (int, optional): Tamanho da fonte do texto dentro dos retângulos. Defaults to 16.\n        fig_width (int, optional): Largura da figura. Defaults to 1400.\n        fig_height (int, optional): Altura da figura. Defaults to 800.\n\n    Returns:\n        go.Figure: O objeto Figure do Plotly com o Treemap.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se alguma coluna em 'path_cols' ou 'value_col' não for encontrada.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    required_cols = path_cols + [value_col]\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Colunas não encontradas no DataFrame: {missing_cols}\")\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise TypeError(f\"A coluna de valor '{value_col}' deve ser numérica.\")\n\n    # Define o caminho, incluindo o nó raiz opcional\n    path = [px.Constant(root_node_label)] + path_cols if root_node_label else path_cols\n\n    # Título Padrão\n    hierarchy_str = \" -> \".join([c.capitalize() for c in path_cols])\n    default_title = f'Distribuição Hierárquica: {hierarchy_str}'\n    final_title = title if title is not None else default_title\n\n    fig = px.treemap(\n        df,\n        path=path,\n        values=value_col,\n        title=final_title\n    )\n\n    fig.update_traces(\n        textinfo=\"label+value+percent parent\", # Mostra label, valor e % do pai\n        textfont_size=textfont_size\n    )\n    fig.update_layout(\n        width=fig_width,\n        height=fig_height,\n        margin=dict(t=50, l=25, r=25, b=25)\n    )\n\n    return fig", "bibliotecas": ["numpy", "pandas", "plotly", "typing"], "versao": "0.1.0"}
{"id_funcao": 82, "titulo": "plot_boxplot", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Gera um boxplot Plotly para visualizar a distribuição numérica, opcionalmente agrupada por categoria.", "codigo_funcao": "def plot_boxplot(\n    df: pd.DataFrame,\n    numeric_col: str,\n    category_col: Optional[str] = None,\n    title: Optional[str] = None,\n    xaxis_title: Optional[str] = None,\n    yaxis_title: Optional[str] = None,\n    points: str = 'all' # 'all', 'outliers', False\n) -> go.Figure:\n    \"\"\"\n    Descrição: Gera um boxplot para visualizar a distribuição de uma coluna numérica,\n             opcionalmente agrupada por uma coluna categórica.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        numeric_col (str): Nome da coluna numérica (eixo Y).\n        category_col (Optional[str], optional): Nome da coluna categórica para agrupar (eixo X).\n            Se None, um único boxplot é gerado. Defaults to None.\n        title (Optional[str], optional): Título do gráfico. Se None, um padrão é gerado. Defaults to None.\n        xaxis_title (Optional[str], optional): Título do eixo X. Se None, usa category_col ou fica vazio. Defaults to None.\n        yaxis_title (Optional[str], optional): Título do eixo Y. Se None, usa numeric_col. Defaults to None.\n        points (str, optional): Quais pontos mostrar no boxplot ('all', 'outliers', False). Defaults to 'all'.\n\n    Returns:\n        go.Figure: O objeto Figure do Plotly com o(s) boxplot(s).\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se 'numeric_col' ou 'category_col' (se fornecido) não forem encontradas,\n                    ou se 'numeric_col' não for numérica.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if numeric_col not in df.columns:\n        raise ValueError(f\"Coluna numérica '{numeric_col}' não encontrada no DataFrame.\")\n    if not pd.api.types.is_numeric_dtype(df[numeric_col]):\n        raise ValueError(f\"A coluna '{numeric_col}' deve ser numérica.\")\n    if category_col is not None and category_col not in df.columns:\n        raise ValueError(f\"Coluna de categoria '{category_col}' não encontrada no DataFrame.\")\n\n    # Títulos padrão\n    base_title = f'Distribuição de {numeric_col.capitalize()}'\n    if category_col:\n        default_title = f'{base_title} por {category_col.capitalize()}'\n        default_xaxis_title = category_col.capitalize()\n    else:\n        default_title = base_title\n        default_xaxis_title = '' # Sem eixo X categórico\n\n    final_title = title if title is not None else default_title\n    final_xaxis_title = xaxis_title if xaxis_title is not None else default_xaxis_title\n    final_yaxis_title = yaxis_title if yaxis_title is not None else numeric_col.capitalize()\n\n    fig = px.box(\n        df,\n        x=category_col, # Será None se não fornecido\n        y=numeric_col,\n        title=final_title,\n        color=category_col, # Colore por categoria se agrupado\n        points=points,\n        labels={\n            numeric_col: final_yaxis_title,\n            category_col if category_col else 'x': final_xaxis_title # Hack para label X se category_col=None\n        }\n    )\n\n    # Remove o nome do eixo X se não houver agrupamento\n    if category_col is None:\n        fig.update_layout(xaxis_title=None, showlegend=False) # Remove título do eixo X e legenda inútil\n\n    return fig", "bibliotecas": ["numpy", "pandas", "plotly", "typing"], "versao": "0.1.0"}
{"id_funcao": 83, "titulo": "plot_histogram_plotly", "categoria": "Visualização de Dados", "subcategoria": "Distribuições Univariadas", "descricao": "Gera um histograma Plotly para visualizar a distribuição numérica, com boxplot marginal opcional.", "codigo_funcao": "def plot_histogram_plotly(\n    df: pd.DataFrame,\n    numeric_col: str,\n    nbins: Optional[int] = None,\n    title: Optional[str] = None,\n    xaxis_title: Optional[str] = None,\n    yaxis_title: str = 'Contagem',\n    show_boxplot: bool = True\n) -> go.Figure:\n    \"\"\"\n    Descrição: Gera um histograma para visualizar a distribuição de uma coluna numérica,\n             usando Plotly Express.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        numeric_col (str): Nome da coluna numérica a ser plotada.\n        nbins (Optional[int], optional): Número de bins desejado. Se None, Plotly escolhe automaticamente. Defaults to None.\n        title (Optional[str], optional): Título do gráfico. Se None, um padrão é gerado. Defaults to None.\n        xaxis_title (Optional[str], optional): Título do eixo X. Se None, usa numeric_col. Defaults to None.\n        yaxis_title (str, optional): Título do eixo Y. Defaults to 'Contagem'.\n        show_boxplot (bool, optional): Se True, mostra um boxplot marginal acima do histograma. Defaults to True.\n\n    Returns:\n        go.Figure: O objeto Figure do Plotly com o histograma.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se 'numeric_col' não for encontrada ou não for numérica.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if numeric_col not in df.columns:\n        raise ValueError(f\"Coluna numérica '{numeric_col}' não encontrada no DataFrame.\")\n    if not pd.api.types.is_numeric_dtype(df[numeric_col]):\n        raise ValueError(f\"A coluna '{numeric_col}' deve ser numérica.\")\n\n    # Títulos padrão\n    default_title = f'Distribuição de {numeric_col.capitalize()}'\n    final_title = title if title is not None else default_title\n    final_xaxis_title = xaxis_title if xaxis_title is not None else numeric_col.capitalize()\n\n    fig = px.histogram(\n        df,\n        x=numeric_col,\n        title=final_title,\n        marginal='box' if show_boxplot else None,\n        nbins=nbins,\n        labels={numeric_col: final_xaxis_title}, # Define label X\n    )\n    # Define label Y\n    fig.update_layout(yaxis_title=yaxis_title)\n\n    return fig", "bibliotecas": ["numpy", "pandas", "plotly", "typing"], "versao": "0.1.0"}
{"id_funcao": 84, "titulo": "plot_bar_chart_from_dict", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Gera um gráfico de barras Plotly ordenado a partir de um dicionário de categorias e valores.", "codigo_funcao": "def plot_bar_chart_from_dict(\n    data_dict: Dict[str, Union[int, float]],\n    category_name: str = 'Categoria',\n    value_name: str = 'Valor',\n    sort_values: bool = True,\n    sort_ascending: bool = False,\n    title: Optional[str] = None,\n    xaxis_title: Optional[str] = None,\n    yaxis_title: Optional[str] = None\n) -> go.Figure:\n    \"\"\"\n    Descrição: Gera um gráfico de barras ordenado a partir de um dicionário\n             de contagens ou valores.\n\n    Args:\n        data_dict (Dict[str, Union[int, float]]): Dicionário onde chaves são categorias (eixo X)\n            e valores são as alturas das barras (eixo Y).\n        category_name (str, optional): Nome a ser usado para a coluna de categorias no eixo X. Defaults to 'Categoria'.\n        value_name (str, optional): Nome a ser usado para a coluna de valores no eixo Y. Defaults to 'Valor'.\n        sort_values (bool, optional): Se True, ordena as barras pelos valores. Defaults to True.\n        sort_ascending (bool, optional): Se True e sort_values=True, ordena em ordem ascendente. Defaults to False (descendente).\n        title (Optional[str], optional): Título do gráfico. Se None, um padrão é gerado. Defaults to None.\n        xaxis_title (Optional[str], optional): Título do eixo X. Se None, usa category_name. Defaults to None.\n        yaxis_title (Optional[str], optional): Título do eixo Y. Se None, usa value_name. Defaults to None.\n\n    Returns:\n        go.Figure: O objeto Figure do Plotly com o gráfico de barras.\n\n    Raises:\n        TypeError: Se 'data_dict' não for um dicionário.\n        ValueError: Se 'data_dict' estiver vazio.\n    \"\"\"\n    if not isinstance(data_dict, dict):\n        raise TypeError(\"O argumento 'data_dict' deve ser um dicionário.\")\n    if not data_dict:\n        raise ValueError(\"O dicionário 'data_dict' não pode estar vazio.\")\n\n    df_plot = pd.DataFrame(data_dict.items(), columns=[category_name, value_name])\n\n    if sort_values:\n        df_plot = df_plot.sort_values(by=value_name, ascending=sort_ascending)\n\n    # Títulos padrão\n    default_title = f'{value_name.capitalize()} por {category_name.capitalize()}'\n    final_title = title if title is not None else default_title\n    final_xaxis_title = xaxis_title if xaxis_title is not None else category_name.capitalize()\n    final_yaxis_title = yaxis_title if yaxis_title is not None else value_name.capitalize()\n\n    fig = px.bar(\n        df_plot,\n        x=category_name,\n        y=value_name,\n        title=final_title,\n        text_auto=True,\n        labels={\n            category_name: final_xaxis_title,\n            value_name: final_yaxis_title\n        }\n    )\n\n    # Garante que o eixo X mantenha a ordem do DataFrame (importante se ordenado)\n    fig.update_xaxes(categoryorder='array', categoryarray=df_plot[category_name])\n\n    return fig", "bibliotecas": ["numpy", "pandas", "plotly", "typing"], "versao": "0.1.0"}
{"id_funcao": 85, "titulo": "plot_list_element_counts_bar", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Gera um gráfico de barras Plotly com a contagem de elementos únicos em listas de uma coluna.", "codigo_funcao": "def plot_list_element_counts_bar(\n    df: pd.DataFrame,\n    list_col: str,\n    element_name: str = 'Elemento',\n    count_col_name: str = 'Contagem',\n    title: Optional[str] = None,\n    xaxis_title: Optional[str] = None,\n    yaxis_title: Optional[str] = None\n) -> go.Figure:\n    \"\"\"\n    Descrição: Gera um gráfico de barras com a contagem de ocorrências de cada\n             elemento único presente nas listas de uma coluna do DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        list_col (str): Nome da coluna que contém listas de elementos (ex: strings).\n        element_name (str, optional): Nome genérico a ser usado para os elementos (eixo X). Defaults to 'Elemento'.\n        count_col_name (str, optional): Nome para a coluna de contagem (eixo Y). Defaults to 'Contagem'.\n        title (Optional[str], optional): Título do gráfico. Se None, um padrão é gerado. Defaults to None.\n        xaxis_title (Optional[str], optional): Título do eixo X. Se None, usa element_name. Defaults to None.\n        yaxis_title (Optional[str], optional): Título do eixo Y. Se None, usa count_col_name. Defaults to None.\n\n    Returns:\n        go.Figure: O objeto Figure do Plotly com o gráfico de barras.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se a coluna 'list_col' não for encontrada no DataFrame.\n        RuntimeError: Se ocorrer um erro ao usar 'explode' (ex: coluna não contém listas).\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if list_col not in df.columns:\n        raise ValueError(f\"Coluna '{list_col}' não encontrada no DataFrame.\")\n\n    df_no_na = df.dropna(subset=[list_col])\n    \n    if not df_no_na.empty and not pd.api.types.is_list_like(df_no_na[list_col].iloc[0]):\n        raise RuntimeError(f\"Coluna '{list_col}' não parece conter listas após remover NaNs. 'explode' não pode ser aplicado.\")\n\n    try:\n        df_exploded = df_no_na.explode(list_col)\n    except Exception as e:\n        raise RuntimeError(f\"Erro ao usar 'explode' na coluna '{list_col}': {e}. \"\n                           \"Verifique se a coluna contém listas.\")\n\n    counts = df_exploded[list_col].value_counts().reset_index()\n    counts.columns = [element_name, count_col_name]\n\n    default_title = f'Contagem de {element_name.capitalize()}s Mais Comuns'\n    final_title = title if title is not None else default_title\n    final_xaxis_title = xaxis_title if xaxis_title is not None else element_name.capitalize()\n    final_yaxis_title = yaxis_title if yaxis_title is not None else count_col_name\n\n    fig = px.bar(\n        counts,\n        x=element_name,\n        y=count_col_name,\n        title=final_title,\n        text_auto=True,\n        labels={\n            element_name: final_xaxis_title,\n            count_col_name: final_yaxis_title\n        }\n    )\n    return fig", "bibliotecas": ["numpy", "pandas", "plotly", "typing"], "versao": "0.1.0"}
{"id_funcao": 86, "titulo": "plot_bar_chart", "categoria": "Visualização de Dados", "subcategoria": "Comparações Categóricas e Frequência", "descricao": "Gera um gráfico de barras Plotly (vertical ou horizontal) da contagem de ocorrências em uma coluna categórica.", "codigo_funcao": "def plot_bar_chart(\n    df: pd.DataFrame,\n    category_col: str,\n    orientation: Literal['v', 'h'] = 'v',\n    count_col_name: str = 'Contagem',\n    title: Optional[str] = None,\n    xaxis_title: Optional[str] = None,\n    yaxis_title: Optional[str] = None,\n    dynamic_height: bool = True, # Aplicável apenas se orientation='h'\n    text_outside: bool = True    # Aplicável apenas se orientation='h'\n) -> go.Figure:\n    \"\"\"\n    Descrição: Gera um gráfico de barras (vertical ou horizontal) da contagem de\n             ocorrências em uma coluna categórica.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        category_col (str): Nome da coluna categórica a ser contada.\n        orientation (Literal['v', 'h'], optional): Orientação do gráfico ('v' para vertical,\n            'h' para horizontal). Defaults to 'v'.\n        count_col_name (str, optional): Nome a ser dado à coluna de contagem no DF interno. Defaults to 'Contagem'.\n        title (Optional[str], optional): Título do gráfico. Se None, um título padrão é gerado. Defaults to None.\n        xaxis_title (Optional[str], optional): Título do eixo X. Se None, gerado automaticamente. Defaults to None.\n        yaxis_title (Optional[str], optional): Título do eixo Y. Se None, gerado automaticamente. Defaults to None.\n        dynamic_height (bool, optional): Se True e orientation='h', ajusta a altura do\n            gráfico dinamicamente com base no número de categorias. Defaults to True.\n        text_outside (bool, optional): Se True e orientation='h', força o texto (contagem)\n            a aparecer fora das barras. Defaults to True.\n\n    Returns:\n        go.Figure: O objeto Figure do Plotly com o gráfico de barras.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se a coluna 'category_col' não for encontrada ou 'orientation' for inválido.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if category_col not in df.columns:\n        raise ValueError(f\"Coluna '{category_col}' não encontrada no DataFrame.\")\n    if orientation not in ['v', 'h']:\n        raise ValueError(\"O argumento 'orientation' deve ser 'v' (vertical) ou 'h' (horizontal).\")\n\n    counts = df[category_col].value_counts().reset_index()\n    counts.columns = [category_col, count_col_name]\n\n    # Define títulos padrão\n    default_title = f'Contagem por {category_col.capitalize()}'\n    final_title = title if title is not None else default_title\n\n    plot_kwargs = {\n        'data_frame': counts,\n        'title': final_title,\n        'text_auto': True,\n    }\n\n    if orientation == 'v':\n        plot_kwargs['x'] = category_col\n        plot_kwargs['y'] = count_col_name\n        final_xaxis_title = xaxis_title if xaxis_title is not None else category_col.capitalize()\n        final_yaxis_title = yaxis_title if yaxis_title is not None else count_col_name\n        plot_kwargs['labels'] = {category_col: final_xaxis_title, count_col_name: final_yaxis_title}\n    else: # orientation == 'h'\n        plot_kwargs['y'] = category_col\n        plot_kwargs['x'] = count_col_name\n        plot_kwargs['orientation'] = 'h'\n        counts = counts.sort_values(count_col_name, ascending=True) # Ordena para plot horizontal\n        plot_kwargs['data_frame'] = counts\n        final_xaxis_title = xaxis_title if xaxis_title is not None else count_col_name\n        final_yaxis_title = yaxis_title if yaxis_title is not None else category_col.capitalize()\n        plot_kwargs['labels'] = {category_col: final_yaxis_title, count_col_name: final_xaxis_title}\n        if dynamic_height:\n            num_categorias = len(counts)\n            altura_grafico = max(400, num_categorias * 30)\n            plot_kwargs['height'] = altura_grafico\n\n    fig = px.bar(**plot_kwargs)\n\n    if orientation == 'h':\n        if text_outside:\n            fig.update_traces(textposition='outside')\n        # Margem esquerda dinâmica baseada no comprimento máximo do rótulo no eixo Y\n        max_label_len = counts[category_col].astype(str).str.len().max()\n        left_margin = max(100, max_label_len * 7) # Ajuste heurístico\n        fig.update_layout(margin=dict(l=left_margin))\n\n    return fig", "bibliotecas": ["numpy", "pandas", "plotly", "typing"], "versao": "0.1.0"}
{"id_funcao": 87, "titulo": "load_data_from_json", "categoria": "Coleta e Carregamento de Dados", "subcategoria": "Carregamento de Arquivos", "descricao": "Carrega um arquivo JSONL (JSON Lines) para um DataFrame pandas.", "codigo_funcao": "def load_data_from_json(filepath: str) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Descrição: Carrega um arquivo JSONL para um DataFrame pandas.\n\n    Args:\n        filepath (str): O caminho para o arquivo JSONL.\n\n    Returns:\n        Optional[pd.DataFrame]: DataFrame carregado ou None se ocorrer um erro.\n\n    Raises:\n        FileNotFoundError: Se o arquivo especificado não for encontrado.\n        ValueError: Se ocorrer um erro ao decodificar o JSON.\n        Exception: Para outros erros inesperados de leitura.\n    \"\"\"\n    p = Path(filepath)\n    if not p.is_file():\n        raise FileNotFoundError(f\"Arquivo não encontrado em: {filepath}\")\n\n    try:\n        df = pd.read_json(filepath, lines=True)\n        return df\n    except ValueError as e:\n        raise ValueError(f\"Erro ao decodificar JSON em {filepath}: {e}\")\n    except Exception as e:\n        raise RuntimeError(f\"Erro inesperado ao carregar {filepath}: {e}\")", "bibliotecas": ["json", "os", "pandas", "pathlib", "shutil", "typing"], "versao": "0.1.0"}
{"id_funcao": 88, "titulo": "perform_optimal_binning", "categoria": "Engenharia e Transformação de Features", "subcategoria": "Discretização (Binning)", "descricao": "Realiza a discretização supervisionada (Optimal Binning) para maximizar o Information Value (IV).", "codigo_funcao": "def perform_optimal_binning(\n    x: Union[pd.Series, np.ndarray],\n    y: Union[pd.Series, np.ndarray],\n    variable_name: str,\n    solver: Literal['cp', 'mip'] = 'cp',\n    dtype: Literal['numerical', 'categorical'] = 'numerical'\n) -> pd.DataFrame:\n    \"\"\"\n    Descrição: Realiza a discretização supervisionada (Optimal Binning) de uma variável\n             contínua em relação a uma variável alvo binária, maximizando o\n             Information Value (IV).\n\n    Args:\n        x (Union[pd.Series, np.ndarray]): A variável preditora contínua.\n        y (Union[pd.Series, np.ndarray]): A variável alvo binária.\n        variable_name (str): O nome da variável preditora.\n        solver (Literal['cp', 'mip'], optional): O solver de otimização a ser usado.\n            Defaults to 'cp'.\n        dtype (Literal['numerical', 'categorical'], optional): O tipo de dado da variável.\n            Defaults to 'numerical'.\n\n    Returns:\n        pd.DataFrame: A tabela de binning detalhada contendo estatísticas como\n            WoE (Weight of Evidence) e IV (Information Value) para cada bin.\n\n    Raises:\n        ImportError: Se a biblioteca 'optbinning' não estiver instalada.\n        TypeError: Se 'x' ou 'y' não forem Séries pandas ou arrays NumPy.\n        ValueError: Se 'x' e 'y' tiverem comprimentos diferentes, ou se 'solver'\n                    ou 'dtype' forem inválidos.\n    \"\"\"\n    try:\n        from optbinning import OptimalBinning\n    except ImportError:\n        raise ImportError(\"A biblioteca 'optbinning' não está instalada. Execute 'pip install optbinning'.\")\n\n    if not isinstance(x, (pd.Series, np.ndarray)):\n        raise TypeError(\"O argumento 'x' deve ser uma Série pandas ou array NumPy.\")\n    if not isinstance(y, (pd.Series, np.ndarray)):\n        raise TypeError(\"O argumento 'y' deve ser uma Série pandas ou array NumPy.\")\n    if len(x) != len(y):\n        raise ValueError(f\"Os comprimentos de 'x' ({len(x)}) e 'y' ({len(y)}) são diferentes.\")\n    if solver not in ['cp', 'mip']:\n         raise ValueError(\"O argumento 'solver' deve ser 'cp' ou 'mip'.\")\n    if dtype not in ['numerical', 'categorical']:\n         raise ValueError(\"O argumento 'dtype' deve ser 'numerical' ou 'categorical'.\")\n\n    # Converte para array numpy caso sejam Séries (optbinning prefere)\n    x_np = np.asarray(x)\n    y_np = np.asarray(y)\n\n    optb = OptimalBinning(name=variable_name, dtype=dtype, solver=solver)\n    optb.fit(x_np, y_np)\n\n    binning_table = optb.binning_table\n    return binning_table.build()", "bibliotecas": ["numpy", "optbinning", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 89, "titulo": "discretize_unsupervised", "categoria": "Engenharia e Transformação de Features", "subcategoria": "Discretização (Binning)", "descricao": "Discretiza uma variável contínua usando estratégias não supervisionadas (uniforme, quantil, k-means).", "codigo_funcao": "def discretize_unsupervised(\n    series: Union[pd.Series, np.ndarray],\n    n_bins: int = 5,\n    strategy: Literal['uniform', 'quantile', 'kmeans'] = 'quantile'\n) -> Tuple[np.ndarray, pd.DataFrame]:\n    \"\"\"\n    Descrição: Discretiza uma variável contínua em N bins usando uma estratégia\n             não supervisionada (uniforme, quantil ou k-means).\n             Valores NaN são removidos antes da discretização e da contagem.\n\n    Args:\n        series (Union[pd.Series, np.ndarray]): Série de dados contínuos a ser discretizada.\n        n_bins (int, optional): O número de bins (intervalos) a serem criados. Deve ser >= 2.\n            Defaults to 5.\n        strategy (Literal['uniform', 'quantile', 'kmeans'], optional): A estratégia\n            de discretização. Defaults to 'quantile'.\n\n    Returns:\n        Tuple[np.ndarray, pd.DataFrame]: Uma tupla contendo:\n            - Um array NumPy com os dados discretizados (codificação ordinal). O array\n              terá o tamanho dos dados não nulos.\n            - Um DataFrame com a tabela de frequência para cada bin.\n\n    Raises:\n        TypeError: Se 'series' não for uma Série pandas ou array NumPy.\n        ValueError: Se a estratégia for inválida, 'n_bins' < 2, ou se a série\n                    contiver apenas NaNs (ou nenhum dado).\n    \"\"\"\n    if not isinstance(series, (pd.Series, np.ndarray)):\n        raise TypeError(\"A entrada 'series' deve ser uma Série pandas ou array NumPy.\")\n    if not isinstance(n_bins, int) or n_bins < 2:\n        raise ValueError(\"'n_bins' deve ser um inteiro >= 2.\")\n    if strategy not in ['uniform', 'quantile', 'kmeans']:\n        raise ValueError(\"A estratégia deve ser 'uniform', 'quantile', ou 'kmeans'.\")\n\n    # Garante que é uma Série pandas para facilitar o manuseio de NaNs e 'cut'\n    if isinstance(series, np.ndarray):\n        series = pd.Series(series.flatten())\n\n    data_clean = series.dropna()\n\n    if len(data_clean) == 0:\n        raise ValueError(\"A série de entrada não contém dados não nulos.\")\n    \n    if len(data_clean) < n_bins:\n        print(f\"Aviso: número de pontos de dados não nulos ({len(data_clean)}) é menor que n_bins ({n_bins}). \"\n              \"A discretização pode falhar ou ser imprecisa.\")\n\n    data_reshaped = data_clean.values.reshape(-1, 1)\n\n    try:\n        discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n        discretized_data = discretizer.fit_transform(data_reshaped).astype(int).flatten()\n    except Exception as e:\n        raise RuntimeError(f\"Erro ao aplicar KBinsDiscretizer (strategy='{strategy}', n_bins={n_bins}): {e}. \"\n                           \"Tente uma estratégia diferente ou menos bins.\")\n\n    # Cria a tabela de frequência usando os limites de bin do discretizador\n    bin_edges = discretizer.bin_edges_[0]\n    \n    freq_table = pd.DataFrame(columns=['Bin', 'Frequência'])\n    \n    # pd.cut pode falhar se os bin_edges não forem únicos (ex: dados muito distorcidos com 'quantile')\n    try:\n        # Garante que as bordas são únicas\n        if len(np.unique(bin_edges)) < len(bin_edges):\n            bin_edges = np.unique(bin_edges) # Usa apenas as bordas únicas\n            print(f\"Aviso: Bordas de bin não eram únicas (dados distorcidos?). \"\n                  f\"Número de bins real pode ser menor que {n_bins}.\")\n\n        intervals = pd.IntervalIndex.from_breaks(bin_edges, closed='left')\n        freq_counts = pd.Series(pd.cut(data_clean, bins=intervals)).value_counts().sort_index()\n\n        freq_table = pd.DataFrame({\n            'Bin': freq_counts.index.astype(str),\n            'Frequência': freq_counts.values\n        })\n    except ValueError as e:\n         print(f\"Aviso: Não foi possível criar tabela de frequência (Erro: {e}). Retornando tabela vazia.\")\n         \n    return discretized_data, freq_table", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 90, "titulo": "calculate_mean_confidence_interval", "categoria": "Estatística e Probabilidade", "subcategoria": "Inferência e Testes de Hipótese", "descricao": "Calcula o intervalo de confiança para a média populacional (μ) usando a distribuição Z ou t.", "codigo_funcao": "def calculate_mean_confidence_interval(\n    sample_mean: float,\n    sample_std: float,\n    sample_size: int,\n    confidence_level: float = 0.95,\n    population_std_known: bool = False\n) -> Tuple[float, float]:\n    \"\"\"\n    Calcula o intervalo de confiança para a média populacional (μ).\n\n    Utiliza a distribuição Z (Normal) para amostras grandes (n >= 30) ou\n    quando o desvio padrão populacional (σ) é conhecido. Para amostras pequenas (n < 30)\n    com σ desconhecido, utiliza a distribuição t-Student.\n\n    Args:\n        sample_mean (float): A média da amostra (x̄).\n        sample_std (float): O desvio padrão da amostra (s) ou da população (σ)\n                           se population_std_known=True. Deve ser >= 0.\n        sample_size (int): O tamanho da amostra (n). Deve ser > 0.\n        confidence_level (float, optional): O nível de confiança desejado (estritamente entre 0 e 1).\n                                           Defaults to 0.95.\n        population_std_known (bool, optional): Se True, força o uso da distribuição Z,\n                                               assumindo que `sample_std` é o desvio\n                                               padrão populacional (σ). Defaults to False.\n\n    Returns:\n        Tuple[float, float]: Uma tupla (limite_inferior, limite_superior) do intervalo de confiança.\n\n    Raises:\n        TypeError: Se sample_mean ou sample_std não forem numéricos, ou sample_size não for int.\n        ValueError: Se sample_size <= 0, confidence_level fora de (0,1), ou sample_std < 0.\n                    Também se sample_size=1 quando t-dist é usada (graus de liberdade 0).\n    \"\"\"\n    if not isinstance(sample_mean, (int, float)): raise TypeError(\"'sample_mean' deve ser numérico.\")\n    if not isinstance(sample_std, (int, float)): raise TypeError(\"'sample_std' deve ser numérico.\")\n    if not isinstance(sample_size, int): raise TypeError(\"'sample_size' deve ser inteiro.\")\n    if not isinstance(confidence_level, (int, float)): raise TypeError(\"'confidence_level' deve ser numérico.\")\n\n    if sample_size <= 0:\n        raise ValueError(\"O tamanho da amostra (sample_size) deve ser um inteiro positivo.\")\n    if not (0 < confidence_level < 1):\n        raise ValueError(\"O nível de confiança (confidence_level) deve estar no intervalo (0, 1).\")\n    if sample_std < 0:\n        raise ValueError(\"O desvio padrão (sample_std) não pode ser negativo.\")\n\n    # Calcula o erro padrão da média\n    # Adiciona pequena tolerância para evitar divisão por zero se sample_size=0 (já validado)\n    # ou se sample_std=0\n    if sample_std == 0:\n         # Se std é 0, o intervalo é apenas a média\n         return (float(sample_mean), float(sample_mean))\n    if sample_size == 0: # Redundante devido à validação, mas seguro\n        raise ValueError(\"Tamanho da amostra não pode ser zero.\") #pragma: no cover\n\n    standard_error = sample_std / np.sqrt(sample_size)\n\n    # Escolhe a distribuição e calcula o intervalo\n    if sample_size >= 30 or population_std_known:\n        # Usa a distribuição Z (Normal)\n        # norm.interval calcula diretamente o intervalo [loc - scale*zc, loc + scale*zc]\n        interval = norm.interval(confidence=confidence_level, loc=sample_mean, scale=standard_error)\n    else:\n        # Usa a distribuição t-Student\n        if sample_size == 1:\n             # t.interval falha com df=0. Retorna intervalo infinito ou NaN?\n             raise ValueError(\"Não é possível calcular o intervalo-t com tamanho de amostra 1 (graus de liberdade 0).\")\n        degrees_of_freedom = sample_size - 1\n        # t.interval calcula [loc - scale*tc, loc + scale*tc]\n        interval = t.interval(confidence=confidence_level, df=degrees_of_freedom, loc=sample_mean, scale=standard_error)\n\n    # Retorna como float nativo\n    return (float(interval[0]), float(interval[1]))", "bibliotecas": ["numpy", "scipy", "typing"], "versao": "0.1.0"}
{"id_funcao": 91, "titulo": "impute_missing_values_advanced", "categoria": "Exploração e Limpeza de Dados", "subcategoria": "Limpeza e Tratamento de Dados", "descricao": "Imputa valores ausentes (NaN) usando estratégias avançadas (KNN, Iterative, Regressão).", "codigo_funcao": "def impute_missing_values_advanced(\n    df: pd.DataFrame,\n    cols: Optional[List[str]] = None,\n    strategy: Literal['knn', 'iterative', 'linear_regression'] = 'knn',\n    n_neighbors: int = 5,\n    regression_features: Optional[List[str]] = None, \n    iterative_estimator: Optional[BaseEstimator] = None, \n    max_iter_iterative: int = 10, \n    inplace: bool = False\n) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Descrição: Imputa valores ausentes (NaN) em colunas numéricas usando estratégias\n               avançadas (KNN, Iterative Imputation, ou Regressão Linear Simples).\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols (List[str], optional): Colunas numéricas onde a imputação será aplicada. Se None,\n            seleciona todas as colunas numéricas com NaNs. Defaults to None.\n        strategy (Literal['knn', 'iterative', 'linear_regression'], optional): A estratégia\n            de imputação avançada. Defaults to 'knn'.\n        n_neighbors (int, optional): Número de vizinhos para strategy='knn'. Defaults to 5.\n        regression_features (Optional[List[str]], optional): Lista de colunas feature para usar\n            na strategy='linear_regression'. Idealmente uma única coluna para LR simples.\n            Ignorado para outras estratégias.\n        iterative_estimator (Optional[BaseEstimator], optional): Estimador scikit-learn a ser\n            usado pelo IterativeImputer (ex: BayesianRidge(), LinearRegression()). Se None,\n            usa o default do IterativeImputer (BayesianRidge). Ignorado para outras estratégias.\n        max_iter_iterative (int, optional): Número máximo de iterações para IterativeImputer.\n            Defaults to 10.\n        inplace (bool, optional): Se True, modifica o DataFrame original. Defaults to False.\n\n    Returns:\n        Optional[pd.DataFrame]: DataFrame com valores imputados, ou None se inplace=True.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame.\n        ValueError: Se a estratégia for inválida, colunas não existirem ou não forem numéricas,\n                    ou parâmetros necessários para a estratégia não forem fornecidos.\n        ImportError: Se IterativeImputer não puder ser importado (requer enable).\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n\n    valid_strategies = ['knn', 'iterative', 'linear_regression']\n    if strategy not in valid_strategies:\n        raise ValueError(f\"Estratégia inválida para imputação avançada. Escolha entre: {valid_strategies}\")\n\n    df_out = df if inplace else df.copy()\n\n    if cols is None:\n        numeric_cols_with_nan = df_out.select_dtypes(include=np.number).isnull().any()\n        cols_to_process = numeric_cols_with_nan[numeric_cols_with_nan].index.tolist()\n        if not cols_to_process:\n             print(\"Aviso: Nenhuma coluna numérica com valores ausentes encontrada.\")\n             return df_out if not inplace else None\n        print(f\"Aviso: 'cols' não fornecido. Tentando imputar colunas numéricas com NaNs: {cols_to_process}\")\n    else:\n        if not isinstance(cols, list):\n             raise TypeError(\"'cols' deve ser uma lista de nomes de colunas.\")\n        missing = [c for c in cols if c not in df_out.columns]\n        if missing:\n            raise ValueError(f\"As seguintes colunas não existem no df: {missing}\")\n        # Filtra para apenas colunas numéricas e com NaNs\n        numeric_cols = df_out[cols].select_dtypes(include=np.number).columns.tolist()\n        if len(numeric_cols) < len(cols):\n            non_numeric = [c for c in cols if c not in numeric_cols]\n            print(f\"Aviso: Ignorando colunas não numéricas especificadas em 'cols': {non_numeric}\")\n        cols_to_process = [c for c in numeric_cols if df_out[c].isnull().any()]\n        if not cols_to_process:\n             print(f\"Aviso: Nenhuma das colunas numéricas especificadas em 'cols' possui valores ausentes.\")\n             return df_out if not inplace else None\n\n    # Estratégia KNNImputer\n    if strategy == 'knn':\n        imputer = KNNImputer(n_neighbors=n_neighbors)\n        try:\n            imputed_data = imputer.fit_transform(df_out[cols_to_process])\n            df_imputed_temp = pd.DataFrame(imputed_data, columns=cols_to_process, index=df_out.index)\n            df_out.update(df_imputed_temp)\n        except Exception as e:\n            raise RuntimeError(f\"Erro ao aplicar KNNImputer (k={n_neighbors}) \"\n                               f\"nas colunas {cols_to_process}: {e}\")\n\n    # Estratégia IterativeImputer\n    elif strategy == 'iterative':\n        if iterative_estimator is None:\n            # Default para BayesianRidge se nada for passado\n             iterative_estimator = BayesianRidge()\n        imputer = IterativeImputer(estimator=iterative_estimator, max_iter=max_iter_iterative, random_state=42)\n        try:\n            imputed_data = imputer.fit_transform(df_out[cols_to_process])\n            df_imputed_temp = pd.DataFrame(imputed_data, columns=cols_to_process, index=df_out.index)\n            df_out.update(df_imputed_temp)\n        except Exception as e:\n            raise RuntimeError(f\"Erro ao aplicar IterativeImputer (estimator={iterative_estimator.__class__.__name__}) \"\n                               f\"nas colunas {cols_to_process}: {e}\")\n\n    # Estratégia Linear Regression Simples (Itera por coluna alvo)\n    elif strategy == 'linear_regression':\n        if regression_features is None or not regression_features:\n            raise ValueError(\"`regression_features` deve ser fornecido para strategy='linear_regression'.\")\n        missing_features = [f for f in regression_features if f not in df_out.columns]\n        if missing_features:\n            raise ValueError(f\"Colunas de feature para regressão não encontradas: {missing_features}\")\n        # Garante que features também sejam numéricas\n        numeric_features = df_out[regression_features].select_dtypes(include=np.number).columns.tolist()\n        if len(numeric_features) < len(regression_features):\n             non_num_feat = [f for f in regression_features if f not in numeric_features]\n             raise ValueError(f\"Colunas de feature para regressão devem ser numéricas. Não numéricas: {non_num_feat}\")\n\n\n        print(f\"Aviso: strategy='linear_regression' imputará cada coluna em {cols_to_process} \"\n              f\"separadamente usando {regression_features} como preditores.\")\n\n        for target_col in cols_to_process:\n             # Dados de treino: linhas onde alvo e features NÃO são NaN\n             train_mask = df_out[target_col].notna() & df_out[numeric_features].notna().all(axis=1)\n             df_train = df_out.loc[train_mask]\n\n             if df_train.empty or len(df_train) < 2:\n                 print(f\"Aviso: Não há dados não nulos suficientes para treinar LR para '{target_col}'. Pulando.\")\n                 continue\n\n             X_train = df_train[numeric_features]\n             y_train = df_train[target_col]\n\n             model = LinearRegression() # Modelo simples\n             model.fit(X_train, y_train)\n\n             # Dados para imputar: linhas onde alvo é NaN mas features NÃO são NaN\n             impute_mask = df_out[target_col].isnull() & df_out[numeric_features].notna().all(axis=1)\n\n             if impute_mask.any():\n                 X_to_predict = df_out.loc[impute_mask, numeric_features]\n                 predicted_values = model.predict(X_to_predict)\n                 df_out.loc[impute_mask, target_col] = predicted_values\n\n    if inplace:\n        return None\n    return df_out", "bibliotecas": ["numpy", "pandas", "scikit-learn", "typing"], "versao": "0.1.0"}
{"id_funcao": 92, "titulo": "drop_or_mark_rows_all_zero", "categoria": "Exploração e Limpeza de Dados", "subcategoria": "Limpeza e Tratamento de Dados", "descricao": "Identifica, remove, marca ou retorna índices de linhas onde um conjunto de colunas são todas zero.", "codigo_funcao": "def drop_or_mark_rows_all_zero(\n    df: pd.DataFrame,\n    cols_to_check: Optional[List[str]] = None,\n    exclude: Optional[List[str]] = None,\n    action: Literal['drop', 'index', 'flag'] = 'drop',\n    flag_col_name: str = 'is_all_zero',\n    inplace: bool = False\n) -> Union[pd.DataFrame, pd.Index, None]:\n    \"\"\"\n    Descrição: Identifica, remove, marca ou retorna índices de linhas onde um\n               conjunto especificado de colunas são todas iguais a zero.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols_to_check (List[str], optional): Lista de colunas a verificar. Se None,\n            usa todas as colunas exceto as em 'exclude'. Defaults to None.\n        exclude (List[str], optional): Lista de colunas a serem ignoradas na\n            verificação, mesmo se 'cols_to_check' for None. Defaults to None.\n        action (Literal['drop', 'index', 'flag'], optional):\n            'drop': Retorna um DataFrame com as linhas removidas.\n            'index': Retorna o índice das linhas que seriam removidas.\n            'flag': Adiciona uma coluna booleana indicando as linhas.\n            Defaults to 'drop'.\n        flag_col_name (str, optional): Nome da coluna a ser criada se action='flag'.\n            Defaults to 'is_all_zero'.\n        inplace (bool, optional): Se True e action='drop' ou action='flag',\n            modifica o DataFrame original. Ignorado para action='index'.\n            Defaults to False.\n\n    Returns:\n        Union[pd.DataFrame, pd.Index, None]:\n            - DataFrame modificado (se action='drop' ou action='flag').\n            - pd.Index com os índices das linhas (se action='index').\n            - None se inplace=True.\n\n    Raises:\n        TypeError: Se 'df' não for DataFrame, ou listas de colunas inválidas.\n        ValueError: Se colunas especificadas não existirem, ou 'action' inválido.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n    if exclude is None:\n        exclude = []\n    if cols_to_check is None:\n        cols_to_check = [col for col in df.select_dtypes(include=np.number).columns if col not in exclude]\n    else:\n        # Garante que cols_to_check é lista e valida\n        if not isinstance(cols_to_check, list) or not all(isinstance(c, str) for c in cols_to_check):\n             raise TypeError(\"'cols_to_check' deve ser uma lista de strings.\")\n        non_existent = [col for col in cols_to_check if col not in df.columns]\n        if non_existent:\n            raise ValueError(f\"Colunas não encontradas em 'cols_to_check': {non_existent}\")\n        # Remove colunas excluídas de cols_to_check se houver sobreposição\n        cols_to_check = [col for col in cols_to_check if col not in exclude]\n\n    if not cols_to_check:\n        if action == 'index':\n            return pd.Index([])\n        elif action == 'flag':\n            df_out = df if inplace else df.copy()\n            df_out[flag_col_name] = False\n            if inplace:\n                return None\n            return df_out\n        # Se action == 'drop'\n        if inplace:\n            return None\n        return df.copy()\n\n    if action not in ['drop', 'index', 'flag']:\n        raise ValueError(\"O argumento 'action' deve ser 'drop', 'index' ou 'flag'.\")\n\n    df_out = df if inplace else df.copy()\n\n    # Cria a máscara booleana para linhas onde TODAS as colunas especificadas são 0\n    zero_mask = (df_out[cols_to_check] == 0).all(axis=1)\n\n    if action == 'index':\n        return df_out.index[zero_mask]\n    elif action == 'flag':\n        df_out[flag_col_name] = zero_mask\n        if inplace:\n            return None\n        return df_out\n    elif action == 'drop':\n        rows_to_drop = df_out.index[zero_mask]\n        df_out.drop(index=rows_to_drop, inplace=True)\n        if inplace:\n            return None\n        return df_out", "bibliotecas": ["numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 93, "titulo": "plot_correlation_heatmap", "categoria": "Visualização de Dados", "subcategoria": "Relações Bivariadas e Multivariadas", "descricao": "Calcula e plota um heatmap da matriz de correlação para colunas selecionadas.", "codigo_funcao": "def plot_correlation_heatmap(\n    df: pd.DataFrame,\n    cols: Optional[List[str]] = None,\n    method: str = 'pearson',\n    ax: Optional[plt.Axes] = None,\n    **heatmap_kwargs\n) -> plt.Axes:\n    \"\"\"\n    Descrição: Calcula e plota um heatmap da matriz de correlação para colunas selecionadas.\n\n    Args:\n        df (pd.DataFrame): DataFrame de entrada.\n        cols (Optional[List[str]], optional): Lista de colunas para incluir na\n            matriz de correlação. Se None, usa todas as colunas numéricas do DataFrame.\n            Defaults to None.\n        method (str, optional): Método de correlação ('pearson', 'kendall', 'spearman').\n            Defaults to 'pearson'.\n        ax (Optional[plt.Axes], optional): Eixo Matplotlib onde plotar. Se None,\n            um novo eixo é criado. Defaults to None.\n        **heatmap_kwargs: Argumentos adicionais a serem passados para sns.heatmap()\n            (ex: cmap='coolwarm', annot=True, fmt=\".2f\").\n\n    Returns:\n        plt.Axes: O objeto Axes do Matplotlib contendo o heatmap.\n\n    Raises:\n        TypeError: Se 'df' não for um DataFrame pandas.\n        ValueError: Se 'cols' for fornecido e alguma coluna não existir, ou se\n                    nenhuma coluna numérica for encontrada/selecionada.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"O argumento 'df' deve ser um DataFrame pandas.\")\n\n    if cols:\n        if not isinstance(cols, list):\n             raise TypeError(\"'cols' deve ser uma lista de nomes de colunas.\")\n        non_existent = [col for col in cols if col not in df.columns]\n        if non_existent:\n            raise ValueError(f\"As seguintes colunas especificadas em 'cols' não existem: {non_existent}\")\n        df_to_corr = df[cols]\n    else:\n        # Seleciona apenas colunas numéricas\n        df_to_corr = df.select_dtypes(include=np.number)\n        if df_to_corr.empty:\n            raise ValueError(\"Nenhuma coluna numérica encontrada no DataFrame e 'cols' não foi fornecido.\")\n        cols = df_to_corr.columns.tolist() # Define cols para uso posterior\n\n    if df_to_corr.shape[1] < 2:\n         raise ValueError(\"Pelo menos duas colunas numéricas são necessárias para calcular a correlação.\")\n\n    # Calcula a matriz de correlação\n    try:\n        corr_matrix = df_to_corr.corr(method=method)\n    except Exception as e:\n        raise RuntimeError(f\"Erro ao calcular a matriz de correlação (método '{method}'): {e}\")\n\n    # Cria o plot se ax não for fornecido\n    if ax is None:\n        fig, ax = plt.subplots(figsize=heatmap_kwargs.pop('figsize', (max(6, len(cols)*0.8), max(5, len(cols)*0.7)))) # Tamanho adaptativo\n\n    # Plota o heatmap\n    sns.heatmap(corr_matrix, ax=ax, **heatmap_kwargs)\n    ax.set_title(heatmap_kwargs.pop('title', f'Matriz de Correlação ({method.capitalize()})'))\n\n    # Ajusta rotação dos labels se muitos\n    if len(cols) > 10:\n         ax.tick_params(axis='x', rotation=90)\n         ax.tick_params(axis='y', rotation=0) \n\n    plt.tight_layout() # Ajusta layout para evitar sobreposição\n\n    return ax", "bibliotecas": ["matplotlib", "numpy", "pandas", "seaborn", "typing"], "versao": "0.1.0"}
{"id_funcao": 94, "titulo": "calculate_centrality_measures", "categoria": "Grafos e Redes", "subcategoria": "Análise de Métricas de Rede", "descricao": "Calcula um conjunto de métricas de centralidade comuns (grau, intermediação, etc.) para os nós de um grafo.", "codigo_funcao": "def calculate_centrality_measures(\n    g: nx.Graph # Aceita Graph ou DiGraph\n) -> pd.DataFrame:\n    \"\"\"\n    Descrição: Calcula um conjunto de métricas de centralidade comuns para todos os nós de um grafo.\n\n    Args:\n        g (nx.Graph): O objeto de grafo NetworkX (Graph ou DiGraph).\n\n    Returns:\n        pd.DataFrame: Um DataFrame com os nós como índice e as métricas de centralidade\n            (grau, intermediação, proximidade, autovetor, PageRank) como colunas.\n            Pode conter NaNs para eigenvector se a convergência falhar.\n\n    Raises:\n        TypeError: Se 'g' não for um objeto de grafo NetworkX.\n        nx.NetworkXError: Se algum cálculo de centralidade falhar (ex: autovetor não converge).\n    \"\"\"\n    if not isinstance(g, (nx.Graph, nx.DiGraph)):\n        raise TypeError(\"O argumento 'g' deve ser um objeto de grafo NetworkX (Graph ou DiGraph).\")\n\n    centrality_data = {}\n    \n    # Grau (Degree)\n    centrality_data['degree'] = dict(g.degree())\n    \n    # Intermediação (Betweenness)\n    centrality_data['betweenness'] = nx.betweenness_centrality(g)\n    \n    # Proximidade (Closeness) - Pode falhar em grafos não conectados\n    try:\n        centrality_data['closeness'] = nx.closeness_centrality(g)\n    except nx.NetworkXError as e:\n         print(f\"Aviso: Não foi possível calcular closeness_centrality (grafo pode não ser conectado): {e}\")\n         centrality_data['closeness'] = {node: float('nan') for node in g.nodes()} # Preenche com NaN\n         \n    # Autovetor (Eigenvector) - Pode não convergir\n    try:\n        centrality_data['eigenvector'] = nx.eigenvector_centrality(g, max_iter=1000, tol=1e-06)\n    except nx.PowerIterationFailedConvergence as e:\n        print(f\"Aviso: eigenvector_centrality não convergiu após 1000 iterações: {e}\")\n        centrality_data['eigenvector'] = {node: float('nan') for node in g.nodes()} # Preenche com NaN\n    except Exception as e: # Captura outros erros possíveis (ex: grafos vazios)\n         print(f\"Aviso: Erro ao calcular eigenvector_centrality: {e}\")\n         centrality_data['eigenvector'] = {node: float('nan') for node in g.nodes()}\n\n    # PageRank\n    centrality_data['pagerank'] = nx.pagerank(g)\n\n    df_centrality = pd.DataFrame(centrality_data)\n    df_centrality.index.name = 'node'\n    return df_centrality.sort_values(by='degree', ascending=False)", "bibliotecas": ["networkx", "numpy", "pandas", "typing"], "versao": "0.1.0"}
{"id_funcao": 95, "titulo": "analyze_graph_properties", "categoria": "Grafos e Redes", "subcategoria": "Análise de Métricas de Rede", "descricao": "Calcula propriedades globais de um grafo (nós, arestas, densidade, diâmetro).", "codigo_funcao": "def analyze_graph_properties(\n    g: nx.Graph # Aceita Graph ou DiGraph\n) -> Dict[str, Any]:\n    \"\"\"\n    Descrição: Calcula e retorna as propriedades globais de um grafo.\n\n    Args:\n        g (nx.Graph): O objeto de grafo NetworkX (Graph ou DiGraph).\n\n    Returns:\n        Dict[str, Any]: Um dicionário contendo o número de nós, número de arestas,\n            densidade do grafo e diâmetro (se aplicável).\n\n    Raises:\n        TypeError: Se 'g' não for um objeto de grafo NetworkX.\n    \"\"\"\n    if not isinstance(g, (nx.Graph, nx.DiGraph)):\n        raise TypeError(\"O argumento 'g' deve ser um objeto de grafo NetworkX (Graph ou DiGraph).\")\n\n    properties = {\n        \"num_nodes\": g.number_of_nodes(),\n        \"num_edges\": g.number_of_edges(),\n        \"density\": nx.density(g), # nx.density geralmente suporta DiGraph\n        \"diameter\": \"N/A\" # Default\n    }\n\n    # Calcula diâmetro se aplicável e possível\n    if properties[\"num_nodes\"] > 0: # Evita erros em grafos vazios\n        if isinstance(g, nx.Graph): # Grafo não direcionado\n            is_connected = nx.is_connected(g) # Verifica a conectividade antes\n            if is_connected:\n                try:\n                    properties[\"diameter\"] = nx.diameter(g)\n                except Exception as e:\n                     # Captura outros erros inesperados no cálculo\n                     print(f\"Aviso: Erro inesperado ao calcular diâmetro para grafo conectado: {e}\")\n                     properties[\"diameter\"] = \"Erro no cálculo\"\n            else:\n                properties[\"diameter\"] = float('inf')\n                # print(\"Aviso: O grafo não direcionado não é conectado, o diâmetro é infinito.\") # Comentado para teste limpo\n        else: # Grafo direcionado (nx.DiGraph)\n            # Diâmetro em DiGraphs usa conectividade forte\n            is_strong = nx.is_strongly_connected(g) # Verifica a conectividade forte antes\n            if is_strong:\n                try:\n                    properties[\"diameter\"] = nx.diameter(g)\n                except nx.NetworkXNotImplemented as ni_err: # Captura o erro específico\n                    print(f\"Aviso: O cálculo de diâmetro pode não ser implementado para DiGraph nesta versão do NetworkX: {ni_err}\")\n                    properties[\"diameter\"] = \"Não Implementado\"\n                except Exception as e:\n                    # Captura outros erros inesperados no cálculo\n                    print(f\"Aviso: Erro inesperado ao calcular diâmetro para grafo fortemente conectado: {e}\")\n                    properties[\"diameter\"] = \"Erro no cálculo\"\n            else:\n                 properties[\"diameter\"] = float('inf')\n                 # print(\"Aviso: O grafo direcionado não é fortemente conectado, o diâmetro (baseado em conectividade forte) é infinito.\") # Comentado\n\n    return properties", "bibliotecas": ["networkx", "numpy", "typing"], "versao": "0.1.0"}
